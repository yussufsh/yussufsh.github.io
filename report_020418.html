<html><head><script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous" rel="stylesheet"></link><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous" rel="stylesheet"></link><script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><script src="helper.js">function hideAll(){console.log("hideAll")}function showme(e){console.log("showme");var l,n=e.substring(7),o=document.getElementsByName("data");for(l=0;l&lt;o.length;l++)o[l].style.display="none";var t=document.getElementsByName("summary");for(l=0;l&lt;t.length;l++)t[l].style.display="none";document.getElementById(n).style.display="block"}</script><style>table, th, td { vertical-align:top; padding: 3px} table {table-layout:fixed} td {word-wrap:break-word} .bs-callout { padding: 5px; margin: 5px 0; border: 1px solid #eee; border-left-width: 5px; border-radius: 3px; font-weight:normal; }.bs-callout-info {border-left-color: #5bc0de;}</style></head><body><div class="page-header"><ul class="nav nav-pills"><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcx86" onclick="showme(this.id);">FULL SUMMARY</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcubuntu16" onclick="showme(this.id);">PPC UBUNTU16</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_x86ubuntu16" onclick="showme(this.id);">X86 UBUNTU16</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcrhel7" onclick="showme(this.id);">PPC RHEL7</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_x86rhel7" onclick="showme(this.id);">X86 RHEL7</a></li><p align="right" role="presentation" style="color:grey">Date: 02-04-2018 12:00 UTC</p></ul></div><div style="table-cell" class="col-sm-2 col-md-2 sidebar"><div class="list-group"><a href="#" class="list-group-item list-group-item-action active" onclick="showme(this.id);" id="anchor_ppcx86">Packages</a><a href="#" id="anchor_accumulo" onclick="showme(this.id);" class="list-group-item list-group-item-action">ACCUMULO</a><a href="#" id="anchor_ambari" onclick="showme(this.id);" class="list-group-item list-group-item-action">AMBARI</a><a href="#" id="anchor_atlas" onclick="showme(this.id);" class="list-group-item list-group-item-action">ATLAS</a><a href="#" id="anchor_falcon" onclick="showme(this.id);" class="list-group-item list-group-item-action">FALCON</a><a href="#" id="anchor_flume" onclick="showme(this.id);" class="list-group-item list-group-item-action">FLUME</a><a href="#" id="anchor_hadoop" onclick="showme(this.id);" class="list-group-item list-group-item-action">HADOOP</a><a href="#" id="anchor_hbase" onclick="showme(this.id);" class="list-group-item list-group-item-action">HBASE</a><a href="#" id="anchor_hive" onclick="showme(this.id);" class="list-group-item list-group-item-action">HIVE</a><a href="#" id="anchor_kafka" onclick="showme(this.id);" class="list-group-item list-group-item-action">KAFKA</a><a href="#" id="anchor_knox" onclick="showme(this.id);" class="list-group-item list-group-item-action">KNOX</a><a href="#" id="anchor_metron" onclick="showme(this.id);" class="list-group-item list-group-item-action">METRON</a><a href="#" id="anchor_oozie" onclick="showme(this.id);" class="list-group-item list-group-item-action">OOZIE</a><a href="#" id="anchor_phoenix" onclick="showme(this.id);" class="list-group-item list-group-item-action">PHOENIX</a><a href="#" id="anchor_pig" onclick="showme(this.id);" class="list-group-item list-group-item-action">PIG</a><a href="#" id="anchor_ranger" onclick="showme(this.id);" class="list-group-item list-group-item-action">RANGER</a><a href="#" id="anchor_slider" onclick="showme(this.id);" class="list-group-item list-group-item-action">SLIDER</a><a href="#" id="anchor_spark" onclick="showme(this.id);" class="list-group-item list-group-item-action">SPARK</a><a href="#" id="anchor_sqoop" onclick="showme(this.id);" class="list-group-item list-group-item-action">SQOOP</a><a href="#" id="anchor_storm" onclick="showme(this.id);" class="list-group-item list-group-item-action">STORM</a><a href="#" id="anchor_tez" onclick="showme(this.id);" class="list-group-item list-group-item-action">TEZ</a><a href="#" id="anchor_zeppelin" onclick="showme(this.id);" class="list-group-item list-group-item-action">ZEPPELIN</a><a href="#" id="anchor_zookeeper" onclick="showme(this.id);" class="list-group-item list-group-item-action">ZOOKEEPER</a></div></div><div style="display: table-cell"><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="accumulo"><div style="font-weight:bold;" class="panel-heading">ACCUMULO</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>830ff6eec93c9c7a5c0cbb78e2d67977c45c3eb4</div><div><b>Last Run: </b>29-03-2018 12:56 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1711</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1711</div><div>Failed Count : 2</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1711</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1711</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.testPerTableClasspath</li></div><div><li>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.test</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 30000 milliseconds</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.testPerTableClasspath</div></li><li><div>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.test</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="ambari"><div style="font-weight:bold;" class="panel-heading">AMBARI</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> origin/trunk</div><div><b>Last Revision: </b>ef2b0ae16fb3975888962d0e2615678d7bdc7fd5</div><div><b>Last Run: </b>21-03-2018 00:31 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 0</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 5507</div><div>Failed Count : 1</div><div>Skipped Count : 83</div></td><td><div>Total Count : 5507</div><div>Failed Count : 1</div><div>Skipped Count : 83</div></td><td><div>Total Count : 5507</div><div>Failed Count : 0</div><div>Skipped Count : 83</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/red.png" align="top" style="width: 16px; height: 16px;"></img>FAILURE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.serveraction.upgrades.ConfigureActionTest.testAllowedReplacment</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.ambari.server.security.encryption.CredentialStoreTest.getExpiredCredentialTest(CredentialStoreTest.java:169)
	at org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired(CredentialStoreTest.java:90)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.ambari.server.serveraction.upgrades.ConfigureActionTest.testAllowedReplacment(ConfigureActionTest.java:865)
</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.ambari.server.serveraction.upgrades.ConfigureActionTest.testAllowedReplacment</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="atlas"><div style="font-weight:bold;" class="panel-heading">ATLAS</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>9ffaaabaaf692a07ea74f929515659fa425feff8</div><div><b>Last Run: </b>29-03-2018 12:56 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 771</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 771</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 771</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 771</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="falcon"><div style="font-weight:bold;" class="panel-heading">FALCON</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>1a5b4f6a509187498a267b0e375c6a065f947af5</div><div><b>Last Run: </b>29-03-2018 12:56 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1002</div><div>Failed Count : 4</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1000</div><div>Failed Count : 2</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1000</div><div>Failed Count : 4</div><div>Skipped Count : 0</div></td><td><div>Total Count : 998</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.falcon.update.UpdateHelperTest.testIsEntityACLUpdated</li></div><div><li>org.apache.falcon.update.UpdateHelperTest.testIsEntityLateProcessUpdated</li></div><div><li>org.apache.falcon.update.UpdateHelperTest.testIsEntityUpdated</li></div><div><li>org.apache.falcon.update.UpdateHelperTest.testShouldUpdateAffectedEntities</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.falcon.entity.v0.EntityGraphTest.initConfigStore</li></div><div><li>org.apache.falcon.notification.service.SchedulerServiceTest.testDeRegistration</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.falcon.update.UpdateHelperTest.testIsEntityACLUpdated</li></div><div><li>org.apache.falcon.update.UpdateHelperTest.testIsEntityLateProcessUpdated</li></div><div><li>org.apache.falcon.update.UpdateHelperTest.testIsEntityUpdated</li></div><div><li>org.apache.falcon.update.UpdateHelperTest.testShouldUpdateAffectedEntities</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to restore configurations for entity type PROCESS</li></div><div><li>expected [1] but found [null]</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.falcon.update.UpdateHelperTest.testIsEntityACLUpdated</div></li><li><div>org.apache.falcon.update.UpdateHelperTest.testIsEntityLateProcessUpdated</div></li><li><div>org.apache.falcon.update.UpdateHelperTest.testIsEntityUpdated</div></li><li><div>org.apache.falcon.update.UpdateHelperTest.testShouldUpdateAffectedEntities</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.falcon.entity.v0.EntityGraphTest.initConfigStore</div></li><li><div>org.apache.falcon.notification.service.SchedulerServiceTest.testDeRegistration</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.falcon.update.UpdateHelperTest.testIsEntityACLUpdated</div></li><li><div>org.apache.falcon.update.UpdateHelperTest.testIsEntityLateProcessUpdated</div></li><li><div>org.apache.falcon.update.UpdateHelperTest.testIsEntityUpdated</div></li><li><div>org.apache.falcon.update.UpdateHelperTest.testShouldUpdateAffectedEntities</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="flume"><div style="font-weight:bold;" class="panel-heading">FLUME</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>0d437810dc850192b48fa3b31608ffcd23b1f1e9</div><div><b>Last Run: </b>29-03-2018 12:56 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1182</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1208</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1179</div><div>Failed Count : 2</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1208</div><div>Failed Count : 1</div><div>Skipped Count : 6</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</li></div><div><li>org.apache.flume.channel.kafka.TestRollback.org.apache.flume.channel.kafka.TestRollback</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.flume.channel.file.TestLog.doTestMinimumRequiredSpaceTooSmallForPut(TestLog.java:241)
	at org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut(TestLog.java:210)
</li></div><div><li>Unable to connect to zookeeper server within timeout: 1000</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.flume.channel.file.TestLog.doTestMinimumRequiredSpaceTooSmallForPut(TestLog.java:241)
	at org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut(TestLog.java:210)
</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.flume.channel.kafka.TestRollback.org.apache.flume.channel.kafka.TestRollback</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hadoop"><div style="font-weight:bold;" class="panel-heading">HADOOP</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>49c747ab187d0650143205ba57ca19607ec4c6bd</div><div><b>Last Run: </b>17-03-2018 14:00 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 18939</div><div>Failed Count : 18</div><div>Skipped Count : 1156</div></td><td><div>Total Count : 18722</div><div>Failed Count : 60</div><div>Skipped Count : 1158</div></td><td><div>Total Count : 18904</div><div>Failed Count : 20</div><div>Skipped Count : 1157</div></td><td><div>Total Count : 18814</div><div>Failed Count : 28</div><div>Skipped Count : 1158</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.ha.TestZKFailoverController.testGracefulFailoverMultipleZKfcs</li></div><div><li>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.balancer.TestBalancer.testBalancerWithStripedFile</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks.testSetRepIncWithUnderReplicatedBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testVolumeFailure</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testSuccessiveVolumeFailures</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore.org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.testFixedSizeThreadPool</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testStartMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage.testContainerLogPageAccess</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.ha.TestZKFailoverController.testGracefulFailoverMultipleZKfcs</li></div><div><li>org.apache.hadoop.security.TestGroupsCaching.testThreadNotBlockedWhenExpiredEntryExistsWithBackgroundRefresh</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notDir_local</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notReadable_local</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notWritable_local</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testDiskIoIgnoresTransientCreateErrors</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testDiskIoDetectsCreateErrors</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notListable</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notListable_local</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testCheckDir_normal_local</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testDiskIoDetectsWriteErrors</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notWritable</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testCheckDir_normal</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notDir</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notReadable</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testDiskIoIgnoresTransientWriteErrors</li></div><div><li>org.apache.hadoop.hdfs.TestDFSShell.testCopyCommandsWithPreserveOption</li></div><div><li>org.apache.hadoop.hdfs.TestHDFSFileSystemContract.testAppend</li></div><div><li>org.apache.hadoop.hdfs.TestLocalDFS.testWorkingDirectory</li></div><div><li>org.apache.hadoop.hdfs.TestMaintenanceState.testDecommissionDifferentNodeAfterMaintenances</li></div><div><li>org.apache.hadoop.hdfs.TestPersistBlocks.TestRestartDfsWithFlush</li></div><div><li>org.apache.hadoop.hdfs.TestPread.testPreadFailureWithChangedBlockLocations</li></div><div><li>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</li></div><div><li>org.apache.hadoop.hdfs.TestReconstructStripedFile.testRecoverAllParityBlocks</li></div><div><li>org.apache.hadoop.hdfs.security.TestDelegationTokenForProxyUser.testWebHdfsDoAs</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean.testStorageTypeStatsWhenStorageFailed</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean.testStorageTypeStatsJMX</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeErasureCodingMetrics.testReconstructionBytesPartialGroup2</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID.testUUIDRegeneration</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testUnderReplicationAfterVolFailure</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testVolumeFailureRecoveredByHotSwappingVolume</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testDataNodeReconfigureWithVolumeFailures</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner.testThrottling</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner.testDirectoryScannerInFederatedCluster</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestSpaceReservation.testTmpSpaceReserve</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestHAAppend.testMultipleAppendsDuringCatchupTailing</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.testCompleteFileAfterCrashFailover</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.testWriteOverGracefulFailoverWithDnFail</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA.testUpdatePipeline</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.TestNNBench.testNNBenchCreateReadAndDelete</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMRTimelineEventHandling</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</li></div><div><li>org.apache.hadoop.yarn.sls.TestSLSRunner.testSimulatorRunning[Testing with: SYNTH, org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler, (nodeFile null)]</li></div><div><li>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[1]</li></div><div><li>org.apache.hadoop.yarn.logaggregation.TestAggregatedLogDeletionService.testRefreshLogRetentionSettings</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage.testContainerLogPageAccess</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testRemoveAttempt</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testIncreaseContainerUnreservedWhenApplicationCompleted</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testExcessiveReservationWhenDecreaseSameContainer</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken</li></div><div><li>org.apache.hadoop.yarn.webapp.TestRMWithCSRFFilter.testNoCustomHeaderFromBrowser</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.io.nativeio.TestNativeIO.testStat</li></div><div><li>org.apache.hadoop.io.nativeio.TestNativeIO.testMultiThreadedStat</li></div><div><li>org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy.testCloseWithExceptionsInStreamer</li></div><div><li>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testUnderReplicationAfterVolFailure</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestNameNodeMetadataConsistency.testGenerationStampInFuture</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestSecurityTokenEditLog.testEditsForCancelOnTokenExpire</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestHAAppend.testMultipleAppendsDuringCatchupTailing</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics.testCapacityMetrics</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.web.resources.TestWebHdfsDataLocality.testExcludeDataNodes</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMRNewTimelineServiceEventHandling</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</li></div><div><li>org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore.org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore</li></div><div><li>org.apache.hadoop.yarn.sls.scheduler.TestTaskRunner.testPreStartQueueing</li></div><div><li>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2CustomizedFlow</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithSaslEncryption[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithSaslEncryption[0]</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.testFixedSizeThreadPool</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage.testContainerLogPageAccess</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestFSRMStateStore.testFSRMStateStoreClientRetry</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.ha.TestZKFailoverController.testGracefulFailoverMultipleZKfcs</li></div><div><li>org.apache.hadoop.ipc.TestIPC.testConnectionIdleTimeouts</li></div><div><li>org.apache.hadoop.security.TestGroupsCaching.testThreadNotBlockedWhenExpiredEntryExistsWithBackgroundRefresh</li></div><div><li>org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager.testMultiNodeOperations</li></div><div><li>org.apache.hadoop.hdfs.TestDecommission.testPendingNodes</li></div><div><li>org.apache.hadoop.hdfs.TestErasureCodingMultipleRacks.testSkewedRack2</li></div><div><li>org.apache.hadoop.hdfs.TestHDFSFileSystemContract.testAppend</li></div><div><li>org.apache.hadoop.hdfs.TestPersistBlocks.TestRestartDfsWithFlush</li></div><div><li>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</li></div><div><li>org.apache.hadoop.hdfs.security.TestDelegationTokenForProxyUser.testWebHdfsDoAs</li></div><div><li>org.apache.hadoop.hdfs.server.balancer.TestBalancerRPCDelay.testBalancerRPCDelay</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID.testUUIDRegeneration</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner.testThrottling</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithJournalNodes</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA.testUpdatePipeline</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</li></div><div><li>org.apache.hadoop.fs.azure.TestClientThrottlingAnalyzer.testManySuccessAndErrorsAndWaiting</li></div><div><li>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2</li></div><div><li>org.apache.hadoop.yarn.logaggregation.TestAggregatedLogDeletionService.testRefreshLogRetentionSettings</li></div><div><li>org.apache.hadoop.registry.server.dns.TestSecureRegistryDNS.testExternalCNAMERecord</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync.testNMSentContainerStatusOnResync</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeProcessFailure</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testStartMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage.testContainerLogPageAccess</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMEmbeddedElector.testCallbackSynchronization</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler.testCSReservationWithRootUnblocked</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testContainerIncreaseAllocationExpiration</li></div><div><li>org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.testDeleteReservationXML</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Unable to become active. Local node did not get an opportunity to do so from ZooKeeper, or the local node took too long to transition to active.</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>Cluster failed to reached expected values of totalSpace (current: 1174405120, expected: 1174405120), or usedSpace (current: 457179166, expected: 452984832), in more than 40000 msec.</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li> Expected to find 'localhost:41529: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:41529: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:35320: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:35320: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:36566: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:36566: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li> Expected to find 'localhost:44634: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:44634: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:37693: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:37693: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:41960: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:41960: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li>initTable on TestDynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException: The request processing has failed because of an unknown error, exception or failure. (Service: AmazonDynamoDBv2; Status Code: 500; Error Code: InternalFailure; Request ID: 49668297-dc74-4918-9472-af94f77fe42f): The request processing has failed because of an unknown error, exception or failure.</li></div><div><li>expected:&lt;3&gt; but was:&lt;2&gt;</li></div><div><li>ContainerState is not correct (timedout)</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileControllerFactory.&lt;init&gt;(LogAggregationFileControllerFactory.java:68)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsPage$ContainersLogsBlock.&lt;init&gt;(ContainerLogsPage.java:100)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage.testContainerLogPageAcc</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to become active. Local node did not get an opportunity to do so from ZooKeeper, or the local node took too long to transition to active.</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test5903334520507024322tmp</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test1498795230920626705</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test6584864813077008841</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test2335399963726861465</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test2659697764578788802</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test6891259757836500600</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test259780093406813557</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test2259207363293821701</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test6409131635423873089</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test5812972311135538926</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test7909222521993262347</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test6571365320603941937tmp</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test6007983364796014975</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test4083909263333575199</li></div><div><li>expected:&lt;1521301843594&gt; but was:&lt;1521301844711&gt;</li></div><div><li>test timed out after 30000 milliseconds</li></div><div><li>test timed out after 20000 milliseconds</li></div><div><li>1</li></div><div><li>End of File Exception between local host is: "in-ibmibm662.persistent.co.in/10.53.17.125"; destination host is: "localhost":34026; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException</li></div><div><li>1</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>test timed out after 5000 milliseconds</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>expected:&lt;6&gt; but was:&lt;5&gt;</li></div><div><li>ecReconstructionBytesRead should be  expected:&lt;7444889&gt; but was:&lt;0&gt;</li></div><div><li>test timed out after 10000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>Timed out waiting for /test1 to reach 2 replicas</li></div><div><li>test timed out after 600000 milliseconds</li></div><div><li>Cannot remove data directory: /var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-hdfs-project/hadoop-hdfs/target/test/data/2/dfs/datapath '/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-hdfs-project/hadoop-hdfs/target/test/data/2/dfs/data': 
 absolute:/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-hdfs-project/hadoop-hdfs/target/test/data/2/dfs/data
 permissions: drwx
path '/va</li></div><div><li>Wrong reserve space for Tmp  expected:&lt;200&gt; but was:&lt;1000&gt;</li></div><div><li>Test resulted in an unexpected exit</li></div><div><li>test timed out after 30000 milliseconds</li></div><div><li>Test resulted in an unexpected exit</li></div><div><li>After waiting the operation updatePipeline still has not taken effect on NN yet</li></div><div><li> Expected to find 'localhost:35955: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:35955: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:45621: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:45621: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:44167: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:44167: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li> Expected to find 'localhost:43865: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:43865: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:35040: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:35040: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:38379: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:38379: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li>create_write should create the file</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>TestSLSRunner catched exception from child thread (TaskRunner.TaskDefinition): [java.lang.reflect.UndeclaredThrowableException]</li></div><div><li>Unexpected number of YARN_CONTAINER_FINISHED event published. expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>
Argument(s) are different! Wanted:
fileSystem.delete(
    mockfs://foo/tmp/logs/me/logs/application_1521310977408_0002,
    true
);
-&gt; at org.apache.hadoop.yarn.logaggregation.TestAggregatedLogDeletionService.testRefreshLogRetentionSettings(TestAggregatedLogDeletionService.java:300)
Actual invocation has different arguments:
fileSystem.delete(
    mockfs://foo/tmp/logs/me/logs/application_1521310</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileControllerFactory.&lt;init&gt;(LogAggregationFileControllerFactory.java:68)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsPage$ContainersLogsBlock.&lt;init&gt;(ContainerLogsPage.java:100)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage.testContainerLogPageAcc</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreTestBase.testRemoveAttempt(RMStateStoreTestBase.java:672)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testRemoveAttempt(TestLev</li></div><div><li>expected null, but was:&lt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User@6cc7b4de&gt;</li></div><div><li>expected:&lt;1024&gt; but was:&lt;2048&gt;</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:621)
	at org.junit.Assert.assertNotNull(Assert.java:631)
	at org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken(TestDelegationTokenRenewer.java:1066)
	at sun.reflect.NativeMetho</li></div><div><li>java.net.BindException: Address already in use</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>length != 10(unixSymbolicPermission=-rw-r--r--.)</li></div><div><li>java.lang.IllegalArgumentException: length != 10(unixSymbolicPermission=-rw-r--r--.)</li></div><div><li>Failed: the number of failed blocks = 2 &gt; the number of parity blocks = 1</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-03-17 03:59:18,293

"datanode DomainSocketWatcher" daemon prio=5 tid=1677 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.hadoop.net.unix.DomainSocketWatcher.doPoll0(Native Method)
        at org.apache.hadoop.net.unix.DomainSocketWatcher.access$900(DomainSocketWatcher.java:52)
        at org.apache.hadoop.net.uni</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-03-17 03:23:47,381

"java.util.concurrent.ThreadPoolExecutor$Worker@61ca59d8[State = -1, empty queue]" daemon prio=5 tid=269 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.conc</li></div><div><li>
fSEditLog.logCancelDelegationToken(
    token for jenkins: HDFS_DELEGATION_TOKEN owner=jenkins, renewer=jenkins, realUser=, issueDate=1521301710715, maxDate=1521301714715, sequenceNumber=1, masterKeyId=1
);
Never wanted here:
-&gt; at org.apache.hadoop.hdfs.server.namenode.TestSecurityTokenEditLog.testEditsForCancelOnTokenExpire(TestSecurityTokenEditLog.java:235)
But invoked here:
-&gt; at org.apache.h</li></div><div><li>Test resulted in an unexpected exit</li></div><div><li>java.lang.AssertionError
	at org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics.testCapacityMetrics(TestNameNodeMetrics.java:219)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.refl</li></div><div><li>expected:&lt;3&gt; but was:&lt;2&gt;</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>initTable on TestDynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException: The request processing has failed because of an unknown error, exception or failure. (Service: AmazonDynamoDBv2; Status Code: 500; Error Code: InternalFailure; Request ID: 328f7315-4b78-47f3-b713-77a8ad76799f): The request processing has failed because of an unknown error, exception or failure.</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.yarn.sls.scheduler.TestTaskRunner.testPreStartQueueing(TestTaskRunner.java:244)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div><div><li>Unexpected number of YARN_CONTAINER_FINISHED event published. expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;2&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor150.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveC</li></div><div><li>expected:&lt;3&gt; but was:&lt;2&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileControllerFactory.&lt;init&gt;(LogAggregationFileControllerFactory.java:68)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsPage$ContainersLogsBlock.&lt;init&gt;(ContainerLogsPage.java:100)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage.testContainerLogPageAcc</li></div><div><li>test timed out after 30000 milliseconds</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to become active. Local node did not get an opportunity to do so from ZooKeeper, or the local node took too long to transition to active.</li></div><div><li>expected:&lt;7&gt; but was:&lt;4&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>Expected InvalidToken</li></div><div><li>Unexpected number of pending nodes expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>expected:&lt;9&gt; but was:&lt;8&gt;</li></div><div><li>test timed out after 30000 milliseconds</li></div><div><li>End of File Exception between local host is: "hj-ibmibm13055.persistent.com/10.53.16.233"; destination host is: "localhost":43465; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>test timed out after 5000 milliseconds</li></div><div><li>test timed out after 100000 milliseconds</li></div><div><li>test timed out after 10000 milliseconds</li></div><div><li>test timed out after 600000 milliseconds</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithJournalNodes(TestDFSUpgradeWithHA.java:687)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>After waiting the operation updatePipeline still has not taken effect on NN yet</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>The actual value 13 is not within the expected range: [5.60, 8.40].</li></div><div><li>Unexpected number of YARN_CONTAINER_FINISHED event published. expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>
Argument(s) are different! Wanted:
fileSystem.delete(
    mockfs://foo/tmp/logs/me/logs/application_1521308851583_0002,
    true
);
-&gt; at org.apache.hadoop.yarn.logaggregation.TestAggregatedLogDeletionService.testRefreshLogRetentionSettings(TestAggregatedLogDeletionService.java:300)
Actual invocation has different arguments:
fileSystem.delete(
    mockfs://foo/tmp/logs/me/logs/application_1521308</li></div><div><li>No A records in answer</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.junit.Assert.assertFalse(Assert.java:74)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync.testNMSentContainerStatusOnResync(TestNodeManagerResync.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Nati</li></div><div><li>Process is still alive!</li></div><div><li>ContainerState is not correct (timedout)</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileControllerFactory.&lt;init&gt;(LogAggregationFileControllerFactory.java:68)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsPage$ContainersLogsBlock.&lt;init&gt;(ContainerLogsPage.java:100)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage.testContainerLogPageAcc</li></div><div><li>
Wanted but not invoked:
adminService.transitionToStandby(&lt;any&gt;);
-&gt; at org.apache.hadoop.yarn.server.resourcemanager.TestRMEmbeddedElector.testCallbackSynchronizationNeutral(TestRMEmbeddedElector.java:215)
Actually, there were zero interactions with this mock.
</li></div><div><li>Y1 Used Resource should be 4 GB expected:&lt;4096&gt; but was:&lt;1024&gt;</li></div><div><li>expected:&lt;2048&gt; but was:&lt;4096&gt;</li></div><div><li>test timed out after 2000 milliseconds</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hdfs.server.balancer.TestBalancer.testBalancerWithStripedFile</div></li><li><div>org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks.testSetRepIncWithUnderReplicatedBlocks</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testVolumeFailure</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testSuccessiveVolumeFailures</div></li><li><div>org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore.org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.testFixedSizeThreadPool</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testStartMultipleContainers</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.security.TestGroupsCaching.testThreadNotBlockedWhenExpiredEntryExistsWithBackgroundRefresh</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notDir_local</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notReadable_local</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notWritable_local</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testDiskIoIgnoresTransientCreateErrors</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testDiskIoDetectsCreateErrors</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notListable</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notListable_local</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testCheckDir_normal_local</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testDiskIoDetectsWriteErrors</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notWritable</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testCheckDir_normal</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notDir</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notReadable</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testDiskIoIgnoresTransientWriteErrors</div></li><li><div>org.apache.hadoop.hdfs.TestDFSShell.testCopyCommandsWithPreserveOption</div></li><li><div>org.apache.hadoop.hdfs.TestHDFSFileSystemContract.testAppend</div></li><li><div>org.apache.hadoop.hdfs.TestLocalDFS.testWorkingDirectory</div></li><li><div>org.apache.hadoop.hdfs.TestMaintenanceState.testDecommissionDifferentNodeAfterMaintenances</div></li><li><div>org.apache.hadoop.hdfs.TestPersistBlocks.TestRestartDfsWithFlush</div></li><li><div>org.apache.hadoop.hdfs.TestPread.testPreadFailureWithChangedBlockLocations</div></li><li><div>org.apache.hadoop.hdfs.TestReconstructStripedFile.testRecoverAllParityBlocks</div></li><li><div>org.apache.hadoop.hdfs.security.TestDelegationTokenForProxyUser.testWebHdfsDoAs</div></li><li><div>org.apache.hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean.testStorageTypeStatsWhenStorageFailed</div></li><li><div>org.apache.hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean.testStorageTypeStatsJMX</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeErasureCodingMetrics.testReconstructionBytesPartialGroup2</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID.testUUIDRegeneration</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testUnderReplicationAfterVolFailure</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testVolumeFailureRecoveredByHotSwappingVolume</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testDataNodeReconfigureWithVolumeFailures</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner.testThrottling</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner.testDirectoryScannerInFederatedCluster</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestSpaceReservation.testTmpSpaceReserve</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestHAAppend.testMultipleAppendsDuringCatchupTailing</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.testCompleteFileAfterCrashFailover</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.testWriteOverGracefulFailoverWithDnFail</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA.testUpdatePipeline</div></li><li><div>org.apache.hadoop.hdfs.TestNNBench.testNNBenchCreateReadAndDelete</div></li><li><div>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMRTimelineEventHandling</div></li><li><div>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</div></li><li><div>org.apache.hadoop.yarn.sls.TestSLSRunner.testSimulatorRunning[Testing with: SYNTH, org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler, (nodeFile null)]</div></li><li><div>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[1]</div></li><li><div>org.apache.hadoop.yarn.logaggregation.TestAggregatedLogDeletionService.testRefreshLogRetentionSettings</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testRemoveAttempt</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testIncreaseContainerUnreservedWhenApplicationCompleted</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testExcessiveReservationWhenDecreaseSameContainer</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken</div></li><li><div>org.apache.hadoop.yarn.webapp.TestRMWithCSRFFilter.testNoCustomHeaderFromBrowser</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.io.nativeio.TestNativeIO.testStat</div></li><li><div>org.apache.hadoop.io.nativeio.TestNativeIO.testMultiThreadedStat</div></li><li><div>org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy.testCloseWithExceptionsInStreamer</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testUnderReplicationAfterVolFailure</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.TestNameNodeMetadataConsistency.testGenerationStampInFuture</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.TestSecurityTokenEditLog.testEditsForCancelOnTokenExpire</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestHAAppend.testMultipleAppendsDuringCatchupTailing</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics.testCapacityMetrics</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.web.resources.TestWebHdfsDataLocality.testExcludeDataNodes</div></li><li><div>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMRNewTimelineServiceEventHandling</div></li><li><div>org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore.org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore</div></li><li><div>org.apache.hadoop.yarn.sls.scheduler.TestTaskRunner.testPreStartQueueing</div></li><li><div>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2CustomizedFlow</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithSaslEncryption[0]</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithSaslEncryption[0]</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.testFixedSizeThreadPool</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestFSRMStateStore.testFSRMStateStoreClientRetry</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.ha.TestZKFailoverController.testGracefulFailoverMultipleZKfcs</div></li><li><div>org.apache.hadoop.ipc.TestIPC.testConnectionIdleTimeouts</div></li><li><div>org.apache.hadoop.security.TestGroupsCaching.testThreadNotBlockedWhenExpiredEntryExistsWithBackgroundRefresh</div></li><li><div>org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager.testMultiNodeOperations</div></li><li><div>org.apache.hadoop.hdfs.TestDecommission.testPendingNodes</div></li><li><div>org.apache.hadoop.hdfs.TestErasureCodingMultipleRacks.testSkewedRack2</div></li><li><div>org.apache.hadoop.hdfs.TestHDFSFileSystemContract.testAppend</div></li><li><div>org.apache.hadoop.hdfs.TestPersistBlocks.TestRestartDfsWithFlush</div></li><li><div>org.apache.hadoop.hdfs.security.TestDelegationTokenForProxyUser.testWebHdfsDoAs</div></li><li><div>org.apache.hadoop.hdfs.server.balancer.TestBalancerRPCDelay.testBalancerRPCDelay</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID.testUUIDRegeneration</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner.testThrottling</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithJournalNodes</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA.testUpdatePipeline</div></li><li><div>org.apache.hadoop.fs.azure.TestClientThrottlingAnalyzer.testManySuccessAndErrorsAndWaiting</div></li><li><div>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2</div></li><li><div>org.apache.hadoop.yarn.logaggregation.TestAggregatedLogDeletionService.testRefreshLogRetentionSettings</div></li><li><div>org.apache.hadoop.registry.server.dns.TestSecureRegistryDNS.testExternalCNAMERecord</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync.testNMSentContainerStatusOnResync</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeProcessFailure</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testStartMultipleContainers</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.TestRMEmbeddedElector.testCallbackSynchronization</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler.testCSReservationWithRootUnblocked</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testContainerIncreaseAllocationExpiration</div></li><li><div>org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.testDeleteReservationXML</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hbase"><div style="font-weight:bold;" class="panel-heading">HBASE</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>cbd8b15e6b65d891961df11ccde67ff705f1d210</div><div><b>Last Run: </b>30-03-2018 10:17 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 4555</div><div>Failed Count : 3</div><div>Skipped Count : 39</div></td><td><div>Total Count : 4570</div><div>Failed Count : 7</div><div>Skipped Count : 39</div></td><td><div>Total Count : 4555</div><div>Failed Count : 5</div><div>Skipped Count : 39</div></td><td><div>Total Count : 4567</div><div>Failed Count : 4</div><div>Skipped Count : 39</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testMasterSwitch</li></div><div><li>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testMetaMoveDuringClientZkClusterRestart</li></div><div><li>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testBasicOperation</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testMasterSwitch</li></div><div><li>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testMetaMoveDuringClientZkClusterRestart</li></div><div><li>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testBasicOperation</li></div><div><li>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.org.apache.hadoop.hbase.client.TestSeparateClientZKCluster</li></div><div><li>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.org.apache.hadoop.hbase.client.TestSeparateClientZKCluster</li></div><div><li>org.apache.hadoop.hbase.master.TestShutdownWithNoRegionServer.org.apache.hadoop.hbase.master.TestShutdownWithNoRegionServer</li></div><div><li>org.apache.hadoop.hbase.master.procedure.TestProcedureAdmin.testGetProcedure</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.TestClientOperationTimeout.testGetTimeout</li></div><div><li>org.apache.hadoop.hbase.TestClientOperationTimeout.testScanTimeout</li></div><div><li>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testMasterSwitch</li></div><div><li>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testMetaMoveDuringClientZkClusterRestart</li></div><div><li>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testBasicOperation</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testMetaMoveDuringClientZkClusterRestart</li></div><div><li>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testBasicOperation</li></div><div><li>org.apache.hadoop.hbase.master.procedure.TestModifyNamespaceProcedure.testRollbackAndDoubleExecution</li></div><div><li>org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.testRegionMerge</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testMasterSwitch(TestSeparateClientZKCluster.java:134)
</li></div><div><li>Failed after attempts=3, exceptions:
Fri Mar 30 12:24:28 UTC 2018, RpcRetryingCaller{globalStartTime=1522412668845, pause=100, maxAttempts=3}, org.apache.hadoop.hbase.MasterNotRunningException: org.apache.hadoop.hbase.ipc.ServerNotRunningYetException: org.apache.hadoop.hbase.ipc.ServerNotRunningYetException: Server is not running yet
 at org.apache.hadoop.hbase.master.HMaster.checkServiceStarted(H</li></div><div><li>Failed after attempts=3, exceptions:
Fri Mar 30 12:24:29 UTC 2018, RpcRetryingCaller{globalStartTime=1522412669296, pause=100, maxAttempts=3}, org.apache.hadoop.hbase.PleaseHoldException: org.apache.hadoop.hbase.PleaseHoldException: Master is initializing
 at org.apache.hadoop.hbase.master.HMaster.checkInitialized(HMaster.java:2771)
 at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.ja</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testMasterSwitch(TestSeparateClientZKCluster.java:134)
</li></div><div><li>Failed after attempts=3, exceptions:
Fri Mar 30 11:56:39 UTC 2018, RpcRetryingCaller{globalStartTime=1522410999128, pause=100, maxAttempts=3}, org.apache.hadoop.hbase.MasterNotRunningException: java.io.IOException: Call to abc600347152/172.17.0.2:39167 failed on local exception: org.apache.hadoop.hbase.ipc.FailedServerException: This server is in the failed servers list: abc600347152/172.17.0.2:39</li></div><div><li>Failed after attempts=3, exceptions:
Fri Mar 30 11:56:40 UTC 2018, RpcRetryingCaller{globalStartTime=1522410999978, pause=100, maxAttempts=3}, org.apache.hadoop.hbase.PleaseHoldException: org.apache.hadoop.hbase.PleaseHoldException: Master is initializing
 at org.apache.hadoop.hbase.master.HMaster.checkInitialized(HMaster.java:2771)
 at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.ja</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread NIOServerCxn.Factory:0.0.0.0/0.0.0.0:62331</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.TestShutdownWithNoRegionServer.tearDown(TestShutdownWithNoRegionServer.java:50)
</li></div><div><li>expected procedure result</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>callTimeout=500, callDuration=603: Call to 30b1bbf36cce/172.17.0.4:50958 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=18, waitTime=498, rpcTimeout=497 </li></div><div><li>callTimeout=500, callDuration=610: Call to 30b1bbf36cce/172.17.0.4:50958 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=20, waitTime=506, rpcTimeout=498 </li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testMasterSwitch(TestSeparateClientZKCluster.java:134)
</li></div><div><li>Failed after attempts=3, exceptions:
Fri Mar 30 12:31:12 UTC 2018, RpcRetryingCaller{globalStartTime=1522413072494, pause=100, maxAttempts=3}, org.apache.hadoop.hbase.MasterNotRunningException: java.io.IOException: Call to 30b1bbf36cce/172.17.0.4:46858 failed on local exception: org.apache.hadoop.hbase.ipc.FailedServerException: This server is in the failed servers list: 30b1bbf36cce/172.17.0.4:46</li></div><div><li>Failed after attempts=3, exceptions:
Fri Mar 30 12:31:12 UTC 2018, RpcRetryingCaller{globalStartTime=1522413072978, pause=100, maxAttempts=3}, org.apache.hadoop.hbase.PleaseHoldException: org.apache.hadoop.hbase.PleaseHoldException: Master is initializing
 at org.apache.hadoop.hbase.master.HMaster.checkInitialized(HMaster.java:2771)
 at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.ja</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>rit=OPEN, location=f54d2fdaf520,36010,1522412902384, table=hbase:meta, region=1588230740</li></div><div><li>Failed after attempts=3, exceptions:
Fri Mar 30 12:28:54 UTC 2018, RpcRetryingCaller{globalStartTime=1522412927715, pause=100, maxAttempts=3}, java.io.IOException: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/meta-region-server
Fri Mar 30 12:29:04 UTC 2018, RpcRetryingCaller{globalStartTime=1522412927715, pause=100, maxAttempts=3}, org.a</li></div><div><li>expected null, but was:&lt;bar&gt;</li></div><div><li>org.apache.hadoop.hbase.DoNotRetryIOException: 223d74c56cdef52ce8732ab38399fa1d NOT splittable
 at org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.checkSplittable(SplitTableRegionProcedure.java:182)
 at org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.&lt;init&gt;(SplitTableRegionProcedure.java:114)
 at org.apache.hadoop.hbase.master.assignment.AssignmentManager.cr</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.org.apache.hadoop.hbase.client.TestSeparateClientZKCluster</div></li><li><div>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.org.apache.hadoop.hbase.client.TestSeparateClientZKCluster</div></li><li><div>org.apache.hadoop.hbase.master.TestShutdownWithNoRegionServer.org.apache.hadoop.hbase.master.TestShutdownWithNoRegionServer</div></li><li><div>org.apache.hadoop.hbase.master.procedure.TestProcedureAdmin.testGetProcedure</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.TestClientOperationTimeout.testGetTimeout</div></li><li><div>org.apache.hadoop.hbase.TestClientOperationTimeout.testScanTimeout</div></li><li><div>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testMasterSwitch</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.master.procedure.TestModifyNamespaceProcedure.testRollbackAndDoubleExecution</div></li><li><div>org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.testRegionMerge</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hive"><div style="font-weight:bold;" class="panel-heading">HIVE</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>14bb998cfdac46f985c6d383df634cd569abc65b</div><div><b>Last Run: </b>29-03-2018 15:26 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 6893</div><div>Failed Count : 45</div><div>Skipped Count : 223</div></td><td><div>Total Count : 6893</div><div>Failed Count : 45</div><div>Skipped Count : 223</div></td><td><div>Total Count : 6893</div><div>Failed Count : 35</div><div>Skipped Count : 223</div></td><td><div>Total Count : 6893</div><div>Failed Count : 34</div><div>Skipped Count : 223</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</li></div><div><li>org.apache.hive.hcatalog.listener.TestNotificationListener.testAMQListener</li></div><div><li>org.apache.hive.hcatalog.listener.TestNotificationListener.testAMQListener</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testFileDumpCorruptDataFiles</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_DelimitedUGI</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testStreamBucketingMatchesRegularBucketing</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testBucketing</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchEmptyAbort</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testConcurrentTransactionBatchCommits</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchAbort</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_RegexUGI</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testAddPartition</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testBucketingWhereBucketColIsNotFirstCol</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchAbortAndCommit</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Regex</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testMultipleTransactionBatchCommits</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Delimited</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testFileDump</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTableValidation</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testEndpointConnection</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testFileDumpCorruptSideFiles</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testHeartbeat</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTimeOutReaper</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testErrorHandling</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchEmptyCommit</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testNoBuckets</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testInterleavedTransactionBatchCommits</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Json</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testRemainingTransactions</li></div><div><li>org.apache.hive.hcatalog.api.repl.commands.TestCommands.testMetadataReplEximCommands</li></div><div><li>org.apache.hive.hcatalog.api.repl.commands.TestCommands.testBasicReplEximCommands</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan1</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan2</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan3</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan4</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan5</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan6</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan1</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan2</li></div><div><li>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</li></div><div><li>org.apache.hive.hcatalog.listener.TestNotificationListener.testAMQListener</li></div><div><li>org.apache.hive.hcatalog.listener.TestNotificationListener.testAMQListener</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testFileDumpCorruptDataFiles</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_DelimitedUGI</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testStreamBucketingMatchesRegularBucketing</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testBucketing</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchEmptyAbort</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testConcurrentTransactionBatchCommits</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchAbort</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_RegexUGI</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testAddPartition</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testBucketingWhereBucketColIsNotFirstCol</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchAbortAndCommit</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Regex</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testMultipleTransactionBatchCommits</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Delimited</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testFileDump</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTableValidation</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testEndpointConnection</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testFileDumpCorruptSideFiles</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testHeartbeat</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTimeOutReaper</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testErrorHandling</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchEmptyCommit</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testNoBuckets</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testInterleavedTransactionBatchCommits</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Json</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testRemainingTransactions</li></div><div><li>org.apache.hive.hcatalog.api.repl.commands.TestCommands.testMetadataReplEximCommands</li></div><div><li>org.apache.hive.hcatalog.api.repl.commands.TestCommands.testBasicReplEximCommands</li></div><div><li>org.apache.hadoop.hive.llap.daemon.impl.comparator.TestAMReporter.testMultipleAM</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan1</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan2</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan3</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan4</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan5</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan6</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan1</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan2</li></div><div><li>org.apache.hadoop.hive.ql.exec.tez.TestDynamicPartitionPruner.testSingleSourceMultipleFiltersOrdering1</li></div><div><li>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</li></div><div><li>org.apache.hive.hcatalog.listener.TestNotificationListener.testAMQListener</li></div><div><li>org.apache.hive.hcatalog.listener.TestNotificationListener.testAMQListener</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testFileDumpCorruptDataFiles</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_DelimitedUGI</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testStreamBucketingMatchesRegularBucketing</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testBucketing</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchEmptyAbort</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testConcurrentTransactionBatchCommits</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchAbort</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_RegexUGI</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testAddPartition</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testBucketingWhereBucketColIsNotFirstCol</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchAbortAndCommit</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Regex</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testMultipleTransactionBatchCommits</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Delimited</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testFileDump</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTableValidation</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testEndpointConnection</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testFileDumpCorruptSideFiles</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testHeartbeat</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTimeOutReaper</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testErrorHandling</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchEmptyCommit</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testNoBuckets</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testInterleavedTransactionBatchCommits</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Json</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testRemainingTransactions</li></div><div><li>org.apache.hive.hcatalog.api.repl.commands.TestCommands.testMetadataReplEximCommands</li></div><div><li>org.apache.hive.hcatalog.api.repl.commands.TestCommands.testBasicReplEximCommands</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</li></div><div><li>org.apache.hive.hcatalog.listener.TestNotificationListener.testAMQListener</li></div><div><li>org.apache.hive.hcatalog.listener.TestNotificationListener.testAMQListener</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testFileDumpCorruptDataFiles</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_DelimitedUGI</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testStreamBucketingMatchesRegularBucketing</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testBucketing</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchEmptyAbort</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testConcurrentTransactionBatchCommits</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchAbort</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_RegexUGI</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testAddPartition</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testBucketingWhereBucketColIsNotFirstCol</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchAbortAndCommit</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Regex</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testMultipleTransactionBatchCommits</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Delimited</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testFileDump</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTableValidation</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testEndpointConnection</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testFileDumpCorruptSideFiles</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testHeartbeat</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTimeOutReaper</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testErrorHandling</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchEmptyCommit</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testNoBuckets</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testInterleavedTransactionBatchCommits</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Json</li></div><div><li>org.apache.hive.hcatalog.streaming.TestStreaming.testRemainingTransactions</li></div><div><li>org.apache.hive.hcatalog.api.repl.commands.TestCommands.testMetadataReplEximCommands</li></div><div><li>org.apache.hive.hcatalog.api.repl.commands.TestCommands.testBasicReplEximCommands</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat(TestHCatMultiOutputFormat.java:301)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>org.codehaus.jackson.JsonNode.asInt()I</li></div><div><li>org.codehaus.jackson.JsonNode.asInt()I</li></div><div><li>Table: mytbl is not found.</li></div><div><li>expected:&lt;[CREATE_DATABASE, CREATE_TABLE, ADD_PARTITION, ALTER_PARTITION, DROP_PARTITION, ALTER_TABLE, DROP_TABLE, DROP_DATABASE]&gt; but was:&lt;[CREATE_DATABASE]&gt;</li></div><div><li>org/joda/time/IllegalInstantException</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>40000:FAILED: RuntimeException MetaException(message:java.lang.NoClassDefFoundError org/joda/time/IllegalInstantException) expected:&lt;null&gt; but was:&lt;java.lang.RuntimeException: MetaException(message:java.lang.NoClassDefFoundError org/joda/time/IllegalInstantException)&gt;</li></div><div><li>40000:FAILED: RuntimeException MetaException(message:java.lang.NoClassDefFoundError Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory) expected:&lt;null&gt; but was:&lt;java.lang.RuntimeException: MetaException(message:java.lang.NoClassDefFoundError Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primiti</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat(TestHCatMultiOutputFormat.java:301)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>org.codehaus.jackson.JsonNode.asInt()I</li></div><div><li>org.codehaus.jackson.JsonNode.asInt()I</li></div><div><li>Table: mytbl is not found.</li></div><div><li>expected:&lt;[CREATE_DATABASE, CREATE_TABLE, ADD_PARTITION, ALTER_PARTITION, DROP_PARTITION, ALTER_TABLE, DROP_TABLE, DROP_DATABASE]&gt; but was:&lt;[CREATE_DATABASE]&gt;</li></div><div><li>org/joda/time/IllegalInstantException</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>40000:FAILED: RuntimeException MetaException(message:java.lang.NoClassDefFoundError org/joda/time/IllegalInstantException) expected:&lt;null&gt; but was:&lt;java.lang.RuntimeException: MetaException(message:java.lang.NoClassDefFoundError org/joda/time/IllegalInstantException)&gt;</li></div><div><li>40000:FAILED: RuntimeException MetaException(message:java.lang.NoClassDefFoundError Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory) expected:&lt;null&gt; but was:&lt;java.lang.RuntimeException: MetaException(message:java.lang.NoClassDefFoundError Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primiti</li></div><div><li>test timed out after 5000 milliseconds</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>test timed out after 5000 milliseconds</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.codehaus.jackson.JsonNode.asInt()I</li></div><div><li>org.codehaus.jackson.JsonNode.asInt()I</li></div><div><li>Table: mytbl is not found.</li></div><div><li>expected:&lt;[CREATE_DATABASE, CREATE_TABLE, ADD_PARTITION, ALTER_PARTITION, DROP_PARTITION, ALTER_TABLE, DROP_TABLE, DROP_DATABASE]&gt; but was:&lt;[CREATE_DATABASE]&gt;</li></div><div><li>org/joda/time/IllegalInstantException</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>40000:FAILED: RuntimeException MetaException(message:java.lang.NoClassDefFoundError org/joda/time/IllegalInstantException) expected:&lt;null&gt; but was:&lt;java.lang.RuntimeException: MetaException(message:java.lang.NoClassDefFoundError org/joda/time/IllegalInstantException)&gt;</li></div><div><li>40000:FAILED: RuntimeException MetaException(message:java.lang.NoClassDefFoundError Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory) expected:&lt;null&gt; but was:&lt;java.lang.RuntimeException: MetaException(message:java.lang.NoClassDefFoundError Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primiti</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.codehaus.jackson.JsonNode.asInt()I</li></div><div><li>org.codehaus.jackson.JsonNode.asInt()I</li></div><div><li>Table: mytbl is not found.</li></div><div><li>expected:&lt;[CREATE_DATABASE, CREATE_TABLE, ADD_PARTITION, ALTER_PARTITION, DROP_PARTITION, ALTER_TABLE, DROP_TABLE, DROP_DATABASE]&gt; but was:&lt;[CREATE_DATABASE]&gt;</li></div><div><li>org/joda/time/IllegalInstantException</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory</li></div><div><li>40000:FAILED: RuntimeException MetaException(message:java.lang.NoClassDefFoundError org/joda/time/IllegalInstantException) expected:&lt;null&gt; but was:&lt;java.lang.RuntimeException: MetaException(message:java.lang.NoClassDefFoundError org/joda/time/IllegalInstantException)&gt;</li></div><div><li>40000:FAILED: RuntimeException MetaException(message:java.lang.NoClassDefFoundError Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory) expected:&lt;null&gt; but was:&lt;java.lang.RuntimeException: MetaException(message:java.lang.NoClassDefFoundError Could not initialize class org.apache.hadoop.hive.serde2.lazy.objectinspector.primiti</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hive.llap.daemon.impl.comparator.TestAMReporter.testMultipleAM</div></li><li><div>org.apache.hadoop.hive.ql.exec.tez.TestDynamicPartitionPruner.testSingleSourceMultipleFiltersOrdering1</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="kafka"><div style="font-weight:bold;" class="panel-heading">KAFKA</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>29838c104249572cebac69c7d156dff4e1cc41f1</div><div><b>Last Run: </b>29-03-2018 20:00 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 8809</div><div>Failed Count : 1</div><div>Skipped Count : 7</div></td><td><div>Total Count : 8809</div><div>Failed Count : 5</div><div>Skipped Count : 7</div></td><td><div>Total Count : 8809</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td><td><div>Total Count : 8809</div><div>Failed Count : 1</div><div>Skipped Count : 7</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>kafka.server.OffsetCommitTest.testUpdateOffsets</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.kafka.common.network.SslSelectorTest.testCloseConnectionInClosingState</li></div><div><li>kafka.api.PlaintextConsumerTest.testPerPartitionLeadWithMaxPollRecords</li></div><div><li>kafka.controller.ControllerIntegrationTest.testControllerMoveIncrementsControllerEpoch</li></div><div><li>kafka.security.auth.SimpleAclAuthorizerTest.testHighConcurrencyModificationOfResourceAcls</li></div><div><li>kafka.server.DynamicBrokerReconfigurationTest.testAddRemoveSslListener</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.kafka.connect.runtime.WorkerTest.testConverterOverrides</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError: expected:&lt;NONE&gt; but was:&lt;UNKNOWN_MEMBER_ID&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError: Channel not expired expected null, but was:&lt;org.apache.kafka.common.network.KafkaChannel@4f&gt;</li></div><div><li>java.lang.AssertionError: The lead should be 10</li></div><div><li>java.util.NoSuchElementException: None.get</li></div><div><li>java.lang.AssertionError: expected acls Set(User:36 has Allow permission for operations: Read from hosts: *, User:7 has Allow permission for operations: Read from hosts: *, User:21 has Allow permission for operations: Read from hosts: *, User:39 has Allow permission for operations: Read from hosts: *, User:43 has Allow permission for operations: Read from hosts: *, User:3 has Allow permission for </li></div><div><li>java.util.concurrent.ExecutionException: java.lang.AssertionError: expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError: 
  Expectation failure on verify:
    WorkerSourceTask.run(): expected: 1, actual: 0</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>kafka.server.OffsetCommitTest.testUpdateOffsets</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.kafka.common.network.SslSelectorTest.testCloseConnectionInClosingState</div></li><li><div>kafka.api.PlaintextConsumerTest.testPerPartitionLeadWithMaxPollRecords</div></li><li><div>kafka.controller.ControllerIntegrationTest.testControllerMoveIncrementsControllerEpoch</div></li><li><div>kafka.security.auth.SimpleAclAuthorizerTest.testHighConcurrencyModificationOfResourceAcls</div></li><li><div>kafka.server.DynamicBrokerReconfigurationTest.testAddRemoveSslListener</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.kafka.connect.runtime.WorkerTest.testConverterOverrides</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="knox"><div style="font-weight:bold;" class="panel-heading">KNOX</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>c8a58d3c73d9ea2afe85cb030f5d7bb1d1ad0188</div><div><b>Last Run: </b>29-03-2018 12:55 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 959</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 958</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 959</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 959</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.knox.gateway.Knox242FuncTest.org.apache.knox.gateway.Knox242FuncTest</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timed out 10000 waiting for URL http://localhost:35366/gateway/testdg-cluster/test-service-path/test-service-resource</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.knox.gateway.Knox242FuncTest.org.apache.knox.gateway.Knox242FuncTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="metron"><div style="font-weight:bold;" class="panel-heading">METRON</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>9e95d4b61410c8033f0a4ea51e831566d5d933d3</div><div><b>Last Run: </b>02-04-2018 06:16 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1670</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1670</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1670</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1670</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="oozie"><div style="font-weight:bold;" class="panel-heading">OOZIE</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>af69c8c9f85f29dfbf7f798052a49b4c40e647e8</div><div><b>Last Run: </b>30-03-2018 03:54 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 2107</div><div>Failed Count : 33</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2107</div><div>Failed Count : 33</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2107</div><div>Failed Count : 3</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2107</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCredentialsSkip</li></div><div><li>org.apache.oozie.jms.TestDefaultConnectionContext.testThreadLocalSession</li></div><div><li>org.apache.oozie.jms.TestHCatMessageHandler.testDropEventTypeMessage</li></div><div><li>org.apache.oozie.jms.TestHCatMessageHandler.testCacheUpdateByMessage</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobSuccessEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuspendEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsAnd</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobFailureEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testConnectionDrop</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsOr</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuccessEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionWaitingEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobFailureEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionStartEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobStartedEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMissEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMissEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMissEvent</li></div><div><li>org.apache.oozie.service.TestJMSAccessorService.testConnectionRetry</li></div><div><li>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testPartitionDependency</li></div><div><li>org.apache.oozie.util.graph.TestGraphGenerator.testGraphWithManyNodes</li></div><div><li>org.apache.oozie.util.graph.TestGraphGenerator.testSimpleGraphPng</li></div><div><li>org.apache.oozie.util.graph.TestGraphGenerator.testSimpleGraphSvg</li></div><div><li>org.apache.oozie.util.graph.TestGraphGenerator.testGraphWithDecisionForkJoin</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCredentialsSkip</li></div><div><li>org.apache.oozie.command.TestXCommand.testXCommandLifecycleLockingFailingToLock</li></div><div><li>org.apache.oozie.command.wf.TestActionCheckXCommand.testActionCheck</li></div><div><li>org.apache.oozie.command.wf.TestWorkflowActionRetryInfoXCommand.testRetryConsoleUrlForked</li></div><div><li>org.apache.oozie.jms.TestDefaultConnectionContext.testThreadLocalSession</li></div><div><li>org.apache.oozie.jms.TestHCatMessageHandler.testDropEventTypeMessage</li></div><div><li>org.apache.oozie.jms.TestHCatMessageHandler.testCacheUpdateByMessage</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobSuccessEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuspendEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsAnd</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobFailureEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testConnectionDrop</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsOr</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuccessEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionWaitingEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobFailureEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionStartEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobStartedEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMissEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMissEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMissEvent</li></div><div><li>org.apache.oozie.service.TestJMSAccessorService.testConnectionRetry</li></div><div><li>org.apache.oozie.service.TestPartitionDependencyManagerService.testPartitionDependency</li></div><div><li>org.apache.oozie.action.hadoop.TestSqoopActionExecutor.testSqoopActionWithRedundantArgsAndFreeFormQuery</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.action.hadoop.TestHiveActionExecutor.testHiveAction</li></div><div><li>org.apache.oozie.action.hadoop.TestHive2ActionExecutor.testHive2Action</li></div><div><li>org.apache.oozie.action.hadoop.TestSqoopActionExecutor.testSqoopActionWithArgsAndFreeFormQuery</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandDate</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>JA020: Could not load credentials of type [abc] with name [abcname]]; perhaps it was not defined in oozie-site.xml?</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestDefaultConnectionContext.testThreadLocalSession(TestDefaultConnectionContext.java:74)
</li></div><div><li>Could not create Transport. Reason: javax.management.InstanceAlreadyExistsException: org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>Could not create Transport. Reason: javax.management.InstanceAlreadyExistsException: org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectors(TestJMSJobEventListener.java:544)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectorsNegative(TestJMSJobEventListener.java:568)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectors(TestJMSJobEventListener.java:239)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobSuccessEvent(TestJMSJobEventListener.java:477)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuspendEvent(TestJMSJobEventListener.java:214)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsAnd(TestJMSJobEventListener.java:316)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobFailureEvent(TestJMSJobEventListener.java:517)
</li></div><div><li>org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsNegative(TestJMSJobEventListener.java:262)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsOr(TestJMSJobEventListener.java:289)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuccessEvent(TestJMSJobEventListener.java:143)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionWaitingEvent(TestJMSJobEventListener.java:403)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobFailureEvent(TestJMSJobEventListener.java:180)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionStartEvent(TestJMSJobEventListener.java:439)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobStartedEvent(TestJMSJobEventListener.java:108)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMetEvent(TestJMSSLAEventListener.java:382)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMetEvent(TestJMSSLAEventListener.java:292)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectorsNegative(TestJMSSLAEventListener.java:261)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMetEvent(TestJMSSLAEventListener.java:332)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMissEvent(TestJMSSLAEventListener.java:103)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectors(TestJMSSLAEventListener.java:231)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMissEvent(TestJMSSLAEventListener.java:143)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMissEvent(TestJMSSLAEventListener.java:191)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.service.TestJMSAccessorService.testConnectionRetry(TestJMSAccessorService.java:183)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testPartitionDependency(TestPartitionDependencyManagerEhcache.java:47)
</li></div><div><li>java.util.concurrent.ExecutionException: java.lang.NullPointerException</li></div><div><li>Render and write PNG failed for graph-workflow-simple.xml: java.util.concurrent.ExecutionException: guru.nidi.graphviz.engine.GraphvizException: Initializing graphviz engine took too long.</li></div><div><li>Render and write SVG failed: java.util.concurrent.ExecutionException: guru.nidi.graphviz.engine.GraphvizException: Initializing graphviz engine took too long.</li></div><div><li>java.util.concurrent.ExecutionException: guru.nidi.graphviz.engine.GraphvizException: Initializing graphviz engine took too long.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>JA020: Could not load credentials of type [abc] with name [abcname]]; perhaps it was not defined in oozie-site.xml?</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.command.TestXCommand.testXCommandLifecycleLockingFailingToLock(TestXCommand.java:186)
</li></div><div><li>YARN App state for app application_1522399584429_0068 expected:&lt;FINISHED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.command.wf.TestWorkflowActionRetryInfoXCommand.validateRetryConsoleUrl(TestWorkflowActionRetryInfoXCommand.java:190)
	at org.apache.oozie.command.wf.TestWorkflowActionRetryInfoXCommand.testRetryConsoleUrlForked(TestWorkflowActionRetryInfoXCommand.java:125)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestDefaultConnectionContext.testThreadLocalSession(TestDefaultConnectionContext.java:74)
</li></div><div><li>Could not create Transport. Reason: javax.management.InstanceAlreadyExistsException: org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>Could not create Transport. Reason: javax.management.InstanceAlreadyExistsException: org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectors(TestJMSJobEventListener.java:544)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectorsNegative(TestJMSJobEventListener.java:568)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectors(TestJMSJobEventListener.java:239)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobSuccessEvent(TestJMSJobEventListener.java:477)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuspendEvent(TestJMSJobEventListener.java:214)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsAnd(TestJMSJobEventListener.java:316)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobFailureEvent(TestJMSJobEventListener.java:517)
</li></div><div><li>org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsNegative(TestJMSJobEventListener.java:262)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsOr(TestJMSJobEventListener.java:289)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuccessEvent(TestJMSJobEventListener.java:143)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionWaitingEvent(TestJMSJobEventListener.java:403)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobFailureEvent(TestJMSJobEventListener.java:180)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionStartEvent(TestJMSJobEventListener.java:439)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobStartedEvent(TestJMSJobEventListener.java:108)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMetEvent(TestJMSSLAEventListener.java:382)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMetEvent(TestJMSSLAEventListener.java:292)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectorsNegative(TestJMSSLAEventListener.java:261)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMetEvent(TestJMSSLAEventListener.java:332)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMissEvent(TestJMSSLAEventListener.java:103)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectors(TestJMSSLAEventListener.java:231)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMissEvent(TestJMSSLAEventListener.java:143)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMissEvent(TestJMSSLAEventListener.java:191)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.service.TestJMSAccessorService.testConnectionRetry(TestJMSAccessorService.java:183)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.service.TestPartitionDependencyManagerService.testPartitionDependency(TestPartitionDependencyManagerService.java:95)
</li></div><div><li>YARN App state for app application_1522406026898_0001 expected:&lt;FINISHED&gt; but was:&lt;RUNNING&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>YARN App state for app application_1522391041739_0002 expected:&lt;FINISHED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>YARN App state for app application_1522391260861_0003 expected:&lt;FINISHED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>YARN App state for app application_1522391562837_0003 expected:&lt;FINISHED&gt; but was:&lt;RUNNING&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;KILLED&gt; but was:&lt;RUNNING&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testPartitionDependency</div></li><li><div>org.apache.oozie.util.graph.TestGraphGenerator.testGraphWithManyNodes</div></li><li><div>org.apache.oozie.util.graph.TestGraphGenerator.testSimpleGraphPng</div></li><li><div>org.apache.oozie.util.graph.TestGraphGenerator.testSimpleGraphSvg</div></li><li><div>org.apache.oozie.util.graph.TestGraphGenerator.testGraphWithDecisionForkJoin</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.command.TestXCommand.testXCommandLifecycleLockingFailingToLock</div></li><li><div>org.apache.oozie.command.wf.TestActionCheckXCommand.testActionCheck</div></li><li><div>org.apache.oozie.command.wf.TestWorkflowActionRetryInfoXCommand.testRetryConsoleUrlForked</div></li><li><div>org.apache.oozie.service.TestPartitionDependencyManagerService.testPartitionDependency</div></li><li><div>org.apache.oozie.action.hadoop.TestSqoopActionExecutor.testSqoopActionWithRedundantArgsAndFreeFormQuery</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.action.hadoop.TestHiveActionExecutor.testHiveAction</div></li><li><div>org.apache.oozie.action.hadoop.TestHive2ActionExecutor.testHive2Action</div></li><li><div>org.apache.oozie.action.hadoop.TestSqoopActionExecutor.testSqoopActionWithArgsAndFreeFormQuery</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandDate</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="phoenix"><div style="font-weight:bold;" class="panel-heading">PHOENIX</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>ec92677218607ce575ec6c189f8a2f2a37369f99</div><div><b>Last Run: </b>29-03-2018 23:45 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1683</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1683</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1683</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1683</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="pig"><div style="font-weight:bold;" class="panel-heading">PIG</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>1fcd7196e21117eb4c365f26adc19210f63fbdec</div><div><b>Last Run: </b>29-03-2018 12:56 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 895</div><div>Failed Count : 20</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 20</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestBuiltin.testRANDOMWithJob</li></div><div><li>org.apache.pig.test.TestBuiltin.testSFPig</li></div><div><li>org.apache.pig.test.TestBuiltin.testUniqueID</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString3</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString5</li></div><div><li>org.apache.pig.test.TestLoad.testLoadRemoteAbsScheme</li></div><div><li>org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194</li></div><div><li>org.apache.pig.test.TestPigServer.testRegisterRemoteScript</li></div><div><li>org.apache.pig.test.TestSchema.testEnabledDisambiguationPassesForDupeAliases</li></div><div><li>org.apache.pig.test.TestSchema.testSchemaSerialization</li></div><div><li>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColonsForNestedSchema</li></div><div><li>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColons</li></div><div><li>org.apache.pig.test.TestStore.testStore</li></div><div><li>org.apache.pig.test.TestStore.testStoreComplexDataWithNull</li></div><div><li>org.apache.pig.test.TestStore.testBinStorageGetSchema</li></div><div><li>org.apache.pig.test.TestStore.testStoreComplexData</li></div><div><li>org.apache.pig.test.TestStore.testSetStoreSchema</li></div><div><li>org.apache.pig.test.TestStore.testSuccessFileCreation1</li></div><div><li>org.apache.pig.test.TestStore.testCleanupOnFailureMultiStore</li></div><div><li>org.apache.pig.test.TestStore.testEmptyPartFileCreation</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestBuiltin.testRANDOMWithJob</li></div><div><li>org.apache.pig.test.TestBuiltin.testSFPig</li></div><div><li>org.apache.pig.test.TestBuiltin.testUniqueID</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString3</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString5</li></div><div><li>org.apache.pig.test.TestLoad.testLoadRemoteAbsScheme</li></div><div><li>org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194</li></div><div><li>org.apache.pig.test.TestPigServer.testRegisterRemoteScript</li></div><div><li>org.apache.pig.test.TestSchema.testEnabledDisambiguationPassesForDupeAliases</li></div><div><li>org.apache.pig.test.TestSchema.testSchemaSerialization</li></div><div><li>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColonsForNestedSchema</li></div><div><li>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColons</li></div><div><li>org.apache.pig.test.TestStore.testStore</li></div><div><li>org.apache.pig.test.TestStore.testStoreComplexDataWithNull</li></div><div><li>org.apache.pig.test.TestStore.testBinStorageGetSchema</li></div><div><li>org.apache.pig.test.TestStore.testStoreComplexData</li></div><div><li>org.apache.pig.test.TestStore.testSetStoreSchema</li></div><div><li>org.apache.pig.test.TestStore.testSuccessFileCreation1</li></div><div><li>org.apache.pig.test.TestStore.testCleanupOnFailureMultiStore</li></div><div><li>org.apache.pig.test.TestStore.testEmptyPartFileCreation</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Unable to open iterator for alias B</li></div><div><li>Input path does not exist: hdfs://localhost:41172/user/jenkins/testSFPig-output.txt</li></div><div><li>Unable to open iterator for alias B</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194(TestLocalRearrange.java:231)
</li></div><div><li>Unable to open iterator for alias b</li></div><div><li>Unable to open iterator for alias C</li></div><div><li>Unable to open iterator for alias c</li></div><div><li>Unable to open iterator for alias F</li></div><div><li>Unable to open iterator for alias E</li></div><div><li>File /tmp/TestStore/TestStore-output-2736506001583525406.txt does not exist.</li></div><div><li>File /tmp/TestStore/TestStore-output--3451905889245939375.txt does not exist.</li></div><div><li>Checking binstorage getSchema output</li></div><div><li>File /tmp/TestStore/TestStore-output--4740349585116633701.txt does not exist.</li></div><div><li>Checking if file /tmp/TestStore/_commitJob_called does  exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Checking if _SUCCESS file exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Checking if file /tmp/TestStore/_setupTask_called1 does  exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>File /tmp/TestStore/TestStore-output--5564787371570102932.txt_1 does not exist.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to open iterator for alias B</li></div><div><li>Input path does not exist: hdfs://localhost:36698/user/jenkins/testSFPig-output.txt</li></div><div><li>Unable to open iterator for alias B</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194(TestLocalRearrange.java:231)
</li></div><div><li>Unable to open iterator for alias b</li></div><div><li>Unable to open iterator for alias C</li></div><div><li>Unable to open iterator for alias c</li></div><div><li>Unable to open iterator for alias F</li></div><div><li>Unable to open iterator for alias E</li></div><div><li>File /tmp/TestStore/TestStore-output--9020002838664822760.txt does not exist.</li></div><div><li>File /tmp/TestStore/TestStore-output-8452997934844139370.txt does not exist.</li></div><div><li>Checking binstorage getSchema output</li></div><div><li>File /tmp/TestStore/TestStore-output--8778254149535406001.txt does not exist.</li></div><div><li>Checking if file /tmp/TestStore/_commitJob_called does  exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Checking if _SUCCESS file exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Checking if file /tmp/TestStore/_setupTask_called1 does  exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>File /tmp/TestStore/TestStore-output--4866496879033518520.txt_1 does not exist.</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="ranger"><div style="font-weight:bold;" class="panel-heading">RANGER</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>ceaed3f3dd590078a59ca9007fec3930542b6152</div><div><b>Last Run: </b>29-03-2018 12:54 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1070</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1070</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1070</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1070</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="slider"><div style="font-weight:bold;" class="panel-heading">SLIDER</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/develop</div><div><b>Last Revision: </b>4032999f35db4877b6b8ffc5e97a59837e22365b</div><div><b>Last Run: </b>29-03-2018 20:54 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 607</div><div>Failed Count : 24</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 24</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.slider.agent.actions.TestActionExists.testExistsLiveCluster</li></div><div><li>org.apache.slider.agent.actions.TestActionList.testActionListSuite</li></div><div><li>org.apache.slider.agent.actions.TestActionStatus.testSuite</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeCommands.testFreezeCommands</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeThawFlexStandaloneAM.testFreezeThawFlexStandaloneAM</li></div><div><li>org.apache.slider.agent.rest.TestStandaloneREST.testStandaloneREST</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testBuildCluster</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testUpdateCluster</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMDestroy.testStandaloneAMDestroy</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMKill.testKillStandaloneAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithDefaultRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestart</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAgentAM.testStandaloneAgentAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneYarnRegistryAM.testStandaloneYarnRegistryAM</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsNoAppContainer</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsWithAppPackage</li></div><div><li>org.apache.slider.client.TestUpgradeCommandOptions.testAll</li></div><div><li>org.apache.slider.providers.agent.TestAddonPackage.testEchoApplicationAddPackage</li></div><div><li>org.apache.slider.providers.agent.TestAgentAAEcho.testAgentEcho</li></div><div><li>org.apache.slider.providers.agent.TestAgentAMManagementWS.testAgentAMManagementWS</li></div><div><li>org.apache.slider.providers.agent.TestAgentEcho.testAgentEcho</li></div><div><li>org.apache.slider.server.appmaster.TestDelayInContainerLaunch.testDelayInContainerLaunch</li></div><div><li>org.apache.slider.server.appmaster.web.rest.publisher.TestPublisherRestResources.testRestURIs</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.slider.agent.actions.TestActionExists.testExistsLiveCluster</li></div><div><li>org.apache.slider.agent.actions.TestActionList.testActionListSuite</li></div><div><li>org.apache.slider.agent.actions.TestActionStatus.testSuite</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeCommands.testFreezeCommands</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeThawFlexStandaloneAM.testFreezeThawFlexStandaloneAM</li></div><div><li>org.apache.slider.agent.rest.TestStandaloneREST.testStandaloneREST</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testBuildCluster</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testUpdateCluster</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMDestroy.testStandaloneAMDestroy</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMKill.testKillStandaloneAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithDefaultRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestart</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAgentAM.testStandaloneAgentAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneYarnRegistryAM.testStandaloneYarnRegistryAM</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsNoAppContainer</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsWithAppPackage</li></div><div><li>org.apache.slider.client.TestUpgradeCommandOptions.testAll</li></div><div><li>org.apache.slider.providers.agent.TestAddonPackage.testEchoApplicationAddPackage</li></div><div><li>org.apache.slider.providers.agent.TestAgentAAEcho.testAgentEcho</li></div><div><li>org.apache.slider.providers.agent.TestAgentAMManagementWS.testAgentAMManagementWS</li></div><div><li>org.apache.slider.providers.agent.TestAgentEcho.testAgentEcho</li></div><div><li>org.apache.slider.server.appmaster.TestDelayInContainerLaunch.testDelayInContainerLaunch</li></div><div><li>org.apache.slider.server.appmaster.web.rest.publisher.TestPublisherRestResources.testRestURIs</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Launch failed with exit code -1</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Application not running: application_1522368676293_0001 state=FAILED </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Failed on local exception: java.io.FileNotFoundException: http://bf3b6e9bafd9:42610/cluster/app/application_1522368224830_0001; Host Details : local host is: "localhost"; destination host is: "http://bf3b6e9bafd9:42610/cluster/app/application_1522368224830_0001":42610; </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>assert report.yarnApplicationState == YarnApplicationState.RUNNING
       |      |                                            |
       |      FAILED                                       RUNNING
       applicationId { id: 1 cluster_timestamp: 1522368619804 } user: "jenkins" queue: "default" name: "testkillstandaloneam" host: "N/A" rpc_port: -1 yarn_application_state: FAILED trackingUrl: "http://bf</li></div><div><li>Cluster teststandaloneamrestartwithdefaultretrywindow not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestart not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestartwithretrywindow not live after 30000 ms</li></div><div><li>assert uri.port in 60000..60010
       |   |            |
       |   42104        [60000, 60001, 60002, 60003, 60004, 60005, 60006, 60007, 60008, 60009, 60010]
       http://bf3b6e9bafd9:42104/cluster/app/application_1522368474741_0001</li></div><div><li>Application not running: application_1522368498635_0001 state=FAILED </li></div><div><li>assert 0 == status
         |  |
         |  -1
         false</li></div><div><li>Launch failed with exit code -1</li></div><div><li>Upgrade command should have failed</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Launch failed with exit code -1</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Application not running: application_1522391331473_0001 state=FAILED </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Lexing failed on line: 1, column: 1, while reading '&lt;', no possible valid JSON value or punctuation could be recognized.</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>assert report.yarnApplicationState == YarnApplicationState.RUNNING
       |      |                                            |
       |      FAILED                                       RUNNING
       applicationId { id: 1 cluster_timestamp: 1522390947182 } user: "jenkins" queue: "default" name: "testkillstandaloneam" host: "N/A" rpc_port: -1 yarn_application_state: FAILED trackingUrl: "http://56</li></div><div><li>Cluster teststandaloneamrestartwithdefaultretrywindow not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestart not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestartwithretrywindow not live after 30000 ms</li></div><div><li>assert uri.port in 60000..60010
       |   |            |
       |   45970        [60000, 60001, 60002, 60003, 60004, 60005, 60006, 60007, 60008, 60009, 60010]
       http://56a645899214:45970/cluster/app/application_1522390920167_0001</li></div><div><li>Application not running: application_1522391001988_0001 state=FAILED </li></div><div><li>assert 0 == status
         |  |
         |  -1
         false</li></div><div><li>Launch failed with exit code -1</li></div><div><li>Upgrade command should have failed</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="spark"><div style="font-weight:bold;" class="panel-heading">SPARK</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>b02e76cbffe9e589b7a4e60f91250ca12a4420b2</div><div><b>Last Run: </b>30-03-2018 11:25 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 13889</div><div>Failed Count : 2</div><div>Skipped Count : 668</div></td><td><div>Total Count : 15272</div><div>Failed Count : 1</div><div>Skipped Count : 674</div></td><td><div>Total Count : 13890</div><div>Failed Count : 4</div><div>Skipped Count : 668</div></td><td><div>Total Count : 15256</div><div>Failed Count : 629</div><div>Skipped Count : 674</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.sql.kafka010.KafkaMicroBatchV2SourceSuite.ensure stream-stream self-join generates only one offset in offset log</li></div><div><li>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.sql.streaming.continuous.ContinuousSuite.basic rate source</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.launcher.LauncherServerSuite.testAppHandleDisconnect</li></div><div><li>org.apache.spark.deploy.yarn.YarnShuffleAuthSuite.external shuffle service</li></div><div><li>org.apache.spark.sql.execution.metric.SQLMetricsSuite.SortMergeJoin(left-anti) metrics</li></div><div><li>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.launcher.LauncherServerSuite.testAppHandleDisconnect</li></div><div><li>org.apache.spark.sql.DataFrameRangeSuite.Cancelling stage in a query with Range.</li></div><div><li>org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite.driver side SQL metrics</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.success sanity check</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.hadoop configuration preserved</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: create client</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: createDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: createDatabase with null description</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: setCurrentDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: databaseExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: listDatabases</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: alterDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: dropDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: createTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: loadTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: tableExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getTableOption</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: alterTable(table: CatalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: alterTable(dbName: String, tableName: String, table: CatalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: alterTable - rename</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: alterTable - change database</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: alterTable - change database and table names</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: listTables(database)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: listTables(database, pattern)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: dropTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: sql create partitioned table</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: createPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getPartitionNames(catalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getPartitions(catalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getPartitionsByFilter</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getPartition</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getPartitions(db: String, table: String)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: loadPartition</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: loadDynamicPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: renamePartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: alterPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: dropPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: createFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: functionExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: renameFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: alterFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getFunctionOption</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: listFunctions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: dropFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: sql set command</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: sql create index and reset</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: version</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getConf</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: setOut</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: setInfo</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: setError</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: newSession</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: withHiveState and addJar</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: reset</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: CREATE TABLE AS SELECT</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: CREATE Partitioned TABLE AS SELECT</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: Delete the temporary staging directory and files after each insert</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: SPARK-13709: reading partitioned Avro table with nested schema</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: Decimal support of Avro Hive serde</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: read avro file containing decimal</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: SPARK-17920: Insert into/overwrite avro table</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: create client</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: createDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: createDatabase with null description</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: setCurrentDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: databaseExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: listDatabases</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: alterDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: dropDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: createTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: loadTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: tableExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getTableOption</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: alterTable(table: CatalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: alterTable(dbName: String, tableName: String, table: CatalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: alterTable - rename</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: alterTable - change database</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: alterTable - change database and table names</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: listTables(database)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: listTables(database, pattern)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: dropTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: sql create partitioned table</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: createPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getPartitionNames(catalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getPartitions(catalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getPartitionsByFilter</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getPartition</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getPartitions(db: String, table: String)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: loadPartition</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: loadDynamicPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: renamePartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: alterPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: dropPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: createFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: functionExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: renameFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: alterFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getFunctionOption</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: listFunctions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: dropFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: sql set command</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: sql create index and reset</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: version</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getConf</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: setOut</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: setInfo</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: setError</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: newSession</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: withHiveState and addJar</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: reset</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: CREATE TABLE AS SELECT</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: CREATE Partitioned TABLE AS SELECT</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: Delete the temporary staging directory and files after each insert</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: SPARK-13709: reading partitioned Avro table with nested schema</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: Decimal support of Avro Hive serde</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: read avro file containing decimal</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: SPARK-17920: Insert into/overwrite avro table</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: create client</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: createDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: createDatabase with null description</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: setCurrentDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: databaseExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: listDatabases</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: alterDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: dropDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: createTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: loadTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: tableExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getTableOption</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: alterTable(table: CatalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: alterTable(dbName: String, tableName: String, table: CatalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: alterTable - rename</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: alterTable - change database</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: alterTable - change database and table names</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: listTables(database)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: listTables(database, pattern)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: dropTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: sql create partitioned table</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: createPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getPartitionNames(catalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getPartitions(catalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getPartitionsByFilter</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getPartition</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getPartitions(db: String, table: String)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: loadPartition</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: loadDynamicPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: renamePartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: alterPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: dropPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: createFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: functionExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: renameFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: alterFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getFunctionOption</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: listFunctions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: dropFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: sql set command</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: sql create index and reset</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: version</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getConf</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: setOut</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: setInfo</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: setError</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: newSession</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: withHiveState and addJar</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: reset</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: CREATE TABLE AS SELECT</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: CREATE Partitioned TABLE AS SELECT</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: Delete the temporary staging directory and files after each insert</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: SPARK-13709: reading partitioned Avro table with nested schema</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: Decimal support of Avro Hive serde</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: read avro file containing decimal</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: SPARK-17920: Insert into/overwrite avro table</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: create client</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: createDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: createDatabase with null description</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: setCurrentDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: databaseExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: listDatabases</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: alterDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: dropDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: createTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: loadTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: tableExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getTableOption</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: alterTable(table: CatalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: alterTable(dbName: String, tableName: String, table: CatalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: alterTable - rename</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: alterTable - change database</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: alterTable - change database and table names</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: listTables(database)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: listTables(database, pattern)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: dropTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: sql create partitioned table</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: createPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getPartitionNames(catalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getPartitions(catalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getPartitionsByFilter</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getPartition</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getPartitions(db: String, table: String)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: loadPartition</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: loadDynamicPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: renamePartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: alterPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: dropPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: createFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: functionExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: renameFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: alterFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getFunctionOption</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: listFunctions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: dropFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: sql set command</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: sql create index and reset</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: version</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getConf</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: setOut</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: setInfo</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: setError</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: newSession</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: withHiveState and addJar</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: reset</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: CREATE TABLE AS SELECT</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: CREATE Partitioned TABLE AS SELECT</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: Delete the temporary staging directory and files after each insert</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: SPARK-13709: reading partitioned Avro table with nested schema</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: Decimal support of Avro Hive serde</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: read avro file containing decimal</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: SPARK-17920: Insert into/overwrite avro table</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: create client</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: createDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: createDatabase with null description</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: setCurrentDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: databaseExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: listDatabases</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: alterDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: dropDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: createTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: loadTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: tableExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getTableOption</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: alterTable(table: CatalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: alterTable(dbName: String, tableName: String, table: CatalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: alterTable - rename</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: alterTable - change database</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: alterTable - change database and table names</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: listTables(database)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: listTables(database, pattern)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: dropTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: sql create partitioned table</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: createPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getPartitionNames(catalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getPartitions(catalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getPartitionsByFilter</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getPartition</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getPartitions(db: String, table: String)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: loadPartition</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: loadDynamicPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: renamePartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: alterPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: dropPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: createFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: functionExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: renameFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: alterFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getFunctionOption</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: listFunctions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: dropFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: sql set command</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: sql create index and reset</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: version</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getConf</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: setOut</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: setInfo</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: setError</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: newSession</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: withHiveState and addJar</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: reset</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: CREATE TABLE AS SELECT</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: CREATE Partitioned TABLE AS SELECT</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: Delete the temporary staging directory and files after each insert</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: SPARK-13709: reading partitioned Avro table with nested schema</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: Decimal support of Avro Hive serde</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: read avro file containing decimal</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: SPARK-17920: Insert into/overwrite avro table</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: create client</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: createDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: createDatabase with null description</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: setCurrentDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: databaseExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: listDatabases</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: alterDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: dropDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: createTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: loadTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: tableExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getTableOption</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: alterTable(table: CatalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: alterTable(dbName: String, tableName: String, table: CatalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: alterTable - rename</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: alterTable - change database</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: alterTable - change database and table names</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: listTables(database)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: listTables(database, pattern)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: dropTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: sql create partitioned table</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: createPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getPartitionNames(catalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getPartitions(catalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getPartitionsByFilter</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getPartition</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getPartitions(db: String, table: String)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: loadPartition</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: loadDynamicPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: renamePartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: alterPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: dropPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: createFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: functionExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: renameFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: alterFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getFunctionOption</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: listFunctions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: dropFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: sql set command</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: sql create index and reset</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: version</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getConf</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: setOut</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: setInfo</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: setError</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: newSession</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: withHiveState and addJar</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: reset</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: CREATE TABLE AS SELECT</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: CREATE Partitioned TABLE AS SELECT</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: Delete the temporary staging directory and files after each insert</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: SPARK-13709: reading partitioned Avro table with nested schema</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: Decimal support of Avro Hive serde</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: read avro file containing decimal</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: SPARK-17920: Insert into/overwrite avro table</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: create client</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: createDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: createDatabase with null description</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: setCurrentDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: databaseExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: listDatabases</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: alterDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: dropDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: createTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: loadTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: tableExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getTableOption</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: alterTable(table: CatalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: alterTable(dbName: String, tableName: String, table: CatalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: alterTable - rename</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: alterTable - change database</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: alterTable - change database and table names</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: listTables(database)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: listTables(database, pattern)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: dropTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: sql create partitioned table</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: createPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getPartitionNames(catalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getPartitions(catalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getPartitionsByFilter</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getPartition</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getPartitions(db: String, table: String)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: loadPartition</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: loadDynamicPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: renamePartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: alterPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: dropPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: createFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: functionExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: renameFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: alterFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getFunctionOption</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: listFunctions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: dropFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: sql set command</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: sql create index and reset</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: version</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getConf</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: setOut</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: setInfo</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: setError</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: newSession</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: withHiveState and addJar</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: reset</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: CREATE TABLE AS SELECT</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: CREATE Partitioned TABLE AS SELECT</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: Delete the temporary staging directory and files after each insert</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: SPARK-13709: reading partitioned Avro table with nested schema</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: Decimal support of Avro Hive serde</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: read avro file containing decimal</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: SPARK-17920: Insert into/overwrite avro table</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: create client</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: createDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: createDatabase with null description</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: setCurrentDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: databaseExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: listDatabases</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: alterDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: dropDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: createTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: loadTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: tableExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getTableOption</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: alterTable(table: CatalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: alterTable(dbName: String, tableName: String, table: CatalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: alterTable - rename</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: alterTable - change database</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: alterTable - change database and table names</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: listTables(database)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: listTables(database, pattern)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: dropTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: sql create partitioned table</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: createPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getPartitionNames(catalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getPartitions(catalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getPartitionsByFilter</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getPartition</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getPartitions(db: String, table: String)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: loadPartition</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: loadDynamicPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: renamePartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: alterPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: dropPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: createFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: functionExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: renameFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: alterFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getFunctionOption</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: listFunctions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: dropFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: sql set command</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: sql create index and reset</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: version</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getConf</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: setOut</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: setInfo</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: setError</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: newSession</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: withHiveState and addJar</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: reset</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: CREATE TABLE AS SELECT</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: CREATE Partitioned TABLE AS SELECT</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: Delete the temporary staging directory and files after each insert</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: SPARK-13709: reading partitioned Avro table with nested schema</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: Decimal support of Avro Hive serde</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: read avro file containing decimal</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: SPARK-17920: Insert into/overwrite avro table</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: create client</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: createDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: createDatabase with null description</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: setCurrentDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: databaseExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: listDatabases</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: alterDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: dropDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: createTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: loadTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: tableExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getTableOption</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: alterTable(table: CatalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: alterTable(dbName: String, tableName: String, table: CatalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: alterTable - rename</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: alterTable - change database</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: alterTable - change database and table names</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: listTables(database)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: listTables(database, pattern)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: dropTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: sql create partitioned table</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: createPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getPartitionNames(catalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getPartitions(catalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getPartitionsByFilter</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getPartition</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getPartitions(db: String, table: String)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: loadPartition</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: loadDynamicPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: renamePartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: alterPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: dropPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: createFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: functionExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: renameFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: alterFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getFunctionOption</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: listFunctions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: dropFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: sql set command</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: sql create index and reset</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: version</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getConf</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: setOut</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: setInfo</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: setError</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: newSession</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: withHiveState and addJar</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: reset</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: CREATE TABLE AS SELECT</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: CREATE Partitioned TABLE AS SELECT</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: Delete the temporary staging directory and files after each insert</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: SPARK-13709: reading partitioned Avro table with nested schema</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: Decimal support of Avro Hive serde</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: read avro file containing decimal</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: SPARK-17920: Insert into/overwrite avro table</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: create client</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: createDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: createDatabase with null description</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: setCurrentDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: databaseExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: listDatabases</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: alterDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: dropDatabase</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: createTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: loadTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: tableExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getTableOption</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: alterTable(table: CatalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: alterTable(dbName: String, tableName: String, table: CatalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: alterTable - rename</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: alterTable - change database</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: alterTable - change database and table names</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: listTables(database)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: listTables(database, pattern)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: dropTable</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: sql create partitioned table</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: createPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getPartitionNames(catalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getPartitions(catalogTable)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getPartitionsByFilter</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getPartition</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getPartitions(db: String, table: String)</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: loadPartition</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: loadDynamicPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: renamePartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: alterPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: dropPartitions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: createFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: functionExists</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: renameFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: alterFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getFunctionOption</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: listFunctions</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: dropFunction</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: sql set command</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: sql create index and reset</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: version</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getConf</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: setOut</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: setInfo</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: setError</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: newSession</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: withHiveState and addJar</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: reset</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: CREATE TABLE AS SELECT</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: CREATE Partitioned TABLE AS SELECT</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: Delete the temporary staging directory and files after each insert</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: SPARK-13709: reading partitioned Avro table with nested schema</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: Decimal support of Avro Hive serde</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: read avro file containing decimal</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: SPARK-17920: Insert into/overwrite avro table</li></div><div><li>org.apache.spark.sql.hive.execution.Hive_2_1_DDLSuite.SPARK-21617: ALTER TABLE for non-compatible DataSource tables</li></div><div><li>org.apache.spark.sql.hive.execution.Hive_2_1_DDLSuite.SPARK-21617: ALTER TABLE for Hive-compatible DataSource tables</li></div><div><li>org.apache.spark.sql.hive.execution.Hive_2_1_DDLSuite.SPARK-21617: ALTER TABLE for Hive tables</li></div><div><li>org.apache.spark.sql.hive.execution.Hive_2_1_DDLSuite.SPARK-21617: ALTER TABLE with incompatible schema on Hive-compatible table</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>&amp;#010;&amp;#010;== Results ==&amp;#010;!== Correct Answer - 6 ==       == Spark Answer - 4 ==&amp;#010;!struct&lt;_1:int,_2:int,_3:int&gt;   struct&lt;key:int,value:int,value:int&gt;&amp;#010; [1,1,1]                        [1,1,1]&amp;#010;![1,1,6]                        [1,6,6]&amp;#010;![1,6,1]                        [2,2,2]&amp;#010;![1,6,6]                        [3,3,3]&amp;#010;![2,2,2]                        &amp;#010;![3,3,3]          </li></div><div><li>Timeout of './bin/spark-submit' '--class' 'org.apache.spark.sql.hive.SparkSQLConfTest' '--name' 'SparkSQLConfTest' '--master' 'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.hive.metastore.version=0.12' '--conf' 'spark.sql.hive.metastore.jars=maven' '--driver-java-options' '-Dderby.system.durability=test' 'file:/var/lib/jen</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>&amp;#010;Error while stopping stream: &amp;#010;query.exception() is not empty after clean stop: org.apache.spark.sql.streaming.StreamingQueryException: Writing job failed.&amp;#010;=== Streaming Query ===&amp;#010;Identifier: [id = 12f21daa-8d37-446f-a838-834d2c135b0a, runId = 2b3844ff-f86c-43ce-8900-5ef427e3e550]&amp;#010;Current Committed Offsets: {org.apache.spark.sql.execution.streaming.continuous.RateStreamCon</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Failed check after 97 tries: Expected error but message went through..</li></div><div><li>The code passed to eventually never returned normally. Attempted 130 times over 2.0024294913500005 minutes. Last failure message: handle.getState().isFinal() was false.</li></div><div><li>2 did not equal 1</li></div><div><li>Timeout of './bin/spark-submit' '--class' 'org.apache.spark.sql.hive.SparkSQLConfTest' '--name' 'SparkSQLConfTest' '--master' 'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.hive.metastore.version=0.12' '--conf' 'spark.sql.hive.metastore.jars=maven' '--driver-java-options' '-Dderby.system.durability=test' 'file:/var/lib/jen</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Failed check after 96 tries: Expected error but message went through..</li></div><div><li>Expected exception org.apache.spark.SparkException to be thrown, but no exception was thrown</li></div><div><li>java.util.NoSuchElementException was thrown.</li></div><div><li>[unresolved dependency: org.apache.hive#hive-metastore;1.2.2: not found, unresolved dependency: org.apache.hive#hive-exec;1.2.2: not found, unresolved dependency: org.apache.hive#hive-common;1.2.2: not found, unresolved dependency: org.apache.hive#hive-serde;1.2.2: not found, unresolved dependency: com.google.guava#guava;14.0.1: not found, unresolved dependency: org.apache.hadoop#hadoop-client;2.6</li></div><div><li>[unresolved dependency: org.apache.hive#hive-metastore;1.2.2: not found, unresolved dependency: org.apache.hive#hive-exec;1.2.2: not found, unresolved dependency: org.apache.hive#hive-common;1.2.2: not found, unresolved dependency: org.apache.hive#hive-serde;1.2.2: not found, unresolved dependency: com.google.guava#guava;14.0.1: not found, unresolved dependency: org.apache.hadoop#hadoop-client;2.6</li></div><div><li>[unresolved dependency: org.apache.hive#hive-metastore;0.12.0: not found, unresolved dependency: org.apache.hive#hive-exec;0.12.0: not found, unresolved dependency: org.apache.hive#hive-common;0.12.0: not found, unresolved dependency: org.apache.hive#hive-serde;0.12.0: not found, unresolved dependency: com.google.guava#guava;14.0.1: not found, unresolved dependency: org.apache.hadoop#hadoop-client</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Expected exception org.apache.spark.sql.AnalysisException to be thrown, but java.lang.NullPointerException was thrown</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Expected exception org.apache.spark.sql.catalyst.analysis.NoSuchPermanentFunctionException to be thrown, but java.lang.NullPointerException was thrown</li></div><div><li>Expected exception org.apache.spark.sql.catalyst.analysis.NoSuchPermanentFunctionException to be thrown, but java.lang.NullPointerException was thrown</li></div><div><li>Expected exception org.apache.spark.sql.catalyst.analysis.NoSuchPermanentFunctionException to be thrown, but java.lang.NullPointerException was thrown</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Expected exception org.apache.spark.sql.catalyst.analysis.NoSuchPermanentFunctionException to be thrown, but java.lang.NullPointerException was thrown</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>[unresolved dependency: org.apache.hive#hive-metastore;0.13.1: not found, unresolved dependency: org.apache.hive#hive-exec;0.13.1: not found, unresolved dependency: org.apache.hive#hive-common;0.13.1: not found, unresolved dependency: org.apache.hive#hive-serde;0.13.1: not found, unresolved dependency: com.google.guava#guava;14.0.1: not found, unresolved dependency: org.apache.hadoop#hadoop-client</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>[unresolved dependency: org.apache.calcite#calcite-core;1.3.0-incubating: not found, unresolved dependency: org.apache.calcite#calcite-avatica;1.3.0-incubating: not found, unresolved dependency: org.apache.hive#hive-metastore;0.14.0: not found, unresolved dependency: org.apache.hive#hive-exec;0.14.0: not found, unresolved dependency: org.apache.hive#hive-common;0.14.0: not found, unresolved depend</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>[unresolved dependency: org.apache.hive#hive-metastore;1.0.0: not found, unresolved dependency: org.apache.hive#hive-exec;1.0.0: not found, unresolved dependency: org.apache.hive#hive-common;1.0.0: not found, unresolved dependency: org.apache.hive#hive-serde;1.0.0: not found, unresolved dependency: com.google.guava#guava;14.0.1: not found, unresolved dependency: org.apache.hadoop#hadoop-client;2.6</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>[unresolved dependency: org.apache.hive#hive-metastore;1.1.0: not found, unresolved dependency: org.apache.hive#hive-exec;1.1.0: not found, unresolved dependency: org.apache.hive#hive-common;1.1.0: not found, unresolved dependency: org.apache.hive#hive-serde;1.1.0: not found, unresolved dependency: com.google.guava#guava;14.0.1: not found, unresolved dependency: org.apache.hadoop#hadoop-client;2.6</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>[unresolved dependency: org.apache.hive#hive-metastore;1.2.2: not found, unresolved dependency: org.apache.hive#hive-exec;1.2.2: not found, unresolved dependency: org.apache.hive#hive-common;1.2.2: not found, unresolved dependency: org.apache.hive#hive-serde;1.2.2: not found, unresolved dependency: com.google.guava#guava;14.0.1: not found, unresolved dependency: org.apache.hadoop#hadoop-client;2.6</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>[unresolved dependency: org.apache.hive#hive-metastore;2.0.1: not found, unresolved dependency: org.apache.hive#hive-exec;2.0.1: not found, unresolved dependency: org.apache.hive#hive-common;2.0.1: not found, unresolved dependency: org.apache.hive#hive-serde;2.0.1: not found, unresolved dependency: com.google.guava#guava;14.0.1: not found, unresolved dependency: org.apache.hadoop#hadoop-client;2.6</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>[unresolved dependency: org.apache.hive#hive-metastore;2.1.1: not found, unresolved dependency: org.apache.hive#hive-exec;2.1.1: not found, unresolved dependency: org.apache.hive#hive-common;2.1.1: not found, unresolved dependency: org.apache.hive#hive-serde;2.1.1: not found, unresolved dependency: com.google.guava#guava;14.0.1: not found, unresolved dependency: org.apache.hadoop#hadoop-client;2.6</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>[unresolved dependency: org.apache.hive#hive-metastore;2.2.0: not found, unresolved dependency: org.apache.hive#hive-exec;2.2.0: not found, unresolved dependency: org.apache.hive#hive-common;2.2.0: not found, unresolved dependency: org.apache.hive#hive-serde;2.2.0: not found, unresolved dependency: com.google.guava#guava;14.0.1: not found, unresolved dependency: org.apache.hadoop#hadoop-client;2.6</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>[unresolved dependency: org.apache.hive#hive-metastore;2.3.2: not found, unresolved dependency: org.apache.hive#hive-exec;2.3.2: not found, unresolved dependency: org.apache.hive#hive-common;2.3.2: not found, unresolved dependency: org.apache.hive#hive-serde;2.3.2: not found, unresolved dependency: com.google.guava#guava;14.0.1: not found, unresolved dependency: org.apache.hadoop#hadoop-client;2.6</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>[unresolved dependency: org.apache.hive#hive-metastore;2.1.1: not found, unresolved dependency: org.apache.hive#hive-exec;2.1.1: not found, unresolved dependency: org.apache.hive#hive-common;2.1.1: not found, unresolved dependency: org.apache.hive#hive-serde;2.1.1: not found, unresolved dependency: com.google.guava#guava;14.0.1: not found, unresolved dependency: org.apache.hadoop#hadoop-client;2.6</li></div><div><li>Table t1 already exists.;</li></div><div><li>Table or view 't1' already exists in database 'default';</li></div><div><li>"Table t1 already exists.;" did not contain "types incompatible with the existing columns"</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.sql.kafka010.KafkaMicroBatchV2SourceSuite.ensure stream-stream self-join generates only one offset in offset log</div></li><li><div>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.sql.streaming.continuous.ContinuousSuite.basic rate source</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.deploy.yarn.YarnShuffleAuthSuite.external shuffle service</div></li><li><div>org.apache.spark.sql.execution.metric.SQLMetricsSuite.SortMergeJoin(left-anti) metrics</div></li><li><div>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.sql.DataFrameRangeSuite.Cancelling stage in a query with Range.</div></li><li><div>org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite.driver side SQL metrics</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.success sanity check</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.hadoop configuration preserved</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: create client</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: createDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: createDatabase with null description</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: setCurrentDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: databaseExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: listDatabases</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: alterDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: dropDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: createTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: loadTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: tableExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getTableOption</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: alterTable(table: CatalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: alterTable(dbName: String, tableName: String, table: CatalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: alterTable - rename</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: alterTable - change database</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: alterTable - change database and table names</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: listTables(database)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: listTables(database, pattern)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: dropTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: sql create partitioned table</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: createPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getPartitionNames(catalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getPartitions(catalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getPartitionsByFilter</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getPartition</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getPartitions(db: String, table: String)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: loadPartition</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: loadDynamicPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: renamePartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: alterPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: dropPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: createFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: functionExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: renameFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: alterFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getFunctionOption</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: listFunctions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: dropFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: sql set command</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: sql create index and reset</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: version</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: getConf</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: setOut</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: setInfo</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: setError</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: newSession</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: withHiveState and addJar</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: reset</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: CREATE TABLE AS SELECT</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: CREATE Partitioned TABLE AS SELECT</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: Delete the temporary staging directory and files after each insert</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: SPARK-13709: reading partitioned Avro table with nested schema</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: Decimal support of Avro Hive serde</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: read avro file containing decimal</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: SPARK-17920: Insert into/overwrite avro table</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: create client</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: createDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: createDatabase with null description</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: setCurrentDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: databaseExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: listDatabases</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: alterDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: dropDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: createTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: loadTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: tableExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getTableOption</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: alterTable(table: CatalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: alterTable(dbName: String, tableName: String, table: CatalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: alterTable - rename</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: alterTable - change database</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: alterTable - change database and table names</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: listTables(database)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: listTables(database, pattern)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: dropTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: sql create partitioned table</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: createPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getPartitionNames(catalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getPartitions(catalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getPartitionsByFilter</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getPartition</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getPartitions(db: String, table: String)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: loadPartition</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: loadDynamicPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: renamePartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: alterPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: dropPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: createFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: functionExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: renameFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: alterFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getFunctionOption</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: listFunctions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: dropFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: sql set command</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: sql create index and reset</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: version</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: getConf</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: setOut</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: setInfo</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: setError</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: newSession</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: withHiveState and addJar</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: reset</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: CREATE TABLE AS SELECT</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: CREATE Partitioned TABLE AS SELECT</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: Delete the temporary staging directory and files after each insert</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: SPARK-13709: reading partitioned Avro table with nested schema</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: Decimal support of Avro Hive serde</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: read avro file containing decimal</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: SPARK-17920: Insert into/overwrite avro table</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: create client</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: createDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: createDatabase with null description</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: setCurrentDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: databaseExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: listDatabases</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: alterDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: dropDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: createTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: loadTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: tableExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getTableOption</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: alterTable(table: CatalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: alterTable(dbName: String, tableName: String, table: CatalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: alterTable - rename</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: alterTable - change database</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: alterTable - change database and table names</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: listTables(database)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: listTables(database, pattern)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: dropTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: sql create partitioned table</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: createPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getPartitionNames(catalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getPartitions(catalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getPartitionsByFilter</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getPartition</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getPartitions(db: String, table: String)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: loadPartition</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: loadDynamicPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: renamePartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: alterPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: dropPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: createFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: functionExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: renameFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: alterFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getFunctionOption</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: listFunctions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: dropFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: sql set command</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: sql create index and reset</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: version</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: getConf</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: setOut</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: setInfo</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: setError</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: newSession</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: withHiveState and addJar</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: reset</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: CREATE TABLE AS SELECT</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: CREATE Partitioned TABLE AS SELECT</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: Delete the temporary staging directory and files after each insert</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: SPARK-13709: reading partitioned Avro table with nested schema</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: Decimal support of Avro Hive serde</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: read avro file containing decimal</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: SPARK-17920: Insert into/overwrite avro table</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: create client</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: createDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: createDatabase with null description</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: setCurrentDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: databaseExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: listDatabases</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: alterDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: dropDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: createTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: loadTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: tableExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getTableOption</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: alterTable(table: CatalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: alterTable(dbName: String, tableName: String, table: CatalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: alterTable - rename</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: alterTable - change database</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: alterTable - change database and table names</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: listTables(database)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: listTables(database, pattern)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: dropTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: sql create partitioned table</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: createPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getPartitionNames(catalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getPartitions(catalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getPartitionsByFilter</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getPartition</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getPartitions(db: String, table: String)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: loadPartition</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: loadDynamicPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: renamePartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: alterPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: dropPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: createFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: functionExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: renameFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: alterFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getFunctionOption</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: listFunctions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: dropFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: sql set command</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: sql create index and reset</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: version</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: getConf</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: setOut</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: setInfo</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: setError</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: newSession</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: withHiveState and addJar</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: reset</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: CREATE TABLE AS SELECT</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: CREATE Partitioned TABLE AS SELECT</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: Delete the temporary staging directory and files after each insert</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: SPARK-13709: reading partitioned Avro table with nested schema</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: Decimal support of Avro Hive serde</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: read avro file containing decimal</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: SPARK-17920: Insert into/overwrite avro table</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: create client</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: createDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: createDatabase with null description</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: setCurrentDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: databaseExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: listDatabases</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: alterDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: dropDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: createTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: loadTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: tableExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getTableOption</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: alterTable(table: CatalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: alterTable(dbName: String, tableName: String, table: CatalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: alterTable - rename</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: alterTable - change database</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: alterTable - change database and table names</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: listTables(database)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: listTables(database, pattern)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: dropTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: sql create partitioned table</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: createPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getPartitionNames(catalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getPartitions(catalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getPartitionsByFilter</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getPartition</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getPartitions(db: String, table: String)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: loadPartition</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: loadDynamicPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: renamePartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: alterPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: dropPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: createFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: functionExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: renameFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: alterFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getFunctionOption</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: listFunctions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: dropFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: sql set command</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: sql create index and reset</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: version</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: getConf</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: setOut</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: setInfo</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: setError</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: newSession</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: withHiveState and addJar</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: reset</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: CREATE TABLE AS SELECT</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: CREATE Partitioned TABLE AS SELECT</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: Delete the temporary staging directory and files after each insert</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: SPARK-13709: reading partitioned Avro table with nested schema</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: Decimal support of Avro Hive serde</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: read avro file containing decimal</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: SPARK-17920: Insert into/overwrite avro table</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: create client</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: createDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: createDatabase with null description</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: setCurrentDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: databaseExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: listDatabases</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: alterDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: dropDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: createTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: loadTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: tableExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getTableOption</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: alterTable(table: CatalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: alterTable(dbName: String, tableName: String, table: CatalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: alterTable - rename</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: alterTable - change database</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: alterTable - change database and table names</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: listTables(database)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: listTables(database, pattern)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: dropTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: sql create partitioned table</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: createPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getPartitionNames(catalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getPartitions(catalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getPartitionsByFilter</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getPartition</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getPartitions(db: String, table: String)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: loadPartition</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: loadDynamicPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: renamePartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: alterPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: dropPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: createFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: functionExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: renameFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: alterFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getFunctionOption</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: listFunctions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: dropFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: sql set command</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: sql create index and reset</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: version</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: getConf</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: setOut</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: setInfo</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: setError</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: newSession</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: withHiveState and addJar</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: reset</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: CREATE TABLE AS SELECT</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: CREATE Partitioned TABLE AS SELECT</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: Delete the temporary staging directory and files after each insert</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: SPARK-13709: reading partitioned Avro table with nested schema</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: Decimal support of Avro Hive serde</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: read avro file containing decimal</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: SPARK-17920: Insert into/overwrite avro table</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: create client</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: createDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: createDatabase with null description</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: setCurrentDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: databaseExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: listDatabases</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: alterDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: dropDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: createTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: loadTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: tableExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getTableOption</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: alterTable(table: CatalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: alterTable(dbName: String, tableName: String, table: CatalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: alterTable - rename</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: alterTable - change database</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: alterTable - change database and table names</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: listTables(database)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: listTables(database, pattern)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: dropTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: sql create partitioned table</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: createPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getPartitionNames(catalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getPartitions(catalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getPartitionsByFilter</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getPartition</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getPartitions(db: String, table: String)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: loadPartition</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: loadDynamicPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: renamePartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: alterPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: dropPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: createFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: functionExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: renameFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: alterFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getFunctionOption</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: listFunctions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: dropFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: sql set command</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: sql create index and reset</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: version</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: getConf</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: setOut</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: setInfo</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: setError</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: newSession</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: withHiveState and addJar</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: reset</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: CREATE TABLE AS SELECT</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: CREATE Partitioned TABLE AS SELECT</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: Delete the temporary staging directory and files after each insert</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: SPARK-13709: reading partitioned Avro table with nested schema</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: Decimal support of Avro Hive serde</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: read avro file containing decimal</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: SPARK-17920: Insert into/overwrite avro table</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: create client</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: createDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: createDatabase with null description</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: setCurrentDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: databaseExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: listDatabases</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: alterDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: dropDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: createTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: loadTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: tableExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getTableOption</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: alterTable(table: CatalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: alterTable(dbName: String, tableName: String, table: CatalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: alterTable - rename</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: alterTable - change database</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: alterTable - change database and table names</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: listTables(database)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: listTables(database, pattern)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: dropTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: sql create partitioned table</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: createPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getPartitionNames(catalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getPartitions(catalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getPartitionsByFilter</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getPartition</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getPartitions(db: String, table: String)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: loadPartition</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: loadDynamicPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: renamePartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: alterPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: dropPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: createFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: functionExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: renameFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: alterFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getFunctionOption</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: listFunctions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: dropFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: sql set command</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: sql create index and reset</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: version</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: getConf</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: setOut</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: setInfo</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: setError</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: newSession</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: withHiveState and addJar</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: reset</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: CREATE TABLE AS SELECT</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: CREATE Partitioned TABLE AS SELECT</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: Delete the temporary staging directory and files after each insert</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: SPARK-13709: reading partitioned Avro table with nested schema</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: Decimal support of Avro Hive serde</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: read avro file containing decimal</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: SPARK-17920: Insert into/overwrite avro table</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: create client</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: createDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: createDatabase with null description</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: setCurrentDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: databaseExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: listDatabases</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: alterDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: dropDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: createTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: loadTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: tableExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getTableOption</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: alterTable(table: CatalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: alterTable(dbName: String, tableName: String, table: CatalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: alterTable - rename</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: alterTable - change database</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: alterTable - change database and table names</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: listTables(database)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: listTables(database, pattern)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: dropTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: sql create partitioned table</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: createPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getPartitionNames(catalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getPartitions(catalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getPartitionsByFilter</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getPartition</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getPartitions(db: String, table: String)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: loadPartition</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: loadDynamicPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: renamePartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: alterPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: dropPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: createFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: functionExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: renameFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: alterFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getFunctionOption</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: listFunctions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: dropFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: sql set command</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: sql create index and reset</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: version</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: getConf</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: setOut</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: setInfo</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: setError</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: newSession</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: withHiveState and addJar</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: reset</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: CREATE TABLE AS SELECT</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: CREATE Partitioned TABLE AS SELECT</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: Delete the temporary staging directory and files after each insert</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: SPARK-13709: reading partitioned Avro table with nested schema</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: Decimal support of Avro Hive serde</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: read avro file containing decimal</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: SPARK-17920: Insert into/overwrite avro table</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: create client</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: createDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: createDatabase with null description</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: setCurrentDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: databaseExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: listDatabases</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: alterDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: dropDatabase</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: createTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: loadTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: tableExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getTableOption</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: alterTable(table: CatalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: alterTable(dbName: String, tableName: String, table: CatalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: alterTable - rename</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: alterTable - change database</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: alterTable - change database and table names</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: listTables(database)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: listTables(database, pattern)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: dropTable</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: sql create partitioned table</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: createPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getPartitionNames(catalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getPartitions(catalogTable)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getPartitionsByFilter</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getPartition</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getPartitionOption(db: String, table: String, spec: TablePartitionSpec)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getPartitionOption(table: CatalogTable, spec: TablePartitionSpec)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getPartitions(db: String, table: String)</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: loadPartition</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: loadDynamicPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: renamePartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: alterPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: dropPartitions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: createFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: functionExists</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: renameFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: alterFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getFunctionOption</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: listFunctions</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: dropFunction</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: sql set command</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: sql create index and reset</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: version</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: getConf</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: setOut</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: setInfo</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: setError</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: newSession</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: withHiveState and addJar</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: reset</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: CREATE TABLE AS SELECT</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: CREATE Partitioned TABLE AS SELECT</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: Delete the temporary staging directory and files after each insert</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: SPARK-13709: reading partitioned Avro table with nested schema</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: Decimal support of Avro Hive serde</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: read avro file containing decimal</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: SPARK-17920: Insert into/overwrite avro table</div></li><li><div>org.apache.spark.sql.hive.execution.Hive_2_1_DDLSuite.SPARK-21617: ALTER TABLE for non-compatible DataSource tables</div></li><li><div>org.apache.spark.sql.hive.execution.Hive_2_1_DDLSuite.SPARK-21617: ALTER TABLE for Hive-compatible DataSource tables</div></li><li><div>org.apache.spark.sql.hive.execution.Hive_2_1_DDLSuite.SPARK-21617: ALTER TABLE for Hive tables</div></li><li><div>org.apache.spark.sql.hive.execution.Hive_2_1_DDLSuite.SPARK-21617: ALTER TABLE with incompatible schema on Hive-compatible table</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="sqoop"><div style="font-weight:bold;" class="panel-heading">SQOOP</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>c146b3f94e937a8c03b6ae85a60800d1c62bdc94</div><div><b>Last Run: </b>30-03-2018 01:55 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 734</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 734</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 734</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 734</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="storm"><div style="font-weight:bold;" class="panel-heading">STORM</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>e6a3ac655db254d2e40a85eaeb932ea65cb6756d</div><div><b>Last Run: </b>29-03-2018 12:56 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1177</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1161</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1177</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1177</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.storm.utils.JCQueueTest.testFirstMessageFirst</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.storm.flux.multilang.MultilangEnvironmentTest.testInvokeNode</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to send halt interrupt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Cannot run program "node": error=2, No such file or directory</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.storm.utils.JCQueueTest.testFirstMessageFirst</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.storm.flux.multilang.MultilangEnvironmentTest.testInvokeNode</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="tez"><div style="font-weight:bold;" class="panel-heading">TEZ</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>7f936f614914a456e3ace9ca5a5f0d218e7c7e01</div><div><b>Last Run: </b>29-03-2018 15:27 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1767</div><div>Failed Count : 2</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1767</div><div>Failed Count : 2</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1767</div><div>Failed Count : 2</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1767</div><div>Failed Count : 3</div><div>Skipped Count : 14</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.history.TestHistoryParser.testParserWithSuccessfulJob</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.app.TestMockDAGAppMaster.testInternalPreemption</li></div><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.test.TestExceptionPropagation.testExceptionPropagationSession</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.test.TestExceptionPropagation.testExceptionPropagationSession</li></div><div><li>org.apache.tez.test.TestRecovery.testRecovery_OrderedWordCount</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.tez.history.TestHistoryParser.verifyJobSpecificInfo(TestHistoryParser.java:266)
	at org.apache.tez.history.TestHistoryParser.testParserWithSuccessfulJob(TestHistoryParser.java:212)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;KILLED&gt; but was:&lt;SUCCEEDED&gt;</li></div><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>Application not running, applicationId=application_1522355021520_0001, yarnApplicationState=FAILED, finalApplicationStatus=FAILED, trackingUrl=N/A, diagnostics=[DAG completed with an ERROR state. Shutting down AM, Session stats:submittedDAGs=9, successfulDAGs=0, failedDAGs=10, killedDAGs=0]</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>Application not running, applicationId=application_1522355213023_0001, yarnApplicationState=FAILED, finalApplicationStatus=FAILED, trackingUrl=N/A, diagnostics=[DAG completed with an ERROR state. Shutting down AM, Session stats:submittedDAGs=6, successfulDAGs=0, failedDAGs=7, killedDAGs=0]</li></div><div><li>expected:&lt;VERTEX_FINISHED&gt; but was:&lt;DAG_SUBMITTED&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.history.TestHistoryParser.testParserWithSuccessfulJob</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.dag.app.TestMockDAGAppMaster.testInternalPreemption</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.test.TestRecovery.testRecovery_OrderedWordCount</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="zeppelin"><div style="font-weight:bold;" class="panel-heading">ZEPPELIN</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>fd27014b02aa9635190b2c4bb76d34589b16792c</div><div><b>Last Run: </b>29-03-2018 15:24 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 779</div><div>Failed Count : 24</div><div>Skipped Count : 5</div></td><td><div>Total Count : 779</div><div>Failed Count : 25</div><div>Skipped Count : 5</div></td><td><div>Total Count : 779</div><div>Failed Count : 42</div><div>Skipped Count : 5</div></td><td><div>Total Count : 779</div><div>Failed Count : 41</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.sparkRTest[0]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.sparkRTest[1]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.sparkSQLTest[1]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.sparkRTest[2]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.sparkRTest[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[3]</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.sparkRTest[0]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.sparkRTest[1]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.sparkSQLTest[1]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.sparkRTest[2]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.sparkRTest[3]</li></div><div><li>org.apache.zeppelin.helium.HeliumBundleFactoryTest.bundlePackage</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[3]</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testGrpcFrameSize</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPython</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.interpreter.InterpreterOutputChangeWatcherTest.test</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkDepLoaderTest[0]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkTest[0]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testPySparkZeppelinContextDynamicForms[0]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testZeppelinContextResource[0]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.sparkRTest[0]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkDepLoaderTest[1]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkTest[1]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testPySparkZeppelinContextDynamicForms[1]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testZeppelinContextResource[1]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.sparkRTest[1]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkDepLoaderTest[2]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkTest[2]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testPySparkZeppelinContextDynamicForms[2]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testZeppelinContextResource[2]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.sparkRTest[2]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkDepLoaderTest[3]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkTest[3]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testPySparkZeppelinContextDynamicForms[3]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testZeppelinContextResource[3]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.sparkRTest[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[3]</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPython</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.spark.PySparkInterpreterTest.testCompletion</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkDepLoaderTest[0]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkTest[0]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testPySparkZeppelinContextDynamicForms[0]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testZeppelinContextResource[0]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.sparkRTest[0]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkDepLoaderTest[1]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkTest[1]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testPySparkZeppelinContextDynamicForms[1]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testZeppelinContextResource[1]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.sparkRTest[1]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkDepLoaderTest[2]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkTest[2]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testPySparkZeppelinContextDynamicForms[2]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testZeppelinContextResource[2]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.sparkRTest[2]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkDepLoaderTest[3]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkTest[3]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testPySparkZeppelinContextDynamicForms[3]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testZeppelinContextResource[3]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.sparkRTest[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[3]</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;[]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>expected:&lt;FINISHED&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;FINISHED&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.thrift.transport.TTransportException</li></div><div><li>expected:&lt;FINISHED&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;FINISHED&gt; but was:&lt;ERROR&gt;</li></div><div><li>/var/lib/jenkins/workspace/zeppelin/zeppelin-zengine/../bin/interpreter.sh: line 235: /var/lib/jenkins/workspace/zeppelin/run/zeppelin-interpreter-spark--2627e3116a5b.pid: No such file or directory
Warning: Master yarn-cluster is deprecated since 2.0. Please use master "yarn" with specified deploy mode instead.
15:51:16,404  WARN org.apache.hadoop.util.NativeCodeLoader:62 - Unable to load native-h</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;[]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>expected:&lt;FINISHED&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;FINISHED&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.thrift.transport.TTransportException</li></div><div><li>expected:&lt;FINISHED&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;FINISHED&gt; but was:&lt;ERROR&gt;</li></div><div><li>com.github.eirslett.maven.plugins.frontend.lib.TaskRunnerException: 'yarn install --fetch-retries=2 --fetch-retry-factor=1 --fetch-retry-mintimeout=5000 --registry=http://registry.npmjs.org/' failed. (error code 1)</li></div><div><li>/var/lib/jenkins/workspace/zeppelin/zeppelin-zengine/../bin/interpreter.sh: line 235: /var/lib/jenkins/workspace/zeppelin/run/zeppelin-interpreter-spark--c87486aa1ff2.pid: No such file or directory
Warning: Master yarn-cluster is deprecated since 2.0. Please use master "yarn" with specified deploy mode instead.
17:47:19,128  WARN org.apache.hadoop.util.NativeCodeLoader:62 - Unable to load native-h</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>Fail to open IPythonInterpreter</li></div><div><li>Fail to open IPythonInterpreter</li></div><div><li>Fail to open IPythonInterpreter</li></div><div><li>java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:712)
	at org.junit.Assert.assertNotNull(Assert.java:722)
	at org.apache.zeppelin.interpreter.InterpreterOutputChangeWatcherTest.test(InterpreterOutputChangeWatcherTest.java:99)
</li></div><div><li>expected:&lt;[]2
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]2
&gt;</li></div><div><li>expected:&lt;[]55
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]55
&gt;</li></div><div><li>expected:&lt;3&gt; but was:&lt;9&gt;</li></div><div><li>expected:&lt;[]hello world
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]hello world
&gt;</li></div><div><li>expected:&lt;FINISHED&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;[]2
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]2
&gt;</li></div><div><li>expected:&lt;[]55
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]55
&gt;</li></div><div><li>expected:&lt;3&gt; but was:&lt;9&gt;</li></div><div><li>expected:&lt;[]hello world
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]hello world
&gt;</li></div><div><li>expected:&lt;FINISHED&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;[]2
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]2
&gt;</li></div><div><li>expected:&lt;[]55
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]55
&gt;</li></div><div><li>expected:&lt;3&gt; but was:&lt;9&gt;</li></div><div><li>expected:&lt;[]hello world
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]hello world
&gt;</li></div><div><li>expected:&lt;FINISHED&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;[]2
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]2
&gt;</li></div><div><li>expected:&lt;[]55
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]55
&gt;</li></div><div><li>expected:&lt;3&gt; but was:&lt;9&gt;</li></div><div><li>expected:&lt;[]hello world
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]hello world
&gt;</li></div><div><li>expected:&lt;FINISHED&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>res6: String = 2.2.1
 doesn't contain 2.1.2</li></div><div><li>res7: String = 2.2.1
 doesn't contain 2.1.2</li></div><div><li>res8: String = 2.2.1
 doesn't contain 2.1.2</li></div><div><li>res9: String = 2.2.1
 doesn't contain 2.0.2</li></div><div><li>res10: String = 2.2.1
 doesn't contain 2.0.2</li></div><div><li>res11: String = 2.2.1
 doesn't contain 2.0.2</li></div><div><li>res12: String = 2.2.1
 doesn't contain 1.6.3</li></div><div><li>res13: String = 2.2.1
 doesn't contain 1.6.3</li></div><div><li>res14: String = 2.2.1
 doesn't contain 1.6.3</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;[]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.zeppelin.spark.PySparkInterpreterTest.testCompletion(PySparkInterpreterTest.java:142)
</li></div><div><li>expected:&lt;[]2
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]2
&gt;</li></div><div><li>expected:&lt;[]55
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]55
&gt;</li></div><div><li>expected:&lt;3&gt; but was:&lt;9&gt;</li></div><div><li>expected:&lt;[]hello world
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]hello world
&gt;</li></div><div><li>expected:&lt;FINISHED&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;[]2
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]2
&gt;</li></div><div><li>expected:&lt;[]55
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]55
&gt;</li></div><div><li>expected:&lt;3&gt; but was:&lt;9&gt;</li></div><div><li>expected:&lt;[]hello world
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]hello world
&gt;</li></div><div><li>expected:&lt;FINISHED&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;[]2
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]2
&gt;</li></div><div><li>expected:&lt;[]55
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]55
&gt;</li></div><div><li>expected:&lt;3&gt; but was:&lt;9&gt;</li></div><div><li>expected:&lt;[]hello world
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]hello world
&gt;</li></div><div><li>expected:&lt;FINISHED&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;[]2
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]2
&gt;</li></div><div><li>expected:&lt;[]55
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]55
&gt;</li></div><div><li>expected:&lt;3&gt; but was:&lt;9&gt;</li></div><div><li>expected:&lt;[]hello world
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
]hello world
&gt;</li></div><div><li>expected:&lt;FINISHED&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>res6: String = 2.2.1
 doesn't contain 2.1.2</li></div><div><li>res7: String = 2.2.1
 doesn't contain 2.1.2</li></div><div><li>res8: String = 2.2.1
 doesn't contain 2.1.2</li></div><div><li>res9: String = 2.2.1
 doesn't contain 2.0.2</li></div><div><li>res10: String = 2.2.1
 doesn't contain 2.0.2</li></div><div><li>res11: String = 2.2.1
 doesn't contain 2.0.2</li></div><div><li>res12: String = 2.2.1
 doesn't contain 1.6.3</li></div><div><li>res13: String = 2.2.1
 doesn't contain 1.6.3</li></div><div><li>res14: String = 2.2.1
 doesn't contain 1.6.3</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.helium.HeliumBundleFactoryTest.bundlePackage</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.python.IPythonInterpreterTest.testGrpcFrameSize</div></li><li><div>org.apache.zeppelin.interpreter.InterpreterOutputChangeWatcherTest.test</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.spark.PySparkInterpreterTest.testCompletion</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="zookeeper"><div style="font-weight:bold;" class="panel-heading">ZOOKEEPER</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>13c9f899ff62be6a2eacb28e522d06703f1dab08</div><div><b>Last Run: </b>30-03-2018 03:51 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1130</div><div>Failed Count : 0</div><div>Skipped Count : 1</div></td><td><div>Total Count : 1130</div><div>Failed Count : 0</div><div>Skipped Count : 1</div></td><td><div>Total Count : 1116</div><div>Failed Count : 4</div><div>Skipped Count : 1</div></td><td><div>Total Count : 1127</div><div>Failed Count : 3</div><div>Skipped Count : 1</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalObserverRun</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.MiniKdcTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosAuthTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosHostBasedAuthTest</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.MiniKdcTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosAuthTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosHostBasedAuthTest</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalObserverRun</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div id="ppcubuntu16" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">PPC UBUNTU16 SUMMARY</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/red.png" align="top" style="width: 16px; height: 16px;"></img>0 (0)</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>4 (4)</td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>18 (7)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (0)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>45 (2)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>33 (5)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>20 (0)</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>24 (0)</td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>24 (0)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr></tbody></table></div><div id="x86ubuntu16" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">X86 UBUNTU16 SUMMARY</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>60 (49)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>7 (4)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>45 (2)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>5 (5)</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>33 (5)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>20 (0)</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>24 (0)</td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>25 (1)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr></tbody></table></div><div id="ppcrhel7" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">PPC RHEL7 SUMMARY</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>4 (4)</td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>20 (17)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>5 (3)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>35 (2)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (3)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>4 (3)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (0)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>42 (2)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>4 (1)</td></tr></tbody></table></div><div id="x86rhel7" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">X86 RHEL7 SUMMARY</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>28 (25)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>4 (2)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>34 (1)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>629 (628)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (1)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>41 (1)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (0)</td></tr></tbody></table></div><div id="ppcx86" style="display:block;font-weight:bold" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">FULL SUMMARY</div></div><table class="table table-striped"><tbody><tr><th></th></tr><tr><th>Package Name</th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (2)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/red.png" align="top" style="width: 16px; height: 16px;"></img>0 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>4 (4)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (2)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>4 (4)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>18 (7)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>60 (49)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>20 (17)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>28 (25)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>7 (4)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>5 (3)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>4 (2)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>45 (2)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>45 (2)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>35 (2)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>34 (1)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>5 (5)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>33 (5)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>33 (5)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (3)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>20 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>20 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>24 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>24 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (2)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>4 (3)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>629 (628)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (1)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>24 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>25 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>42 (2)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>41 (1)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>4 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (0)</td></tr></tbody></table></div></div></body></html>