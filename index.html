<html><head><script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous" rel="stylesheet"></link><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous" rel="stylesheet"></link><script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><script src="helper.js">function hideAll(){console.log("hideAll")}function showme(e){console.log("showme");var l,n=e.substring(7),o=document.getElementsByName("data");for(l=0;l&lt;o.length;l++)o[l].style.display="none";var t=document.getElementsByName("summary");for(l=0;l&lt;t.length;l++)t[l].style.display="none";document.getElementById(n).style.display="block"}</script><style>table, th, td { vertical-align:top; padding: 3px} table {table-layout:fixed} td {word-wrap:break-word} .bs-callout { padding: 5px; margin: 5px 0; border: 1px solid #eee; border-left-width: 5px; border-radius: 3px; font-weight:normal; }.bs-callout-info {border-left-color: #5bc0de;}</style></head><body><div class="page-header"><ul class="nav nav-pills"><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcx86" onclick="showme(this.id);">FULL SUMMARY</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcubuntu16" onclick="showme(this.id);">PPC UBUNTU16</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_x86ubuntu16" onclick="showme(this.id);">X86 UBUNTU16</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcrhel7" onclick="showme(this.id);">PPC RHEL7</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_x86rhel7" onclick="showme(this.id);">X86 RHEL7</a></li><p align="right" role="presentation" style="color:grey">Date: 19-03-2018 10:00 UTC</p></ul></div><div style="table-cell" class="col-sm-2 col-md-2 sidebar"><div class="list-group"><a href="#" class="list-group-item list-group-item-action active">Packages</a><a href="#" id="anchor_accumulo" onclick="showme(this.id);" class="list-group-item list-group-item-action">ACCUMULO</a><a href="#" id="anchor_ambari" onclick="showme(this.id);" class="list-group-item list-group-item-action">AMBARI</a><a href="#" id="anchor_atlas" onclick="showme(this.id);" class="list-group-item list-group-item-action">ATLAS</a><a href="#" id="anchor_falcon" onclick="showme(this.id);" class="list-group-item list-group-item-action">FALCON</a><a href="#" id="anchor_flume" onclick="showme(this.id);" class="list-group-item list-group-item-action">FLUME</a><a href="#" id="anchor_hadoop" onclick="showme(this.id);" class="list-group-item list-group-item-action">HADOOP</a><a href="#" id="anchor_hbase" onclick="showme(this.id);" class="list-group-item list-group-item-action">HBASE</a><a href="#" id="anchor_hive" onclick="showme(this.id);" class="list-group-item list-group-item-action">HIVE</a><a href="#" id="anchor_kafka" onclick="showme(this.id);" class="list-group-item list-group-item-action">KAFKA</a><a href="#" id="anchor_knox" onclick="showme(this.id);" class="list-group-item list-group-item-action">KNOX</a><a href="#" id="anchor_metron" onclick="showme(this.id);" class="list-group-item list-group-item-action">METRON</a><a href="#" id="anchor_oozie" onclick="showme(this.id);" class="list-group-item list-group-item-action">OOZIE</a><a href="#" id="anchor_phoenix" onclick="showme(this.id);" class="list-group-item list-group-item-action">PHOENIX</a><a href="#" id="anchor_pig" onclick="showme(this.id);" class="list-group-item list-group-item-action">PIG</a><a href="#" id="anchor_ranger" onclick="showme(this.id);" class="list-group-item list-group-item-action">RANGER</a><a href="#" id="anchor_slider" onclick="showme(this.id);" class="list-group-item list-group-item-action">SLIDER</a><a href="#" id="anchor_spark" onclick="showme(this.id);" class="list-group-item list-group-item-action">SPARK</a><a href="#" id="anchor_sqoop" onclick="showme(this.id);" class="list-group-item list-group-item-action">SQOOP</a><a href="#" id="anchor_storm" onclick="showme(this.id);" class="list-group-item list-group-item-action">STORM</a><a href="#" id="anchor_tez" onclick="showme(this.id);" class="list-group-item list-group-item-action">TEZ</a><a href="#" id="anchor_zeppelin" onclick="showme(this.id);" class="list-group-item list-group-item-action">ZEPPELIN</a><a href="#" id="anchor_zookeeper" onclick="showme(this.id);" class="list-group-item list-group-item-action">ZOOKEEPER</a></div></div><div style="display: table-cell"><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="accumulo"><div style="font-weight:bold;" class="panel-heading">ACCUMULO</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>dce8c359056905405abaea9b99efc21dadbc5920</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1704</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1704</div><div>Failed Count : 2</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1704</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1704</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol><div><li>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.testPerTableClasspath</li></div><div><li>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.test</li></div></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 30000 milliseconds</li></div></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.testPerTableClasspath</div></li><li><div>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.test</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="ambari"><div style="font-weight:bold;" class="panel-heading">AMBARI</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> origin/trunk</div><div><b>Last Revision: </b>7a267d21eb1dd636be0e4563bba3046ca0d89dd3</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 0</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 5505</div><div>Failed Count : 1</div><div>Skipped Count : 83</div></td><td><div>Total Count : 5505</div><div>Failed Count : 0</div><div>Skipped Count : 83</div></td><td><div>Total Count : 5505</div><div>Failed Count : 0</div><div>Skipped Count : 83</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/red.png" align="top" style="width: 16px; height: 16px;"></img>FAILURE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol><div><li>org.apache.ambari.server.controller.metrics.RestMetricsPropertyProviderTest.testRestMetricsPropertyProviderAsAdministrator</li></div></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol><div><li>expected:&lt;400&gt; but was:&lt;5&gt;</li></div></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.ambari.server.controller.metrics.RestMetricsPropertyProviderTest.testRestMetricsPropertyProviderAsAdministrator</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="atlas"><div style="font-weight:bold;" class="panel-heading">ATLAS</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>3b074d9ceb20d512edb04569d9ae54215b94134d</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 771</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 771</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 771</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 771</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="falcon"><div style="font-weight:bold;" class="panel-heading">FALCON</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>1a5b4f6a509187498a267b0e375c6a065f947af5</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1005</div><div>Failed Count : 4</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1003</div><div>Failed Count : 1</div><div>Skipped Count : 7</div></td><td><div>Total Count : 999</div><div>Failed Count : 16</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1007</div><div>Failed Count : 12</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol><div><li>org.apache.falcon.update.UpdateHelperTest.testIsEntityACLUpdated</li></div><div><li>org.apache.falcon.update.UpdateHelperTest.testIsEntityLateProcessUpdated</li></div><div><li>org.apache.falcon.update.UpdateHelperTest.testIsEntityUpdated</li></div><div><li>org.apache.falcon.update.UpdateHelperTest.testShouldUpdateAffectedEntities</li></div></ol></td><td><ol><div><li>org.apache.falcon.entity.v0.EntityGraphTest.initConfigStore</li></div></ol></td><td><ol><div><li>org.apache.falcon.update.UpdateHelperTest.testIsEntityACLUpdated</li></div><div><li>org.apache.falcon.update.UpdateHelperTest.testIsEntityLateProcessUpdated</li></div><div><li>org.apache.falcon.update.UpdateHelperTest.testIsEntityUpdated</li></div><div><li>org.apache.falcon.update.UpdateHelperTest.testShouldUpdateAffectedEntities</li></div><div><li>org.apache.falcon.oozie.feed.OozieFeedWorkflowBuilderTest.testReplicationCoordsForTableStorage</li></div><div><li>org.apache.falcon.oozie.feed.OozieFeedWorkflowBuilderTest.testReplicationCoordsForTableStorage</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapper</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapper</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapperWithFSInputFeedAndTableOutputFeed</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapperWithFSInputFeedAndTableOutputFeed</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapperWithTableInputFeedAndFSOutputFeed</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapperWithTableInputFeedAndFSOutputFeed</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessWithNoInputsAndOutputs</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessWithNoInputsAndOutputs</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testSparkProcess</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testSparkSQLProcess</li></div></ol></td><td><ol><div><li>org.apache.falcon.oozie.feed.OozieFeedWorkflowBuilderTest.testReplicationCoordsForTableStorage</li></div><div><li>org.apache.falcon.oozie.feed.OozieFeedWorkflowBuilderTest.testReplicationCoordsForTableStorage</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapper</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapper</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapperWithFSInputFeedAndTableOutputFeed</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapperWithFSInputFeedAndTableOutputFeed</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapperWithTableInputFeedAndFSOutputFeed</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessMapperWithTableInputFeedAndFSOutputFeed</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessWithNoInputsAndOutputs</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testHiveProcessWithNoInputsAndOutputs</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testSparkProcess</li></div><div><li>org.apache.falcon.oozie.process.OozieProcessWorkflowBuilderTest.testSparkSQLProcess</li></div></ol></td></tr><tr><td>Description</td><td><ol><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div></ol></td><td><ol><div><li>Unable to restore configurations for entity type PROCESS</li></div></ol></td><td><ol><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div><div><li>Parent path is not a directory: /var/lib/jenkins/workspace/falcon/common/target/falcon/tmp-hadoop-jenkins/jail-fs/testCluster/projects/falcon/staging/falcon/workflows/process/sample</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div></ol></td><td><ol><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div><div><li>com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.falcon.update.UpdateHelperTest.testIsEntityACLUpdated</div></li><li><div>org.apache.falcon.update.UpdateHelperTest.testIsEntityLateProcessUpdated</div></li><li><div>org.apache.falcon.update.UpdateHelperTest.testIsEntityUpdated</div></li><li><div>org.apache.falcon.update.UpdateHelperTest.testShouldUpdateAffectedEntities</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.falcon.entity.v0.EntityGraphTest.initConfigStore</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.falcon.update.UpdateHelperTest.testIsEntityACLUpdated</div></li><li><div>org.apache.falcon.update.UpdateHelperTest.testIsEntityLateProcessUpdated</div></li><li><div>org.apache.falcon.update.UpdateHelperTest.testIsEntityUpdated</div></li><li><div>org.apache.falcon.update.UpdateHelperTest.testShouldUpdateAffectedEntities</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="flume"><div style="font-weight:bold;" class="panel-heading">FLUME</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>0d437810dc850192b48fa3b31608ffcd23b1f1e9</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1182</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1208</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1182</div><div>Failed Count : 1</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1208</div><div>Failed Count : 1</div><div>Skipped Count : 6</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol></ol></td><td><ol><div><li>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</li></div></ol></td><td><ol><div><li>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</li></div></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol></ol></td><td><ol><div><li>java.lang.AssertionError
	at org.apache.flume.channel.file.TestLog.doTestMinimumRequiredSpaceTooSmallForPut(TestLog.java:241)
	at org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut(TestLog.java:210)
</li></div></ol></td><td><ol><div><li>java.lang.AssertionError
	at org.apache.flume.channel.file.TestLog.doTestMinimumRequiredSpaceTooSmallForPut(TestLog.java:241)
	at org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut(TestLog.java:210)
</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hadoop"><div style="font-weight:bold;" class="panel-heading">HADOOP</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>49c747ab187d0650143205ba57ca19607ec4c6bd</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 18939</div><div>Failed Count : 18</div><div>Skipped Count : 1156</div></td><td><div>Total Count : 18722</div><div>Failed Count : 60</div><div>Skipped Count : 1158</div></td><td><div>Total Count : 18904</div><div>Failed Count : 20</div><div>Skipped Count : 1157</div></td><td><div>Total Count : 18814</div><div>Failed Count : 28</div><div>Skipped Count : 1158</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol><div><li>org.apache.hadoop.ha.TestZKFailoverController.testGracefulFailoverMultipleZKfcs</li></div><div><li>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.balancer.TestBalancer.testBalancerWithStripedFile</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks.testSetRepIncWithUnderReplicatedBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testVolumeFailure</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testSuccessiveVolumeFailures</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore.org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.testFixedSizeThreadPool</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testStartMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage.testContainerLogPageAccess</li></div></ol></td><td><ol><div><li>org.apache.hadoop.ha.TestZKFailoverController.testGracefulFailoverMultipleZKfcs</li></div><div><li>org.apache.hadoop.security.TestGroupsCaching.testThreadNotBlockedWhenExpiredEntryExistsWithBackgroundRefresh</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notDir_local</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notReadable_local</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notWritable_local</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testDiskIoIgnoresTransientCreateErrors</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testDiskIoDetectsCreateErrors</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notListable</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notListable_local</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testCheckDir_normal_local</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testDiskIoDetectsWriteErrors</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notWritable</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testCheckDir_normal</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notDir</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notReadable</li></div><div><li>org.apache.hadoop.util.TestDiskChecker.testDiskIoIgnoresTransientWriteErrors</li></div><div><li>org.apache.hadoop.hdfs.TestDFSShell.testCopyCommandsWithPreserveOption</li></div><div><li>org.apache.hadoop.hdfs.TestHDFSFileSystemContract.testAppend</li></div><div><li>org.apache.hadoop.hdfs.TestLocalDFS.testWorkingDirectory</li></div><div><li>org.apache.hadoop.hdfs.TestMaintenanceState.testDecommissionDifferentNodeAfterMaintenances</li></div><div><li>org.apache.hadoop.hdfs.TestPersistBlocks.TestRestartDfsWithFlush</li></div><div><li>org.apache.hadoop.hdfs.TestPread.testPreadFailureWithChangedBlockLocations</li></div><div><li>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</li></div><div><li>org.apache.hadoop.hdfs.TestReconstructStripedFile.testRecoverAllParityBlocks</li></div><div><li>org.apache.hadoop.hdfs.security.TestDelegationTokenForProxyUser.testWebHdfsDoAs</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean.testStorageTypeStatsWhenStorageFailed</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean.testStorageTypeStatsJMX</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeErasureCodingMetrics.testReconstructionBytesPartialGroup2</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID.testUUIDRegeneration</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testUnderReplicationAfterVolFailure</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testVolumeFailureRecoveredByHotSwappingVolume</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testDataNodeReconfigureWithVolumeFailures</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner.testThrottling</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner.testDirectoryScannerInFederatedCluster</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestSpaceReservation.testTmpSpaceReserve</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestHAAppend.testMultipleAppendsDuringCatchupTailing</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.testCompleteFileAfterCrashFailover</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.testWriteOverGracefulFailoverWithDnFail</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA.testUpdatePipeline</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.TestNNBench.testNNBenchCreateReadAndDelete</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMRTimelineEventHandling</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</li></div><div><li>org.apache.hadoop.yarn.sls.TestSLSRunner.testSimulatorRunning[Testing with: SYNTH, org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler, (nodeFile null)]</li></div><div><li>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[1]</li></div><div><li>org.apache.hadoop.yarn.logaggregation.TestAggregatedLogDeletionService.testRefreshLogRetentionSettings</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage.testContainerLogPageAccess</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testRemoveAttempt</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testIncreaseContainerUnreservedWhenApplicationCompleted</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testExcessiveReservationWhenDecreaseSameContainer</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken</li></div><div><li>org.apache.hadoop.yarn.webapp.TestRMWithCSRFFilter.testNoCustomHeaderFromBrowser</li></div></ol></td><td><ol><div><li>org.apache.hadoop.io.nativeio.TestNativeIO.testStat</li></div><div><li>org.apache.hadoop.io.nativeio.TestNativeIO.testMultiThreadedStat</li></div><div><li>org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy.testCloseWithExceptionsInStreamer</li></div><div><li>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testUnderReplicationAfterVolFailure</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestNameNodeMetadataConsistency.testGenerationStampInFuture</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestSecurityTokenEditLog.testEditsForCancelOnTokenExpire</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestHAAppend.testMultipleAppendsDuringCatchupTailing</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics.testCapacityMetrics</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.web.resources.TestWebHdfsDataLocality.testExcludeDataNodes</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMRNewTimelineServiceEventHandling</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</li></div><div><li>org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore.org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore</li></div><div><li>org.apache.hadoop.yarn.sls.scheduler.TestTaskRunner.testPreStartQueueing</li></div><div><li>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2CustomizedFlow</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithSaslEncryption[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithSaslEncryption[0]</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.testFixedSizeThreadPool</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage.testContainerLogPageAccess</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestFSRMStateStore.testFSRMStateStoreClientRetry</li></div></ol></td><td><ol><div><li>org.apache.hadoop.ha.TestZKFailoverController.testGracefulFailoverMultipleZKfcs</li></div><div><li>org.apache.hadoop.ipc.TestIPC.testConnectionIdleTimeouts</li></div><div><li>org.apache.hadoop.security.TestGroupsCaching.testThreadNotBlockedWhenExpiredEntryExistsWithBackgroundRefresh</li></div><div><li>org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager.testMultiNodeOperations</li></div><div><li>org.apache.hadoop.hdfs.TestDecommission.testPendingNodes</li></div><div><li>org.apache.hadoop.hdfs.TestErasureCodingMultipleRacks.testSkewedRack2</li></div><div><li>org.apache.hadoop.hdfs.TestHDFSFileSystemContract.testAppend</li></div><div><li>org.apache.hadoop.hdfs.TestPersistBlocks.TestRestartDfsWithFlush</li></div><div><li>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</li></div><div><li>org.apache.hadoop.hdfs.security.TestDelegationTokenForProxyUser.testWebHdfsDoAs</li></div><div><li>org.apache.hadoop.hdfs.server.balancer.TestBalancerRPCDelay.testBalancerRPCDelay</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID.testUUIDRegeneration</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner.testThrottling</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithJournalNodes</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA.testUpdatePipeline</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</li></div><div><li>org.apache.hadoop.fs.azure.TestClientThrottlingAnalyzer.testManySuccessAndErrorsAndWaiting</li></div><div><li>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2</li></div><div><li>org.apache.hadoop.yarn.logaggregation.TestAggregatedLogDeletionService.testRefreshLogRetentionSettings</li></div><div><li>org.apache.hadoop.registry.server.dns.TestSecureRegistryDNS.testExternalCNAMERecord</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync.testNMSentContainerStatusOnResync</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeProcessFailure</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testStartMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage.testContainerLogPageAccess</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMEmbeddedElector.testCallbackSynchronization</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler.testCSReservationWithRootUnblocked</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testContainerIncreaseAllocationExpiration</li></div><div><li>org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.testDeleteReservationXML</li></div></ol></td></tr><tr><td>Description</td><td><ol><div><li>Unable to become active. Local node did not get an opportunity to do so from ZooKeeper, or the local node took too long to transition to active.</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>Cluster failed to reached expected values of totalSpace (current: 1174405120, expected: 1174405120), or usedSpace (current: 457179166, expected: 452984832), in more than 40000 msec.</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li> Expected to find 'localhost:41529: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:41529: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:35320: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:35320: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:36566: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:36566: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li> Expected to find 'localhost:44634: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:44634: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:37693: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:37693: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:41960: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:41960: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li>initTable on TestDynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException: The request processing has failed because of an unknown error, exception or failure. (Service: AmazonDynamoDBv2; Status Code: 500; Error Code: InternalFailure; Request ID: 49668297-dc74-4918-9472-af94f77fe42f): The request processing has failed because of an unknown error, exception or failure.</li></div><div><li>expected:&lt;3&gt; but was:&lt;2&gt;</li></div><div><li>ContainerState is not correct (timedout)</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileControllerFactory.&lt;init&gt;(LogAggregationFileControllerFactory.java:68)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsPage$ContainersLogsBlock.&lt;init&gt;(ContainerLogsPage.java:100)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage.testContainerLogPageAcc</li></div></ol></td><td><ol><div><li>Unable to become active. Local node did not get an opportunity to do so from ZooKeeper, or the local node took too long to transition to active.</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test5903334520507024322tmp</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test1498795230920626705</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test6584864813077008841</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test2335399963726861465</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test2659697764578788802</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test6891259757836500600</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test259780093406813557</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test2259207363293821701</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test6409131635423873089</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test5812972311135538926</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test7909222521993262347</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test6571365320603941937tmp</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test6007983364796014975</li></div><div><li>/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-common-project/hadoop-common/target/test/data/4/test4083909263333575199</li></div><div><li>expected:&lt;1521301843594&gt; but was:&lt;1521301844711&gt;</li></div><div><li>test timed out after 30000 milliseconds</li></div><div><li>test timed out after 20000 milliseconds</li></div><div><li>1</li></div><div><li>End of File Exception between local host is: "in-ibmibm662.persistent.co.in/10.53.17.125"; destination host is: "localhost":34026; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException</li></div><div><li>1</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>test timed out after 5000 milliseconds</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>expected:&lt;6&gt; but was:&lt;5&gt;</li></div><div><li>ecReconstructionBytesRead should be  expected:&lt;7444889&gt; but was:&lt;0&gt;</li></div><div><li>test timed out after 10000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>Timed out waiting for /test1 to reach 2 replicas</li></div><div><li>test timed out after 600000 milliseconds</li></div><div><li>Cannot remove data directory: /var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-hdfs-project/hadoop-hdfs/target/test/data/2/dfs/datapath '/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-hdfs-project/hadoop-hdfs/target/test/data/2/dfs/data': 
 absolute:/var/lib/jenkins/workspace/hadoop/label/x86ub16/hadoop-hdfs-project/hadoop-hdfs/target/test/data/2/dfs/data
 permissions: drwx
path '/va</li></div><div><li>Wrong reserve space for Tmp  expected:&lt;200&gt; but was:&lt;1000&gt;</li></div><div><li>Test resulted in an unexpected exit</li></div><div><li>test timed out after 30000 milliseconds</li></div><div><li>Test resulted in an unexpected exit</li></div><div><li>After waiting the operation updatePipeline still has not taken effect on NN yet</li></div><div><li> Expected to find 'localhost:35955: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:35955: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:45621: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:45621: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:44167: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:44167: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li> Expected to find 'localhost:43865: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:43865: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:35040: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:35040: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:38379: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:38379: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li>create_write should create the file</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>TestSLSRunner catched exception from child thread (TaskRunner.TaskDefinition): [java.lang.reflect.UndeclaredThrowableException]</li></div><div><li>Unexpected number of YARN_CONTAINER_FINISHED event published. expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>
Argument(s) are different! Wanted:
fileSystem.delete(
    mockfs://foo/tmp/logs/me/logs/application_1521310977408_0002,
    true
);
-&gt; at org.apache.hadoop.yarn.logaggregation.TestAggregatedLogDeletionService.testRefreshLogRetentionSettings(TestAggregatedLogDeletionService.java:300)
Actual invocation has different arguments:
fileSystem.delete(
    mockfs://foo/tmp/logs/me/logs/application_1521310</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileControllerFactory.&lt;init&gt;(LogAggregationFileControllerFactory.java:68)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsPage$ContainersLogsBlock.&lt;init&gt;(ContainerLogsPage.java:100)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage.testContainerLogPageAcc</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreTestBase.testRemoveAttempt(RMStateStoreTestBase.java:672)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testRemoveAttempt(TestLev</li></div><div><li>expected null, but was:&lt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User@6cc7b4de&gt;</li></div><div><li>expected:&lt;1024&gt; but was:&lt;2048&gt;</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:621)
	at org.junit.Assert.assertNotNull(Assert.java:631)
	at org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken(TestDelegationTokenRenewer.java:1066)
	at sun.reflect.NativeMetho</li></div><div><li>java.net.BindException: Address already in use</li></div></ol></td><td><ol><div><li>length != 10(unixSymbolicPermission=-rw-r--r--.)</li></div><div><li>java.lang.IllegalArgumentException: length != 10(unixSymbolicPermission=-rw-r--r--.)</li></div><div><li>Failed: the number of failed blocks = 2 &gt; the number of parity blocks = 1</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-03-17 03:59:18,293

"datanode DomainSocketWatcher" daemon prio=5 tid=1677 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.hadoop.net.unix.DomainSocketWatcher.doPoll0(Native Method)
        at org.apache.hadoop.net.unix.DomainSocketWatcher.access$900(DomainSocketWatcher.java:52)
        at org.apache.hadoop.net.uni</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-03-17 03:23:47,381

"java.util.concurrent.ThreadPoolExecutor$Worker@61ca59d8[State = -1, empty queue]" daemon prio=5 tid=269 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.conc</li></div><div><li>
fSEditLog.logCancelDelegationToken(
    token for jenkins: HDFS_DELEGATION_TOKEN owner=jenkins, renewer=jenkins, realUser=, issueDate=1521301710715, maxDate=1521301714715, sequenceNumber=1, masterKeyId=1
);
Never wanted here:
-&gt; at org.apache.hadoop.hdfs.server.namenode.TestSecurityTokenEditLog.testEditsForCancelOnTokenExpire(TestSecurityTokenEditLog.java:235)
But invoked here:
-&gt; at org.apache.h</li></div><div><li>Test resulted in an unexpected exit</li></div><div><li>java.lang.AssertionError
	at org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics.testCapacityMetrics(TestNameNodeMetrics.java:219)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.refl</li></div><div><li>expected:&lt;3&gt; but was:&lt;2&gt;</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>initTable on TestDynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException: The request processing has failed because of an unknown error, exception or failure. (Service: AmazonDynamoDBv2; Status Code: 500; Error Code: InternalFailure; Request ID: 328f7315-4b78-47f3-b713-77a8ad76799f): The request processing has failed because of an unknown error, exception or failure.</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.yarn.sls.scheduler.TestTaskRunner.testPreStartQueueing(TestTaskRunner.java:244)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div><div><li>Unexpected number of YARN_CONTAINER_FINISHED event published. expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;2&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor150.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveC</li></div><div><li>expected:&lt;3&gt; but was:&lt;2&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileControllerFactory.&lt;init&gt;(LogAggregationFileControllerFactory.java:68)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsPage$ContainersLogsBlock.&lt;init&gt;(ContainerLogsPage.java:100)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage.testContainerLogPageAcc</li></div><div><li>test timed out after 30000 milliseconds</li></div></ol></td><td><ol><div><li>Unable to become active. Local node did not get an opportunity to do so from ZooKeeper, or the local node took too long to transition to active.</li></div><div><li>expected:&lt;7&gt; but was:&lt;4&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>Expected InvalidToken</li></div><div><li>Unexpected number of pending nodes expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>expected:&lt;9&gt; but was:&lt;8&gt;</li></div><div><li>test timed out after 30000 milliseconds</li></div><div><li>End of File Exception between local host is: "hj-ibmibm13055.persistent.com/10.53.16.233"; destination host is: "localhost":43465; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>test timed out after 5000 milliseconds</li></div><div><li>test timed out after 100000 milliseconds</li></div><div><li>test timed out after 10000 milliseconds</li></div><div><li>test timed out after 600000 milliseconds</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithJournalNodes(TestDFSUpgradeWithHA.java:687)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>After waiting the operation updatePipeline still has not taken effect on NN yet</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>The actual value 13 is not within the expected range: [5.60, 8.40].</li></div><div><li>Unexpected number of YARN_CONTAINER_FINISHED event published. expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>
Argument(s) are different! Wanted:
fileSystem.delete(
    mockfs://foo/tmp/logs/me/logs/application_1521308851583_0002,
    true
);
-&gt; at org.apache.hadoop.yarn.logaggregation.TestAggregatedLogDeletionService.testRefreshLogRetentionSettings(TestAggregatedLogDeletionService.java:300)
Actual invocation has different arguments:
fileSystem.delete(
    mockfs://foo/tmp/logs/me/logs/application_1521308</li></div><div><li>No A records in answer</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.junit.Assert.assertFalse(Assert.java:74)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync.testNMSentContainerStatusOnResync(TestNodeManagerResync.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Nati</li></div><div><li>Process is still alive!</li></div><div><li>ContainerState is not correct (timedout)</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileControllerFactory.&lt;init&gt;(LogAggregationFileControllerFactory.java:68)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsPage$ContainersLogsBlock.&lt;init&gt;(ContainerLogsPage.java:100)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage.testContainerLogPageAcc</li></div><div><li>
Wanted but not invoked:
adminService.transitionToStandby(&lt;any&gt;);
-&gt; at org.apache.hadoop.yarn.server.resourcemanager.TestRMEmbeddedElector.testCallbackSynchronizationNeutral(TestRMEmbeddedElector.java:215)
Actually, there were zero interactions with this mock.
</li></div><div><li>Y1 Used Resource should be 4 GB expected:&lt;4096&gt; but was:&lt;1024&gt;</li></div><div><li>expected:&lt;2048&gt; but was:&lt;4096&gt;</li></div><div><li>test timed out after 2000 milliseconds</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.hadoop.hdfs.server.balancer.TestBalancer.testBalancerWithStripedFile</div></li><li><div>org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks.testSetRepIncWithUnderReplicatedBlocks</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testVolumeFailure</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testSuccessiveVolumeFailures</div></li><li><div>org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore.org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.testFixedSizeThreadPool</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.hadoop.security.TestGroupsCaching.testThreadNotBlockedWhenExpiredEntryExistsWithBackgroundRefresh</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notDir_local</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notReadable_local</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notWritable_local</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testDiskIoIgnoresTransientCreateErrors</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testDiskIoDetectsCreateErrors</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notListable</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notListable_local</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testCheckDir_normal_local</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testDiskIoDetectsWriteErrors</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notWritable</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testCheckDir_normal</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notDir</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testCheckDir_notReadable</div></li><li><div>org.apache.hadoop.util.TestDiskChecker.testDiskIoIgnoresTransientWriteErrors</div></li><li><div>org.apache.hadoop.hdfs.TestDFSShell.testCopyCommandsWithPreserveOption</div></li><li><div>org.apache.hadoop.hdfs.TestHDFSFileSystemContract.testAppend</div></li><li><div>org.apache.hadoop.hdfs.TestLocalDFS.testWorkingDirectory</div></li><li><div>org.apache.hadoop.hdfs.TestMaintenanceState.testDecommissionDifferentNodeAfterMaintenances</div></li><li><div>org.apache.hadoop.hdfs.TestPersistBlocks.TestRestartDfsWithFlush</div></li><li><div>org.apache.hadoop.hdfs.TestPread.testPreadFailureWithChangedBlockLocations</div></li><li><div>org.apache.hadoop.hdfs.TestReconstructStripedFile.testRecoverAllParityBlocks</div></li><li><div>org.apache.hadoop.hdfs.security.TestDelegationTokenForProxyUser.testWebHdfsDoAs</div></li><li><div>org.apache.hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean.testStorageTypeStatsWhenStorageFailed</div></li><li><div>org.apache.hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean.testStorageTypeStatsJMX</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeErasureCodingMetrics.testReconstructionBytesPartialGroup2</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID.testUUIDRegeneration</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testVolumeFailureRecoveredByHotSwappingVolume</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testDataNodeReconfigureWithVolumeFailures</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner.testThrottling</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner.testDirectoryScannerInFederatedCluster</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestSpaceReservation.testTmpSpaceReserve</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.testCompleteFileAfterCrashFailover</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.testWriteOverGracefulFailoverWithDnFail</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA.testUpdatePipeline</div></li><li><div>org.apache.hadoop.hdfs.TestNNBench.testNNBenchCreateReadAndDelete</div></li><li><div>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMRTimelineEventHandling</div></li><li><div>org.apache.hadoop.yarn.sls.TestSLSRunner.testSimulatorRunning[Testing with: SYNTH, org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler, (nodeFile null)]</div></li><li><div>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[1]</div></li><li><div>org.apache.hadoop.yarn.logaggregation.TestAggregatedLogDeletionService.testRefreshLogRetentionSettings</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testRemoveAttempt</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testIncreaseContainerUnreservedWhenApplicationCompleted</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testExcessiveReservationWhenDecreaseSameContainer</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken</div></li><li><div>org.apache.hadoop.yarn.webapp.TestRMWithCSRFFilter.testNoCustomHeaderFromBrowser</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.hadoop.io.nativeio.TestNativeIO.testStat</div></li><li><div>org.apache.hadoop.io.nativeio.TestNativeIO.testMultiThreadedStat</div></li><li><div>org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy.testCloseWithExceptionsInStreamer</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.TestNameNodeMetadataConsistency.testGenerationStampInFuture</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.TestSecurityTokenEditLog.testEditsForCancelOnTokenExpire</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics.testCapacityMetrics</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.web.resources.TestWebHdfsDataLocality.testExcludeDataNodes</div></li><li><div>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMRNewTimelineServiceEventHandling</div></li><li><div>org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore.org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore</div></li><li><div>org.apache.hadoop.yarn.sls.scheduler.TestTaskRunner.testPreStartQueueing</div></li><li><div>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2CustomizedFlow</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithSaslEncryption[0]</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithSaslEncryption[0]</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.testFixedSizeThreadPool</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestFSRMStateStore.testFSRMStateStoreClientRetry</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.hadoop.ipc.TestIPC.testConnectionIdleTimeouts</div></li><li><div>org.apache.hadoop.security.TestGroupsCaching.testThreadNotBlockedWhenExpiredEntryExistsWithBackgroundRefresh</div></li><li><div>org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager.testMultiNodeOperations</div></li><li><div>org.apache.hadoop.hdfs.TestDecommission.testPendingNodes</div></li><li><div>org.apache.hadoop.hdfs.TestErasureCodingMultipleRacks.testSkewedRack2</div></li><li><div>org.apache.hadoop.hdfs.TestHDFSFileSystemContract.testAppend</div></li><li><div>org.apache.hadoop.hdfs.TestPersistBlocks.TestRestartDfsWithFlush</div></li><li><div>org.apache.hadoop.hdfs.security.TestDelegationTokenForProxyUser.testWebHdfsDoAs</div></li><li><div>org.apache.hadoop.hdfs.server.balancer.TestBalancerRPCDelay.testBalancerRPCDelay</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID.testUUIDRegeneration</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner.testThrottling</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithJournalNodes</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA.testUpdatePipeline</div></li><li><div>org.apache.hadoop.fs.azure.TestClientThrottlingAnalyzer.testManySuccessAndErrorsAndWaiting</div></li><li><div>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2</div></li><li><div>org.apache.hadoop.yarn.logaggregation.TestAggregatedLogDeletionService.testRefreshLogRetentionSettings</div></li><li><div>org.apache.hadoop.registry.server.dns.TestSecureRegistryDNS.testExternalCNAMERecord</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync.testNMSentContainerStatusOnResync</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeProcessFailure</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.TestRMEmbeddedElector.testCallbackSynchronization</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler.testCSReservationWithRootUnblocked</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testContainerIncreaseAllocationExpiration</div></li><li><div>org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.testDeleteReservationXML</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hbase"><div style="font-weight:bold;" class="panel-heading">HBASE</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>31da4d0bce69b3a47066a5df675756087ce4dc60</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 4524</div><div>Failed Count : 0</div><div>Skipped Count : 45</div></td><td><div>Total Count : 2599</div><div>Failed Count : 1</div><div>Skipped Count : 25</div></td><td><div>Total Count : 4524</div><div>Failed Count : 2</div><div>Skipped Count : 45</div></td><td><div>Total Count : 4536</div><div>Failed Count : 2</div><div>Skipped Count : 45</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol><div><li>org.apache.hadoop.hbase.master.TestCatalogJanitor.testParentCleanedEvenIfDaughterGoneFirst</li></div></ol></td><td><ol><div><li>org.apache.hadoop.hbase.rsgroup.TestRSGroupsOfflineMode.org.apache.hadoop.hbase.rsgroup.TestRSGroupsOfflineMode</li></div><div><li>org.apache.hadoop.hbase.client.TestMultiParallel.testActiveThreadsCount</li></div></ol></td><td><ol><div><li>org.apache.hadoop.hbase.rsgroup.TestEnableRSGroup.testEnableRSGroup</li></div><div><li>org.apache.hadoop.hbase.client.TestClientPushback.testClientTracksServerPushback</li></div></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.master.TestCatalogJanitor.parentWithSpecifiedEndKeyCleanedEvenIfDaughterGoneFirst(TestCatalogJanitor.java:267)
	at org.apache.hadoop.hbase.master.TestCatalogJanitor.testParentCleanedEvenIfDaughterGoneFirst(TestCatalogJanitor.java:156)
</li></div></ol></td><td><ol><div><li>Shutting down</li></div><div><li>expected:&lt;3&gt; but was:&lt;4&gt;</li></div></ol></td><td><ol><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.rsgroup.TestEnableRSGroup.testEnableRSGroup(TestEnableRSGroup.java:80)
</li></div><div><li>We did not find some load on the memstore expected:&lt;20&gt; but was:&lt;40&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.hadoop.hbase.master.TestCatalogJanitor.testParentCleanedEvenIfDaughterGoneFirst</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.hadoop.hbase.rsgroup.TestRSGroupsOfflineMode.org.apache.hadoop.hbase.rsgroup.TestRSGroupsOfflineMode</div></li><li><div>org.apache.hadoop.hbase.client.TestMultiParallel.testActiveThreadsCount</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.hadoop.hbase.rsgroup.TestEnableRSGroup.testEnableRSGroup</div></li><li><div>org.apache.hadoop.hbase.client.TestClientPushback.testClientTracksServerPushback</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hive"><div style="font-weight:bold;" class="panel-heading">HIVE</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>db4fe384fdf57bb89d1f468b68ae6625e6f0ba77</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 6881</div><div>Failed Count : 13</div><div>Skipped Count : 222</div></td><td><div>Total Count : 6881</div><div>Failed Count : 12</div><div>Skipped Count : 222</div></td><td><div>Total Count : 6881</div><div>Failed Count : 3</div><div>Skipped Count : 222</div></td><td><div>Total Count : 6881</div><div>Failed Count : 2</div><div>Skipped Count : 222</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol><div><li>org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan1</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan2</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan3</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan4</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan5</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan6</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan1</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan2</li></div><div><li>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</li></div></ol></td><td><ol><div><li>org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat</li></div><div><li>org.apache.hadoop.hive.llap.daemon.impl.comparator.TestAMReporter.testMultipleAM</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan1</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan2</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan3</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan4</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan5</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan6</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan1</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan2</li></div><div><li>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</li></div></ol></td><td><ol><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</li></div></ol></td><td><ol><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</li></div></ol></td></tr><tr><td>Description</td><td><ol><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat(TestHCatMultiOutputFormat.java:297)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:797)
	at org.apac</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div></ol></td><td><ol><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat(TestHCatMultiOutputFormat.java:297)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>test timed out after 5000 milliseconds</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:797)
	at org.apac</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div></ol></td><td><ol><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:797)
	at org.apac</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div></ol></td><td><ol><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:797)
	at org.apac</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.hadoop.hive.llap.daemon.impl.comparator.TestAMReporter.testMultipleAM</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="kafka"><div style="font-weight:bold;" class="panel-heading">KAFKA</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>ae31ee63dc3bdb9068543791bea44a12a97aa928</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 8778</div><div>Failed Count : 1</div><div>Skipped Count : 7</div></td><td><div>Total Count : 8778</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td><td><div>Total Count : 8778</div><div>Failed Count : 1</div><div>Skipped Count : 7</div></td><td><div>Total Count : 8778</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol><div><li>kafka.admin.DeleteTopicTest.testAddPartitionDuringDeleteTopic</li></div></ol></td><td><ol></ol></td><td><ol><div><li>kafka.api.TransactionsTest.testFencingOnTransactionExpiration</li></div></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol><div><li>kafka.admin.AdminOperationException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/topics/test</li></div></ol></td><td><ol></ol></td><td><ol><div><li>java.lang.AssertionError: expected:&lt;1&gt; but was:&lt;0&gt;</li></div></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>kafka.admin.DeleteTopicTest.testAddPartitionDuringDeleteTopic</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>kafka.api.TransactionsTest.testFencingOnTransactionExpiration</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="knox"><div style="font-weight:bold;" class="panel-heading">KNOX</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>4c1cb80cca7c26b11f32febd23ec887f13b7606d</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 947</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 947</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 947</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 947</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="metron"><div style="font-weight:bold;" class="panel-heading">METRON</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>e3eeec38a66e1f10c296c110d47bc8bc3e995629</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1631</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1631</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1631</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1631</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="oozie"><div style="font-weight:bold;" class="panel-heading">OOZIE</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>da8e5b5184a0afba033ae5358b7ca6a03a38b618</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 2096</div><div>Failed Count : 33</div><div>Skipped Count : 2</div></td><td><div>Total Count : 910</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 2096</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2096</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCredentialsSkip</li></div><div><li>org.apache.oozie.jms.TestDefaultConnectionContext.testThreadLocalSession</li></div><div><li>org.apache.oozie.jms.TestHCatMessageHandler.testDropEventTypeMessage</li></div><div><li>org.apache.oozie.jms.TestHCatMessageHandler.testCacheUpdateByMessage</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobSuccessEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuspendEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsAnd</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobFailureEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testConnectionDrop</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsOr</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuccessEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionWaitingEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobFailureEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionStartEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobStartedEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMissEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMissEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMissEvent</li></div><div><li>org.apache.oozie.service.TestJMSAccessorService.testConnectionRetry</li></div><div><li>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testPartitionDependency</li></div><div><li>org.apache.oozie.util.graph.TestGraphGenerator.testGraphWithManyNodes</li></div><div><li>org.apache.oozie.util.graph.TestGraphGenerator.testSimpleGraphPng</li></div><div><li>org.apache.oozie.util.graph.TestGraphGenerator.testSimpleGraphSvg</li></div><div><li>org.apache.oozie.util.graph.TestGraphGenerator.testGraphWithDecisionForkJoin</li></div></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol><div><li>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandActionNumbers</li></div></ol></td></tr><tr><td>Description</td><td><ol><div><li>JA020: Could not load credentials of type [abc] with name [abcname]]; perhaps it was not defined in oozie-site.xml?</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestDefaultConnectionContext.testThreadLocalSession(TestDefaultConnectionContext.java:74)
</li></div><div><li>Could not create Transport. Reason: javax.management.InstanceAlreadyExistsException: org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>Could not create Transport. Reason: javax.management.InstanceAlreadyExistsException: org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectors(TestJMSJobEventListener.java:544)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectorsNegative(TestJMSJobEventListener.java:568)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectors(TestJMSJobEventListener.java:239)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobSuccessEvent(TestJMSJobEventListener.java:477)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuspendEvent(TestJMSJobEventListener.java:214)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsAnd(TestJMSJobEventListener.java:316)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobFailureEvent(TestJMSJobEventListener.java:517)
</li></div><div><li>org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsNegative(TestJMSJobEventListener.java:262)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsOr(TestJMSJobEventListener.java:289)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuccessEvent(TestJMSJobEventListener.java:143)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionWaitingEvent(TestJMSJobEventListener.java:403)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobFailureEvent(TestJMSJobEventListener.java:180)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionStartEvent(TestJMSJobEventListener.java:439)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobStartedEvent(TestJMSJobEventListener.java:108)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMetEvent(TestJMSSLAEventListener.java:382)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMetEvent(TestJMSSLAEventListener.java:292)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectorsNegative(TestJMSSLAEventListener.java:261)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMetEvent(TestJMSSLAEventListener.java:332)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMissEvent(TestJMSSLAEventListener.java:103)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectors(TestJMSSLAEventListener.java:231)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMissEvent(TestJMSSLAEventListener.java:143)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMissEvent(TestJMSSLAEventListener.java:191)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.service.TestJMSAccessorService.testConnectionRetry(TestJMSAccessorService.java:183)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testPartitionDependency(TestPartitionDependencyManagerEhcache.java:47)
</li></div><div><li>java.util.concurrent.ExecutionException: java.lang.NullPointerException</li></div><div><li>Render and write PNG failed for graph-workflow-simple.xml: java.util.concurrent.TimeoutException</li></div><div><li>Render and write SVG failed: java.util.concurrent.TimeoutException</li></div><div><li>java.util.concurrent.TimeoutException</li></div></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol><div><li>expected:&lt;RUNNING&gt; but was:&lt;RUNNINGWITHERROR&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCredentialsSkip</div></li><li><div>org.apache.oozie.jms.TestDefaultConnectionContext.testThreadLocalSession</div></li><li><div>org.apache.oozie.jms.TestHCatMessageHandler.testDropEventTypeMessage</div></li><li><div>org.apache.oozie.jms.TestHCatMessageHandler.testCacheUpdateByMessage</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectors</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectorsNegative</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectors</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobSuccessEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuspendEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsAnd</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobFailureEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testConnectionDrop</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsNegative</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsOr</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuccessEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionWaitingEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobFailureEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionStartEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobStartedEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMetEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMetEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectorsNegative</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMetEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMissEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectors</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMissEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMissEvent</div></li><li><div>org.apache.oozie.service.TestJMSAccessorService.testConnectionRetry</div></li><li><div>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testPartitionDependency</div></li><li><div>org.apache.oozie.util.graph.TestGraphGenerator.testGraphWithManyNodes</div></li><li><div>org.apache.oozie.util.graph.TestGraphGenerator.testSimpleGraphPng</div></li><li><div>org.apache.oozie.util.graph.TestGraphGenerator.testSimpleGraphSvg</div></li><li><div>org.apache.oozie.util.graph.TestGraphGenerator.testGraphWithDecisionForkJoin</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandActionNumbers</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="phoenix"><div style="font-weight:bold;" class="panel-heading">PHOENIX</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>74228aee724e24ddb00bef2be0c7430172b699a8</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1668</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1668</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1668</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1668</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="pig"><div style="font-weight:bold;" class="panel-heading">PIG</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>1fcd7196e21117eb4c365f26adc19210f63fbdec</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 895</div><div>Failed Count : 20</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 20</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol><div><li>org.apache.pig.test.TestBuiltin.testRANDOMWithJob</li></div><div><li>org.apache.pig.test.TestBuiltin.testSFPig</li></div><div><li>org.apache.pig.test.TestBuiltin.testUniqueID</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString3</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString5</li></div><div><li>org.apache.pig.test.TestLoad.testLoadRemoteAbsScheme</li></div><div><li>org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194</li></div><div><li>org.apache.pig.test.TestPigServer.testRegisterRemoteScript</li></div><div><li>org.apache.pig.test.TestSchema.testEnabledDisambiguationPassesForDupeAliases</li></div><div><li>org.apache.pig.test.TestSchema.testSchemaSerialization</li></div><div><li>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColonsForNestedSchema</li></div><div><li>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColons</li></div><div><li>org.apache.pig.test.TestStore.testStore</li></div><div><li>org.apache.pig.test.TestStore.testStoreComplexDataWithNull</li></div><div><li>org.apache.pig.test.TestStore.testBinStorageGetSchema</li></div><div><li>org.apache.pig.test.TestStore.testStoreComplexData</li></div><div><li>org.apache.pig.test.TestStore.testSetStoreSchema</li></div><div><li>org.apache.pig.test.TestStore.testSuccessFileCreation1</li></div><div><li>org.apache.pig.test.TestStore.testCleanupOnFailureMultiStore</li></div><div><li>org.apache.pig.test.TestStore.testEmptyPartFileCreation</li></div></ol></td><td><ol><div><li>org.apache.pig.test.TestBuiltin.testRANDOMWithJob</li></div><div><li>org.apache.pig.test.TestBuiltin.testSFPig</li></div><div><li>org.apache.pig.test.TestBuiltin.testUniqueID</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString3</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString5</li></div><div><li>org.apache.pig.test.TestLoad.testLoadRemoteAbsScheme</li></div><div><li>org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194</li></div><div><li>org.apache.pig.test.TestPigServer.testRegisterRemoteScript</li></div><div><li>org.apache.pig.test.TestSchema.testEnabledDisambiguationPassesForDupeAliases</li></div><div><li>org.apache.pig.test.TestSchema.testSchemaSerialization</li></div><div><li>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColonsForNestedSchema</li></div><div><li>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColons</li></div><div><li>org.apache.pig.test.TestStore.testStore</li></div><div><li>org.apache.pig.test.TestStore.testStoreComplexDataWithNull</li></div><div><li>org.apache.pig.test.TestStore.testBinStorageGetSchema</li></div><div><li>org.apache.pig.test.TestStore.testStoreComplexData</li></div><div><li>org.apache.pig.test.TestStore.testSetStoreSchema</li></div><div><li>org.apache.pig.test.TestStore.testSuccessFileCreation1</li></div><div><li>org.apache.pig.test.TestStore.testCleanupOnFailureMultiStore</li></div><div><li>org.apache.pig.test.TestStore.testEmptyPartFileCreation</li></div></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol><div><li>Unable to open iterator for alias B</li></div><div><li>Input path does not exist: hdfs://localhost:36032/user/jenkins/testSFPig-output.txt</li></div><div><li>Unable to open iterator for alias B</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194(TestLocalRearrange.java:231)
</li></div><div><li>Unable to open iterator for alias b</li></div><div><li>Unable to open iterator for alias C</li></div><div><li>Unable to open iterator for alias c</li></div><div><li>Unable to open iterator for alias F</li></div><div><li>Unable to open iterator for alias E</li></div><div><li>File /tmp/TestStore/TestStore-output--2984800167240568105.txt does not exist.</li></div><div><li>File /tmp/TestStore/TestStore-output-5250542413113738315.txt does not exist.</li></div><div><li>Checking binstorage getSchema output</li></div><div><li>File /tmp/TestStore/TestStore-output--226586596485746708.txt does not exist.</li></div><div><li>Checking if file /tmp/TestStore/_commitJob_called does  exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Checking if _SUCCESS file exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Checking if file /tmp/TestStore/_setupTask_called1 does  exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>File /tmp/TestStore/TestStore-output-6188989128835050368.txt_1 does not exist.</li></div></ol></td><td><ol><div><li>Unable to open iterator for alias B</li></div><div><li>Input path does not exist: hdfs://localhost:46090/user/jenkins/testSFPig-output.txt</li></div><div><li>Unable to open iterator for alias B</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194(TestLocalRearrange.java:231)
</li></div><div><li>Unable to open iterator for alias b</li></div><div><li>Unable to open iterator for alias C</li></div><div><li>Unable to open iterator for alias c</li></div><div><li>Unable to open iterator for alias F</li></div><div><li>Unable to open iterator for alias E</li></div><div><li>File /tmp/TestStore/TestStore-output-4156664112248684963.txt does not exist.</li></div><div><li>File /tmp/TestStore/TestStore-output-3877441552249941111.txt does not exist.</li></div><div><li>Checking binstorage getSchema output</li></div><div><li>File /tmp/TestStore/TestStore-output--5876763387347128377.txt does not exist.</li></div><div><li>Checking if file /tmp/TestStore/_commitJob_called does  exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Checking if _SUCCESS file exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Checking if file /tmp/TestStore/_setupTask_called1 does  exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>File /tmp/TestStore/TestStore-output-2223274212162987091.txt_1 does not exist.</li></div></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="ranger"><div style="font-weight:bold;" class="panel-heading">RANGER</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>2aaa012dcf74dfd3952aabcc5c58681ab5d77054</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1033</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1033</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1033</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1033</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="slider"><div style="font-weight:bold;" class="panel-heading">SLIDER</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/develop</div><div><b>Last Revision: </b>8234cbe4e5ca676f39cf121bb8ea16778d624b58</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 607</div><div>Failed Count : 24</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 24</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol><div><li>org.apache.slider.agent.actions.TestActionExists.testExistsLiveCluster</li></div><div><li>org.apache.slider.agent.actions.TestActionList.testActionListSuite</li></div><div><li>org.apache.slider.agent.actions.TestActionStatus.testSuite</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeCommands.testFreezeCommands</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeThawFlexStandaloneAM.testFreezeThawFlexStandaloneAM</li></div><div><li>org.apache.slider.agent.rest.TestStandaloneREST.testStandaloneREST</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testBuildCluster</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testUpdateCluster</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMDestroy.testStandaloneAMDestroy</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMKill.testKillStandaloneAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithDefaultRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestart</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAgentAM.testStandaloneAgentAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneYarnRegistryAM.testStandaloneYarnRegistryAM</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsNoAppContainer</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsWithAppPackage</li></div><div><li>org.apache.slider.client.TestUpgradeCommandOptions.testAll</li></div><div><li>org.apache.slider.providers.agent.TestAddonPackage.testEchoApplicationAddPackage</li></div><div><li>org.apache.slider.providers.agent.TestAgentAAEcho.testAgentEcho</li></div><div><li>org.apache.slider.providers.agent.TestAgentAMManagementWS.testAgentAMManagementWS</li></div><div><li>org.apache.slider.providers.agent.TestAgentEcho.testAgentEcho</li></div><div><li>org.apache.slider.server.appmaster.TestDelayInContainerLaunch.testDelayInContainerLaunch</li></div><div><li>org.apache.slider.server.appmaster.web.rest.publisher.TestPublisherRestResources.testRestURIs</li></div></ol></td><td><ol><div><li>org.apache.slider.agent.actions.TestActionExists.testExistsLiveCluster</li></div><div><li>org.apache.slider.agent.actions.TestActionList.testActionListSuite</li></div><div><li>org.apache.slider.agent.actions.TestActionStatus.testSuite</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeCommands.testFreezeCommands</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeThawFlexStandaloneAM.testFreezeThawFlexStandaloneAM</li></div><div><li>org.apache.slider.agent.rest.TestStandaloneREST.testStandaloneREST</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testBuildCluster</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testUpdateCluster</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMDestroy.testStandaloneAMDestroy</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMKill.testKillStandaloneAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithDefaultRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestart</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAgentAM.testStandaloneAgentAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneYarnRegistryAM.testStandaloneYarnRegistryAM</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsNoAppContainer</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsWithAppPackage</li></div><div><li>org.apache.slider.client.TestUpgradeCommandOptions.testAll</li></div><div><li>org.apache.slider.providers.agent.TestAddonPackage.testEchoApplicationAddPackage</li></div><div><li>org.apache.slider.providers.agent.TestAgentAAEcho.testAgentEcho</li></div><div><li>org.apache.slider.providers.agent.TestAgentAMManagementWS.testAgentAMManagementWS</li></div><div><li>org.apache.slider.providers.agent.TestAgentEcho.testAgentEcho</li></div><div><li>org.apache.slider.server.appmaster.TestDelayInContainerLaunch.testDelayInContainerLaunch</li></div><div><li>org.apache.slider.server.appmaster.web.rest.publisher.TestPublisherRestResources.testRestURIs</li></div></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol><div><li>Launch failed with exit code -1</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Application not running: application_1521316218036_0001 state=FAILED </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Failed on local exception: java.io.FileNotFoundException: http://d4a861821570:37574/cluster/app/application_1521315762865_0001; Host Details : local host is: "localhost"; destination host is: "http://d4a861821570:37574/cluster/app/application_1521315762865_0001":37574; </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>assert report.yarnApplicationState == YarnApplicationState.RUNNING
       |      |                                            |
       |      FAILED                                       RUNNING
       applicationId { id: 1 cluster_timestamp: 1521316161717 } user: "jenkins" queue: "default" name: "testkillstandaloneam" host: "N/A" rpc_port: -1 yarn_application_state: FAILED trackingUrl: "http://d4</li></div><div><li>Cluster teststandaloneamrestartwithdefaultretrywindow not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestart not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestartwithretrywindow not live after 30000 ms</li></div><div><li>assert uri.port in 60000..60010
       |   |            |
       |   35276        [60000, 60001, 60002, 60003, 60004, 60005, 60006, 60007, 60008, 60009, 60010]
       http://d4a861821570:35276/cluster/app/application_1521316016531_0001</li></div><div><li>Application not running: application_1521316040788_0001 state=FAILED </li></div><div><li>assert 0 == status
         |  |
         |  -1
         false</li></div><div><li>Launch failed with exit code -1</li></div><div><li>Upgrade command should have failed</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div></ol></td><td><ol><div><li>Launch failed with exit code -1</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Application not running: application_1521334370709_0001 state=FAILED </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Failed on local exception: java.io.FileNotFoundException: http://2535ab413697:35649/cluster/app/application_1521334399823_0001; Host Details : local host is: "localhost"; destination host is: "http://2535ab413697:35649/cluster/app/application_1521334399823_0001":35649; </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>assert report.yarnApplicationState == YarnApplicationState.RUNNING
       |      |                                            |
       |      FAILED                                       RUNNING
       applicationId { id: 1 cluster_timestamp: 1521334001968 } user: "jenkins" queue: "default" name: "testkillstandaloneam" host: "N/A" rpc_port: -1 yarn_application_state: FAILED trackingUrl: "http://25</li></div><div><li>Cluster teststandaloneamrestartwithdefaultretrywindow not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestart not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestartwithretrywindow not live after 30000 ms</li></div><div><li>assert uri.port in 60000..60010
       |   |            |
       |   44837        [60000, 60001, 60002, 60003, 60004, 60005, 60006, 60007, 60008, 60009, 60010]
       http://2535ab413697:44837/cluster/app/application_1521333973526_0001</li></div><div><li>Application not running: application_1521334057828_0001 state=FAILED </li></div><div><li>assert 0 == status
         |  |
         |  -1
         false</li></div><div><li>Launch failed with exit code -1</li></div><div><li>Upgrade command should have failed</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="spark"><div style="font-weight:bold;" class="panel-heading">SPARK</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>8a1efe3076f29259151f1fba2ff894487efb6c4e</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 13797</div><div>Failed Count : 1</div><div>Skipped Count : 668</div></td><td><div>Total Count : 15476</div><div>Failed Count : 1</div><div>Skipped Count : 675</div></td><td><div>Total Count : 9466</div><div>Failed Count : 393</div><div>Skipped Count : 60</div></td><td><div>Total Count : 15477</div><div>Failed Count : 1</div><div>Skipped Count : 675</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol><div><li>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</li></div></ol></td><td><ol><div><li>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</li></div></ol></td><td><ol><div><li>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.mergeSpillsWithFileStreamAndSnappy</li></div><div><li>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.mergeSpillsWithTransferToAndSnappy</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</li></div><div><li>org.apache.spark.sql.CachedTableSuite.SPARK-19765: UNCACHE TABLE should un-cache all cached plans that refer to this table</li></div><div><li>org.apache.spark.sql.CachedTableSuite.refreshByPath should refresh all cached plans with the specified path</li></div><div><li>org.apache.spark.sql.DataFrameJoinSuite.broadcast join hint using broadcast function</li></div><div><li>org.apache.spark.sql.DataFrameSuite.inputFiles</li></div><div><li>org.apache.spark.sql.DataFrameSuite.SPARK-6941: Better error message for inserting into RDD-based Table</li></div><div><li>org.apache.spark.sql.DataFrameSuite.SPARK-11301: fix case sensitivity for filter on partitioned columns</li></div><div><li>org.apache.spark.sql.DataFrameSuite.fix case sensitivity of partition by</li></div><div><li>org.apache.spark.sql.DatasetSuite.SPARK-22472: add null check for top-level primitive values</li></div><div><li>org.apache.spark.sql.FileBasedDataSourceSuite.SPARK-23072 Write and read back unicode column names - parquet</li></div><div><li>org.apache.spark.sql.FileBasedDataSourceSuite.SPARK-22146 read files containing special characters using parquet</li></div><div><li>org.apache.spark.sql.FileBasedDataSourceSuite.Enabling/disabling ignoreMissingFiles using parquet</li></div><div><li>org.apache.spark.sql.MathFunctionsSuite.round/bround with table columns</li></div><div><li>org.apache.spark.sql.MetadataCacheSuite.SPARK-16336 Suggest doing table refresh when encountering FileNotFoundException</li></div><div><li>org.apache.spark.sql.MetadataCacheSuite.SPARK-16337 temporary view refresh</li></div><div><li>org.apache.spark.sql.MetadataCacheSuite.case sensitivity support in temporary view refresh</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.specifying database name for a temporary view is not allowed</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-15327: fail to compile generated code with complex data structure</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.data source table created in InMemoryCatalog should be able to read/write</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-15752 optimize metadata only query for datasource table</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-16975: Column-partition path starting '_' should be handled correctly</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-16674: field names containing dots for both fields and partitioned fields</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-18053: ARRAY equality is broken</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-19157: should be able to change spark.sql.runSQLOnFiles at runtime</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.should be able to resolve a persistent view</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-19059: read file based table whose name starts with underscore</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-23079: constraints should be inferred correctly with aliases</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-22356: overlapped columns between data and partition schema in data source tables</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.columnresolution-negative.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.columnresolution.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.describe-part-after-analyze.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.describe-table-column.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.null-handling.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.order-by-nulls-ordering.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.tablesample-negative.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.typeCoercion/native/decimalArithmeticOperations.sql</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.SPARK-18856: non-empty partitioned table should not report zero size</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.analyzing views is not supported</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.analyze column command - unsupported types and invalid columns</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.test table-level statistics for data source table</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.analyze column command - result verification</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.column stats collection for null columns</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.change stats after truncate command</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.change stats after set location command</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.change stats after insert command for datasource table</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.invalidation of tableRelationCache after inserts</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.invalidation of tableRelationCache after table truncation</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.invalidation of tableRelationCache after alter table add partition</li></div><div><li>org.apache.spark.sql.SubquerySuite.SPARK-21835: Join in correlated subquery should be duplicateResolved: case 1</li></div><div><li>org.apache.spark.sql.SubquerySuite.SPARK-21835: Join in correlated subquery should be duplicateResolved: case 2</li></div><div><li>org.apache.spark.sql.UDFSuite.SPARK-8005 input_file_name</li></div><div><li>org.apache.spark.sql.UserDefinedTypeSuite.Standard mode - UDTs with Parquet</li></div><div><li>org.apache.spark.sql.UserDefinedTypeSuite.Legacy mode - UDTs with Parquet</li></div><div><li>org.apache.spark.sql.UserDefinedTypeSuite.Standard mode - Repartition UDTs with Parquet</li></div><div><li>org.apache.spark.sql.UserDefinedTypeSuite.Legacy mode - Repartition UDTs with Parquet</li></div><div><li>org.apache.spark.sql.execution.DataSourceScanExecRedactionSuite.treeString is redacted</li></div><div><li>org.apache.spark.sql.execution.DataSourceScanExecRedactionSuite.explain is redacted using SQLConf</li></div><div><li>org.apache.spark.sql.execution.GlobalTempViewSuite.CREATE GLOBAL TEMP VIEW USING</li></div><div><li>org.apache.spark.sql.execution.SameResultSuite.FileSourceScanExec: different orders of data filters and partition filters</li></div><div><li>org.apache.spark.sql.execution.SimpleSQLViewSuite.create view for partitioned parquet table</li></div><div><li>org.apache.spark.sql.execution.SimpleSQLViewSuite.Using view after adding more columns</li></div><div><li>org.apache.spark.sql.execution.SimpleSQLViewSuite.correctly handle a view with custom column names</li></div><div><li>org.apache.spark.sql.execution.SimpleSQLViewSuite.resolve a view when the dataTypes of referenced table columns changed</li></div><div><li>org.apache.spark.sql.execution.SimpleSQLViewSuite.sparkSession API view resolution with different default database</li></div><div><li>org.apache.spark.sql.execution.WholeStageCodegenSuite.Skip splitting consume function when parameter number exceeds JVM limit</li></div><div><li>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite.SPARK-22673: InMemoryRelation should utilize existing stats of the plan to be cached</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create partitioned data source table without user specified schema</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create partitioned data source table with user specified schema</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create non-partitioned data source table without user specified schema</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create non-partitioned data source table with user specified schema</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.alter table: rename cached table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create Data Source Table As Select</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.truncate table - datasource table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.truncate partitioned table - datasource table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.create temporary view with mismatched schema</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.truncate table - external table, temporary table, view (not allowed)</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.truncate table - non-partitioned table (not allowed)</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.SPARK-16034 Partition columns should match when appending to existing data source tables</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.SET LOCATION for managed table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.insert data to a data source table which has a non-existing location should succeed</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.insert into a data source table with a non-existing partition location should succeed</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.read data from a data source table with non-existing partition location should succeed</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.create datasource table with a non-existing location</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.CTAS for external data source table with a non-existing location</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.CTAS for external data source table with a existed location</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.data source table:partition column name containing a b</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.data source table:partition column name containing a:b</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.data source table:partition column name containing a%b</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.data source table:partition column name containing a,b</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a b for datasource table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a:b for datasource table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a%b for datasource table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a b for database</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a:b for database</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a%b for database</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.alter datasource table add columns - parquet</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.alter datasource table add columns - partitioned - parquet</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.basic DDL using locale tr - caseSensitive true</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.basic DDL using locale tr - caseSensitive false</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create Hive Table As Select</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.SPARK-23348: append data to data source table with saveAsTable</li></div><div><li>org.apache.spark.sql.execution.datasources.FileFormatWriterSuite.empty file should be skipped while write to file</li></div><div><li>org.apache.spark.sql.execution.datasources.FileFormatWriterSuite.SPARK-22252: FileFormatWriter should respect the input query schema</li></div><div><li>org.apache.spark.sql.execution.datasources.FileIndexSuite.SPARK-20367 - properly unescape column names in inferPartitioning</li></div><div><li>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite.[SPARK-16818] partition pruned file scans implement sameResult correctly</li></div><div><li>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite.[SPARK-16818] exchange reuse respects differences in partition pruning</li></div><div><li>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite.[SPARK-18753] keep pushed-down null literal as a filter in Spark-side post-filter</li></div><div><li>org.apache.spark.sql.execution.datasources.HadoopFsRelationSuite.sizeInBytes should be the total size of all files</li></div><div><li>org.apache.spark.sql.execution.datasources.HadoopFsRelationSuite.SPARK-22790: spark.sql.sources.compressionFactor takes effect</li></div><div><li>org.apache.spark.sql.execution.datasources.json.JsonSuite.SPARK-7565 MapType in JsonRDD</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetCompressionCodecPrecedenceSuite.Create parquet table with compression</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite.All Types Dictionary</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite.All Types Null</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite.Read row group containing both dictionary and plain encoded pages</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite.read parquet footers in parallel</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.basic data types (without binary)</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.raw binary</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.string</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - fixed-length decimals</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - fixed-length decimals</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.date type</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - map</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - map</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - array</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - array</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - array and double</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - array and double</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - struct</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - struct</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - nested struct with array of array as field</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - nested struct with array of array as field</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - nested map with struct as value type</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - nested map with struct as value type</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.nulls</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.nones</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.compression codec</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.save - overwrite</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.save - ignore</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.save - throw</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.save - append</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-8121: spark.sql.parquet.output.committer.class shouldn't be overridden</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-7837 Do not close output writer twice when commitTask() fails</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-11044 Parquet writer version fixed as version1</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.null and non-null strings</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.read dictionary and plain encoded timestamp_millis written as INT64</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-12589 copy() on rows returned from reader works for strings</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.VectorizedParquetRecordReader - direct path read</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.VectorizedParquetRecordReader - partition column types</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-23173 Writing a file with data converted from JSON with and incorrect user schema</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetInteroperabilitySuite.parquet timestamp conversion</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite.schema merging failure error message</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite.Read Parquet file generated by parquet-thrift</li></div><div><li>org.apache.spark.sql.execution.metric.SQLMetricsSuite.range metrics</li></div><div><li>org.apache.spark.sql.execution.metric.SQLMetricsSuite.writing data out metrics: parquet</li></div><div><li>org.apache.spark.sql.execution.metric.SQLMetricsSuite.writing data out metrics with dynamic partition: parquet</li></div><div><li>org.apache.spark.sql.internal.CatalogSuite.dropTempView should not un-cache and drop metastore table if a same-name table exists</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.read bucketed data</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.avoid shuffle when join 2 bucketed tables</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only shuffle one side when join bucketed table and non-bucketed table</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only shuffle one side when 2 bucketed tables have different bucket number</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only shuffle one side when 2 bucketed tables have different bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.shuffle when join keys are not equal to bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.shuffle when join 2 bucketed tables with bucketing disabled</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.check sort and shuffle when bucket and sort columns are join keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.avoid shuffle and sort when sort columns are a super set of join keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only sort one side when sort columns are different</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only sort one side when sort columns are same but their ordering is different</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.avoid shuffle when grouping keys are equal to bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.avoid shuffle when grouping keys are a super-set of bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.SPARK-17698 Join predicates should not contain filter clauses</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.SPARK-19122 Re-order join predicates if they match with the child's output partitioning</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.SPARK-19122 No re-ordering should happen if set of join columns != set of child's partitioning columns</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.SPARK-22042 ReorderJoinPredicates can break when child's partitioning is not decided</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.error if there exists any malformed bucket files</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.disable bucketing when the output doesn't contain all bucketing columns</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data with sortBy</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data without partitionBy</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data without partitionBy with sortBy</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data with bucketing disabled</li></div><div><li>org.apache.spark.sql.sources.CreateTableAsSelectSuite.create table using as select - with partitioned by</li></div><div><li>org.apache.spark.sql.sources.CreateTableAsSelectSuite.create table using as select - with valid number of buckets</li></div><div><li>org.apache.spark.sql.sources.CreateTableAsSelectSuite.SPARK-17409: CTAS of decimal calculation</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.write many partitions</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.write many partitions with repeats</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.partitioned columns should appear at the end of schema</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.maxRecordsPerFile setting in non-partitioned write path</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.maxRecordsPerFile setting in dynamic partition writes</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.append data to an existing partitioned table without custom partition path</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.timeZone setting in dynamic partition writes</li></div><div><li>org.apache.spark.sql.streaming.DeduplicateSuite.deduplicate with file sink</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSinkSuite.unpartitioned writing and batch reading</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSinkSuite.SPARK-21167: encode and decode path correctly</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSinkSuite.partitioned writing and batch reading</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSinkSuite.partitioned writing and batch reading with 'basePath'</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSinkSuite.writing with aggregation</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSourceSuite.FileStreamSource schema: parquet, existing files, no schema</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSourceSuite.FileStreamSource schema: parquet, existing files, schema</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSourceSuite.read from parquet files</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSourceSuite.read from parquet files with changing schema</li></div><div><li>org.apache.spark.sql.streaming.FileStreamStressSuite.fault tolerance stress test - unpartitioned output</li></div><div><li>org.apache.spark.sql.streaming.FileStreamStressSuite.fault tolerance stress test - partitioned output</li></div><div><li>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite.SPARK-18510: use user specified types for partition columns in file sources</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.write path implements onTaskCommit API correctly</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.parquet - API and behavior regarding schema</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.column nullability and comment - write and then read</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-17230: write out results of decimal calculation</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Append should not fail if the table not exists but a same-name temp view exist</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Append should not fail if the table already exists and a same-name temp view exist</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode ErrorIfExists should not fail if the table not exists but a same-name temp view exist</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Overwrite should not drop the temp view if the table not exists but a same-name temp view exist</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Overwrite should not fail if the table already exists and a same-name temp view exist</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Ignore should create the table if the table not exists but a same-name temp view exist</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-18510: use user specified types for partition columns in file sources</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-18899: append to a bucketed table using DataFrameWriter with mismatched bucketing</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-18913: append to a table with special column names</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-20460 Check name duplication in schema</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.use Spark jobs to list files</li></div><div><li>org.apache.spark.sql.util.DataFrameCallbackSuite.execute callback functions for DataFrameWriter</li></div><div><li>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</li></div><div><li>org.apache.spark.sql.hive.CachedTableSuite.REFRESH TABLE also needs to recache the data (data source tables)</li></div><div><li>org.apache.spark.sql.hive.CachedTableSuite.SPARK-15678: REFRESH PATH</li></div><div><li>org.apache.spark.sql.hive.CachedTableSuite.Cache/Uncache Qualified Tables</li></div><div><li>org.apache.spark.sql.hive.CachedTableSuite.SPARK-11246 cache parquet table</li></div><div><li>org.apache.spark.sql.hive.CompressionCodecSuite.both table-level and session-level compression are set</li></div><div><li>org.apache.spark.sql.hive.CompressionCodecSuite.table-level compression is not set but session-level compressions is set</li></div><div><li>org.apache.spark.sql.hive.CompressionCodecSuite.test table containing mixed compression codec</li></div><div><li>org.apache.spark.sql.hive.HiveParquetSuite.Case insensitive attribute names</li></div><div><li>org.apache.spark.sql.hive.HiveParquetSuite.SELECT on Parquet table</li></div><div><li>org.apache.spark.sql.hive.HiveParquetSuite.Simple column projection + filter on Parquet table</li></div><div><li>org.apache.spark.sql.hive.HiveParquetSuite.Converting Hive to Parquet Table via saveAsParquetFile</li></div><div><li>org.apache.spark.sql.hive.HiveParquetSuite.INSERT OVERWRITE TABLE Parquet table</li></div><div><li>org.apache.spark.sql.hive.InsertSuite.Data source table - INSERT OVERWRITE - partition IF NOT EXISTS</li></div><div><li>org.apache.spark.sql.hive.InsertSuite.Data source table - SPARK-16037: INSERT statement should match columns by position</li></div><div><li>org.apache.spark.sql.hive.InsertSuite.Data source table - INSERT INTO a partitioned table (semantic and error handling)</li></div><div><li>org.apache.spark.sql.hive.InsertSuite.Data source table - insertInto() should match columns by position and ignore column names</li></div><div><li>org.apache.spark.sql.hive.InsertSuite.Data source table - insertInto() should match unnamed columns by position</li></div><div><li>org.apache.spark.sql.hive.InsertSuite.SPARK-21165: FileFormatWriter should only rely on attributes from analyzed plan</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.convert partition provider to hive with repair table</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.when partition management is enabled, new tables have partition provider hive</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.when partition management is disabled, new tables have no partition provider</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.when partition management is disabled, we preserve the old behavior even for new tables</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert overwrite partition of legacy datasource table</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert overwrite partition of new datasource table overwrites just partition</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18544 append with saveAsTable - partition management true</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18635 special chars in partition values - partition management true</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18659 insert overwrite table files - partition management true</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18659 insert overwrite table with lowercase - partition management true</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-19887 partition value is null - partition management true</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18544 append with saveAsTable - partition management false</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18635 special chars in partition values - partition management false</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18659 insert overwrite table files - partition management false</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18659 insert overwrite table with lowercase - partition management false</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-19887 partition value is null - partition management false</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert into partial dynamic partitions</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert into fully dynamic partitions</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert into static partition</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.overwrite partial dynamic partitions</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.overwrite fully dynamic partitions</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.overwrite static partition</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.append data with DataFrameWriter</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-19359: renaming partition should not leave useless directories</li></div><div><li>org.apache.spark.sql.hive.ShowCreateTableSuite.data source table using Dataset API</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.SPARK-18856: non-empty partitioned table should not report zero size</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.size estimation for relations is based on row size * number of rows</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.analyze non hive compatible datasource tables</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.test statistics of LogicalRelation converted from Hive serde tables</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.verify serialized column stats after analyzing columns</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.verify column stats can be deserialized from tblproperties</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.serialization and deserialization of histograms to/from hive metastore</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.test table-level statistics for data source table created in HiveExternalCatalog</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.test table-level statistics for partitioned data source table</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.test refreshing table stats of cached data source table by `ANALYZE TABLE` statement</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create partitioned data source table without user specified schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create partitioned data source table with user specified schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create non-partitioned data source table without user specified schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create non-partitioned data source table with user specified schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.alter table: rename cached table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create Data Source Table As Select</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.truncate table - datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.truncate partitioned table - datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.create temporary view with mismatched schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.truncate table - external table, temporary table, view (not allowed)</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.truncate table - non-partitioned table (not allowed)</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.SPARK-16034 Partition columns should match when appending to existing data source tables</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.SET LOCATION for managed table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.insert data to a data source table which has a non-existing location should succeed</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.insert into a data source table with a non-existing partition location should succeed</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.read data from a data source table with non-existing partition location should succeed</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.create datasource table with a non-existing location</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.CTAS for external data source table with a non-existing location</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.CTAS for external data source table with a existed location</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.data source table:partition column name containing a b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.data source table:partition column name containing a:b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.data source table:partition column name containing a%b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.data source table:partition column name containing a,b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a b for datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a:b for datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a%b for datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a b for database</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a:b for database</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a%b for database</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.alter datasource table add columns - parquet</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.alter datasource table add columns - partitioned - parquet</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.basic DDL using locale tr - caseSensitive true</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.basic DDL using locale tr - caseSensitive false</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.drop external tables in default database</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.drop external data source table in default database</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.SPARK-19129: drop partition with a empty string will drop the whole table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.drop views</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter views - rename</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter views - set/unset tblproperties</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter views and alter table - misuse</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.drop view using drop table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.create view with mismatched schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.CREATE TABLE LIKE a temporary view</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.CREATE TABLE LIKE an external data source table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.CREATE TABLE LIKE a view</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.desc table for data source table - no user-defined schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.truncate table - datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.truncate partitioned table - datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.create hive serde table with Catalog</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.create hive serde table with DataFrameWriter.saveAsTable</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.append data to hive serde table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.partitioned table should always put partition columns at the end of table schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.partition column name of parquet table containing a b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.partition column name of parquet table containing a:b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.partition column name of parquet table containing a%b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.partition column name of parquet table containing a,b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter hive serde table add columns -- partitioned - PARQUET</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter hive serde table add columns -- with predicate - PARQUET</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.SPARK-22252: FileFormatWriter should respect the input query schema in HIVE</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.create view for partitioned parquet table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.Using view after adding more columns</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.correctly handle a view with custom column names</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.resolve a view when the dataTypes of referenced table columns changed</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.sparkSession API view resolution with different default database</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.create a permanent/temp view using a hive, built-in, and permanent user function</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.create a permanent/temp view using a temporary function</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.SPARK-14933 - create view from hive parquet table</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-6851: Self-joined converted parquet tables</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.CTAS without serde without location</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.CTAS with default fileformat</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.CTAS without serde with location</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.CTAS with serde</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.specifying database name for a temporary view is not allowed</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-10741: Sort on Aggregate using parquet</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.run sql directly on files - parquet</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.run sql directly on files - hive</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-11453: append data to partitioned table</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.insert into datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-15752 optimize metadata only query for hive table</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-17354: Partitioning by dates/timestamps works with Parquet vectorized reader</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-19912 String literals should be escaped for Hive metastore partition pruning</li></div><div><li>org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite.saveAsTable()/load() - partitioned table - boolean type</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.read bucketed data</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.avoid shuffle when join 2 bucketed tables</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only shuffle one side when join bucketed table and non-bucketed table</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only shuffle one side when 2 bucketed tables have different bucket number</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only shuffle one side when 2 bucketed tables have different bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.shuffle when join keys are not equal to bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.shuffle when join 2 bucketed tables with bucketing disabled</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.check sort and shuffle when bucket and sort columns are join keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.avoid shuffle and sort when sort columns are a super set of join keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only sort one side when sort columns are different</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only sort one side when sort columns are same but their ordering is different</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.avoid shuffle when grouping keys are equal to bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.avoid shuffle when grouping keys are a super-set of bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.SPARK-17698 Join predicates should not contain filter clauses</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.SPARK-19122 Re-order join predicates if they match with the child's output partitioning</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.SPARK-19122 No re-ordering should happen if set of join columns != set of child's partitioning columns</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.SPARK-22042 ReorderJoinPredicates can break when child's partitioning is not decided</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.error if there exists any malformed bucket files</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.disable bucketing when the output doesn't contain all bucketing columns</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data with sortBy</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data without partitionBy</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data without partitionBy with sortBy</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data with bucketing disabled</li></div><div><li>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite.saveAsTable()/load() - partitioned table - boolean type</li></div></ol></td><td><ol><div><li>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</li></div></ol></td></tr><tr><td>Description</td><td><ol><div><li>Timeout of './bin/spark-submit' '--class' 'org.apache.spark.sql.hive.SparkSQLConfTest' '--name' 'SparkSQLConfTest' '--master' 'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.hive.metastore.version=0.12' '--conf' 'spark.sql.hive.metastore.jars=maven' '--driver-java-options' '-Dderby.system.durability=test' 'file:/var/lib/jen</li></div></ol></td><td><ol><div><li>Timeout of './bin/spark-submit' '--class' 'org.apache.spark.sql.hive.SparkSQLConfTest' '--name' 'SparkSQLConfTest' '--master' 'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.hive.metastore.version=0.12' '--conf' 'spark.sql.hive.metastore.jars=maven' '--driver-java-options' '-Dderby.system.durability=test' 'file:/var/lib/jen</li></div></ol></td><td><ol><div><li>java.lang.reflect.InvocationTargetException
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.testMergingSpills(UnsafeShuffleWriterSuite.java:311)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.testMergingSpills(UnsafeShuffleWriterSuite.java:296)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.mergeSpillsWithFileStreamAndSnappy(UnsafeShuffleWriterSuite.java:364)
Cau</li></div><div><li>java.lang.reflect.InvocationTargetException
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.testMergingSpills(UnsafeShuffleWriterSuite.java:311)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.testMergingSpills(UnsafeShuffleWriterSuite.java:296)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.mergeSpillsWithTransferToAndSnappy(UnsafeShuffleWriterSuite.java:359)
Cau</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #2&amp;#010;CREATE TABLE t1 USING parquet AS SELECT 1 AS i1</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #2&amp;#010;CREATE TABLE t1 USING parquet AS SELECT 1 AS i1</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #1&amp;#010;INSERT INTO TABLE t PARTITION (ds='2017-08-01', hr=10)&amp;#010;VALUES ('k1', 100), ('k2', 200), ('k3', 300)</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #17&amp;#010;INSERT INTO desc_col_table values 1, 2, 3, 4</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #1&amp;#010;insert into t1 values(1,0,0)</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #1&amp;#010;INSERT INTO spark_10747 VALUES (6, 12, 10), (6, 11, 4), (6, 9, 10), (6, 15, 8),&amp;#010;(6, 15, 8), (6, 7, 4), (6, 7, 8), (6, 13, null), (6, 10, null)</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #2&amp;#010;CREATE TABLE t1 USING parquet AS SELECT 1 AS i1</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #5&amp;#010;insert into decimals_test values(1, 100.0, 999.0), (2, 12345.123, 12345.123),&amp;#010;  (3, 0.1234567891011, 1234.1), (4, 123456789123456789.0, 1.123456789123456789)</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>"[Job aborted due to stage failure: Task 1 in stage 37.0 failed 1 times, most recent failure: Lost task 1.0 in stage 37.0 (TID 59, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows.&amp;#010; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:294)&amp;#010</li></div><div><li>"Job aborted due to stage failure: Task 0 in stage 38.0 failed 1 times, most recent failure: Lost task 0.0 in stage 38.0 (TID 60, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows.&amp;#010; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:294)&amp;#010;</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted due to stage failure: Task 0 in stage 68.0 failed 1 times, most recent failure: Lost task 0.0 in stage 68.0 (TID 98, localhost, executor driver): java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy&amp;#010; at org.apache.parquet.hadoop.codec.SnappyDecompressor.decompress(SnappyDecompressor.java:62)&amp;#010; at org.apache.parquet.hadoop.codec.NonBlockedDecompre</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy&amp;#010; at org.apache.parquet.hadoop.codec.SnappyDecompressor.decompress(SnappyDecompressor.java:62)&amp;#010; at org.apache.parquet.hadoop.codec.NonBlockedDecompresso</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Query [id = c14f77b2-c26c-4db0-a833-c04b560d0686, runId = 0972b39a-7860-4b6c-947d-56b1ed1f099d] terminated with exception: Job aborted.</li></div><div><li>Query [id = 9eaf2abd-f410-45fb-be48-a48f41128d14, runId = 11c75dcc-e6c1-4215-81b6-8e7250380223] terminated with exception: Job aborted.</li></div><div><li>Query [id = 17c8e767-b119-49d0-a250-e9d0dd032bfb, runId = 795ca1c4-ec3c-4869-a53f-4cc6bd8a07b8] terminated with exception: Job aborted.</li></div><div><li>Query [id = 0d5290f8-abad-46fd-9254-129df8225ba3, runId = 5777fb2c-0a8d-4736-93b9-305286a1fc0f] terminated with exception: Job aborted.</li></div><div><li>Query [id = 408f760f-82e0-43b2-82c3-e1958f432970, runId = 35069209-7215-472a-8061-6053d42deadc] terminated with exception: Job aborted.</li></div><div><li>Query [id = 587e81cd-fc48-4c02-81ed-ebf8b31feffd, runId = cbaacf2d-2f57-4928-9a66-8180190c7de2] terminated with exception: Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>&amp;#010;Error adding data: Job aborted.&amp;#010;org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:233)&amp;#010; org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)&amp;#010; org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)&amp;#010; org.apac</li></div><div><li>Job aborted.</li></div><div><li>Query [id = 9709b621-ffb3-4fcb-9fe9-2951136c7971, runId = bb7bd79d-91fe-40d0-8b28-455046f6689f] terminated with exception: Job aborted.</li></div><div><li>Query [id = 42d65a65-4665-4bf2-9e8d-519ff97c33b7, runId = 13b1f681-e6ee-4f9c-b655-7c488c257f8e] terminated with exception: Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div></ol></td><td><ol><div><li>Timeout of './bin/spark-submit' '--class' 'org.apache.spark.sql.hive.SparkSQLConfTest' '--name' 'SparkSQLConfTest' '--master' 'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.hive.metastore.version=0.12' '--conf' 'spark.sql.hive.metastore.jars=maven' '--driver-java-options' '-Dderby.system.durability=test' 'file:/var/lib/jen</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.mergeSpillsWithFileStreamAndSnappy</div></li><li><div>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.mergeSpillsWithTransferToAndSnappy</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</div></li><li><div>org.apache.spark.sql.CachedTableSuite.SPARK-19765: UNCACHE TABLE should un-cache all cached plans that refer to this table</div></li><li><div>org.apache.spark.sql.CachedTableSuite.refreshByPath should refresh all cached plans with the specified path</div></li><li><div>org.apache.spark.sql.DataFrameJoinSuite.broadcast join hint using broadcast function</div></li><li><div>org.apache.spark.sql.DataFrameSuite.inputFiles</div></li><li><div>org.apache.spark.sql.DataFrameSuite.SPARK-6941: Better error message for inserting into RDD-based Table</div></li><li><div>org.apache.spark.sql.DataFrameSuite.SPARK-11301: fix case sensitivity for filter on partitioned columns</div></li><li><div>org.apache.spark.sql.DataFrameSuite.fix case sensitivity of partition by</div></li><li><div>org.apache.spark.sql.DatasetSuite.SPARK-22472: add null check for top-level primitive values</div></li><li><div>org.apache.spark.sql.FileBasedDataSourceSuite.SPARK-23072 Write and read back unicode column names - parquet</div></li><li><div>org.apache.spark.sql.FileBasedDataSourceSuite.SPARK-22146 read files containing special characters using parquet</div></li><li><div>org.apache.spark.sql.FileBasedDataSourceSuite.Enabling/disabling ignoreMissingFiles using parquet</div></li><li><div>org.apache.spark.sql.MathFunctionsSuite.round/bround with table columns</div></li><li><div>org.apache.spark.sql.MetadataCacheSuite.SPARK-16336 Suggest doing table refresh when encountering FileNotFoundException</div></li><li><div>org.apache.spark.sql.MetadataCacheSuite.SPARK-16337 temporary view refresh</div></li><li><div>org.apache.spark.sql.MetadataCacheSuite.case sensitivity support in temporary view refresh</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.specifying database name for a temporary view is not allowed</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.SPARK-15327: fail to compile generated code with complex data structure</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.data source table created in InMemoryCatalog should be able to read/write</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.SPARK-15752 optimize metadata only query for datasource table</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.SPARK-16975: Column-partition path starting '_' should be handled correctly</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.SPARK-16674: field names containing dots for both fields and partitioned fields</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.SPARK-18053: ARRAY equality is broken</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.SPARK-19157: should be able to change spark.sql.runSQLOnFiles at runtime</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.should be able to resolve a persistent view</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.SPARK-19059: read file based table whose name starts with underscore</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.SPARK-23079: constraints should be inferred correctly with aliases</div></li><li><div>org.apache.spark.sql.SQLQuerySuite.SPARK-22356: overlapped columns between data and partition schema in data source tables</div></li><li><div>org.apache.spark.sql.SQLQueryTestSuite.columnresolution-negative.sql</div></li><li><div>org.apache.spark.sql.SQLQueryTestSuite.columnresolution.sql</div></li><li><div>org.apache.spark.sql.SQLQueryTestSuite.describe-part-after-analyze.sql</div></li><li><div>org.apache.spark.sql.SQLQueryTestSuite.describe-table-column.sql</div></li><li><div>org.apache.spark.sql.SQLQueryTestSuite.null-handling.sql</div></li><li><div>org.apache.spark.sql.SQLQueryTestSuite.order-by-nulls-ordering.sql</div></li><li><div>org.apache.spark.sql.SQLQueryTestSuite.tablesample-negative.sql</div></li><li><div>org.apache.spark.sql.SQLQueryTestSuite.typeCoercion/native/decimalArithmeticOperations.sql</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.SPARK-18856: non-empty partitioned table should not report zero size</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.analyzing views is not supported</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.analyze column command - unsupported types and invalid columns</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.test table-level statistics for data source table</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.analyze column command - result verification</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.column stats collection for null columns</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.change stats after truncate command</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.change stats after set location command</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.change stats after insert command for datasource table</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.invalidation of tableRelationCache after inserts</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.invalidation of tableRelationCache after table truncation</div></li><li><div>org.apache.spark.sql.StatisticsCollectionSuite.invalidation of tableRelationCache after alter table add partition</div></li><li><div>org.apache.spark.sql.SubquerySuite.SPARK-21835: Join in correlated subquery should be duplicateResolved: case 1</div></li><li><div>org.apache.spark.sql.SubquerySuite.SPARK-21835: Join in correlated subquery should be duplicateResolved: case 2</div></li><li><div>org.apache.spark.sql.UDFSuite.SPARK-8005 input_file_name</div></li><li><div>org.apache.spark.sql.UserDefinedTypeSuite.Standard mode - UDTs with Parquet</div></li><li><div>org.apache.spark.sql.UserDefinedTypeSuite.Legacy mode - UDTs with Parquet</div></li><li><div>org.apache.spark.sql.UserDefinedTypeSuite.Standard mode - Repartition UDTs with Parquet</div></li><li><div>org.apache.spark.sql.UserDefinedTypeSuite.Legacy mode - Repartition UDTs with Parquet</div></li><li><div>org.apache.spark.sql.execution.DataSourceScanExecRedactionSuite.treeString is redacted</div></li><li><div>org.apache.spark.sql.execution.DataSourceScanExecRedactionSuite.explain is redacted using SQLConf</div></li><li><div>org.apache.spark.sql.execution.GlobalTempViewSuite.CREATE GLOBAL TEMP VIEW USING</div></li><li><div>org.apache.spark.sql.execution.SameResultSuite.FileSourceScanExec: different orders of data filters and partition filters</div></li><li><div>org.apache.spark.sql.execution.SimpleSQLViewSuite.create view for partitioned parquet table</div></li><li><div>org.apache.spark.sql.execution.SimpleSQLViewSuite.Using view after adding more columns</div></li><li><div>org.apache.spark.sql.execution.SimpleSQLViewSuite.correctly handle a view with custom column names</div></li><li><div>org.apache.spark.sql.execution.SimpleSQLViewSuite.resolve a view when the dataTypes of referenced table columns changed</div></li><li><div>org.apache.spark.sql.execution.SimpleSQLViewSuite.sparkSession API view resolution with different default database</div></li><li><div>org.apache.spark.sql.execution.WholeStageCodegenSuite.Skip splitting consume function when parameter number exceeds JVM limit</div></li><li><div>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite.SPARK-22673: InMemoryRelation should utilize existing stats of the plan to be cached</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create partitioned data source table without user specified schema</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create partitioned data source table with user specified schema</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create non-partitioned data source table without user specified schema</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create non-partitioned data source table with user specified schema</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.alter table: rename cached table</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create Data Source Table As Select</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.truncate table - datasource table</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.truncate partitioned table - datasource table</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.create temporary view with mismatched schema</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.truncate table - external table, temporary table, view (not allowed)</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.truncate table - non-partitioned table (not allowed)</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.SPARK-16034 Partition columns should match when appending to existing data source tables</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.SET LOCATION for managed table</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.insert data to a data source table which has a non-existing location should succeed</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.insert into a data source table with a non-existing partition location should succeed</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.read data from a data source table with non-existing partition location should succeed</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.create datasource table with a non-existing location</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.CTAS for external data source table with a non-existing location</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.CTAS for external data source table with a existed location</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.data source table:partition column name containing a b</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.data source table:partition column name containing a:b</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.data source table:partition column name containing a%b</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.data source table:partition column name containing a,b</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a b for datasource table</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a:b for datasource table</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a%b for datasource table</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a b for database</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a:b for database</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a%b for database</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.alter datasource table add columns - parquet</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.alter datasource table add columns - partitioned - parquet</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.basic DDL using locale tr - caseSensitive true</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.basic DDL using locale tr - caseSensitive false</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create Hive Table As Select</div></li><li><div>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.SPARK-23348: append data to data source table with saveAsTable</div></li><li><div>org.apache.spark.sql.execution.datasources.FileFormatWriterSuite.empty file should be skipped while write to file</div></li><li><div>org.apache.spark.sql.execution.datasources.FileFormatWriterSuite.SPARK-22252: FileFormatWriter should respect the input query schema</div></li><li><div>org.apache.spark.sql.execution.datasources.FileIndexSuite.SPARK-20367 - properly unescape column names in inferPartitioning</div></li><li><div>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite.[SPARK-16818] partition pruned file scans implement sameResult correctly</div></li><li><div>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite.[SPARK-16818] exchange reuse respects differences in partition pruning</div></li><li><div>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite.[SPARK-18753] keep pushed-down null literal as a filter in Spark-side post-filter</div></li><li><div>org.apache.spark.sql.execution.datasources.HadoopFsRelationSuite.sizeInBytes should be the total size of all files</div></li><li><div>org.apache.spark.sql.execution.datasources.HadoopFsRelationSuite.SPARK-22790: spark.sql.sources.compressionFactor takes effect</div></li><li><div>org.apache.spark.sql.execution.datasources.json.JsonSuite.SPARK-7565 MapType in JsonRDD</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetCompressionCodecPrecedenceSuite.Create parquet table with compression</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite.All Types Dictionary</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite.All Types Null</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite.Read row group containing both dictionary and plain encoded pages</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite.read parquet footers in parallel</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.basic data types (without binary)</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.raw binary</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.string</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - fixed-length decimals</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - fixed-length decimals</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.date type</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - map</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - map</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - array</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - array</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - array and double</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - array and double</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - struct</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - struct</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - nested struct with array of array as field</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - nested struct with array of array as field</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - nested map with struct as value type</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - nested map with struct as value type</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.nulls</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.nones</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.compression codec</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.save - overwrite</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.save - ignore</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.save - throw</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.save - append</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-8121: spark.sql.parquet.output.committer.class shouldn't be overridden</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-7837 Do not close output writer twice when commitTask() fails</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-11044 Parquet writer version fixed as version1</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.null and non-null strings</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.read dictionary and plain encoded timestamp_millis written as INT64</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-12589 copy() on rows returned from reader works for strings</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.VectorizedParquetRecordReader - direct path read</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.VectorizedParquetRecordReader - partition column types</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-23173 Writing a file with data converted from JSON with and incorrect user schema</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetInteroperabilitySuite.parquet timestamp conversion</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite.schema merging failure error message</div></li><li><div>org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite.Read Parquet file generated by parquet-thrift</div></li><li><div>org.apache.spark.sql.execution.metric.SQLMetricsSuite.range metrics</div></li><li><div>org.apache.spark.sql.execution.metric.SQLMetricsSuite.writing data out metrics: parquet</div></li><li><div>org.apache.spark.sql.execution.metric.SQLMetricsSuite.writing data out metrics with dynamic partition: parquet</div></li><li><div>org.apache.spark.sql.internal.CatalogSuite.dropTempView should not un-cache and drop metastore table if a same-name table exists</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.read bucketed data</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.avoid shuffle when join 2 bucketed tables</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only shuffle one side when join bucketed table and non-bucketed table</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only shuffle one side when 2 bucketed tables have different bucket number</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only shuffle one side when 2 bucketed tables have different bucket keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.shuffle when join keys are not equal to bucket keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.shuffle when join 2 bucketed tables with bucketing disabled</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.check sort and shuffle when bucket and sort columns are join keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.avoid shuffle and sort when sort columns are a super set of join keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only sort one side when sort columns are different</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only sort one side when sort columns are same but their ordering is different</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.avoid shuffle when grouping keys are equal to bucket keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.avoid shuffle when grouping keys are a super-set of bucket keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.SPARK-17698 Join predicates should not contain filter clauses</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.SPARK-19122 Re-order join predicates if they match with the child's output partitioning</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.SPARK-19122 No re-ordering should happen if set of join columns != set of child's partitioning columns</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.SPARK-22042 ReorderJoinPredicates can break when child's partitioning is not decided</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.error if there exists any malformed bucket files</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.disable bucketing when the output doesn't contain all bucketing columns</div></li><li><div>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data</div></li><li><div>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data with sortBy</div></li><li><div>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data without partitionBy</div></li><li><div>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data without partitionBy with sortBy</div></li><li><div>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data with bucketing disabled</div></li><li><div>org.apache.spark.sql.sources.CreateTableAsSelectSuite.create table using as select - with partitioned by</div></li><li><div>org.apache.spark.sql.sources.CreateTableAsSelectSuite.create table using as select - with valid number of buckets</div></li><li><div>org.apache.spark.sql.sources.CreateTableAsSelectSuite.SPARK-17409: CTAS of decimal calculation</div></li><li><div>org.apache.spark.sql.sources.PartitionedWriteSuite.write many partitions</div></li><li><div>org.apache.spark.sql.sources.PartitionedWriteSuite.write many partitions with repeats</div></li><li><div>org.apache.spark.sql.sources.PartitionedWriteSuite.partitioned columns should appear at the end of schema</div></li><li><div>org.apache.spark.sql.sources.PartitionedWriteSuite.maxRecordsPerFile setting in non-partitioned write path</div></li><li><div>org.apache.spark.sql.sources.PartitionedWriteSuite.maxRecordsPerFile setting in dynamic partition writes</div></li><li><div>org.apache.spark.sql.sources.PartitionedWriteSuite.append data to an existing partitioned table without custom partition path</div></li><li><div>org.apache.spark.sql.sources.PartitionedWriteSuite.timeZone setting in dynamic partition writes</div></li><li><div>org.apache.spark.sql.streaming.DeduplicateSuite.deduplicate with file sink</div></li><li><div>org.apache.spark.sql.streaming.FileStreamSinkSuite.unpartitioned writing and batch reading</div></li><li><div>org.apache.spark.sql.streaming.FileStreamSinkSuite.SPARK-21167: encode and decode path correctly</div></li><li><div>org.apache.spark.sql.streaming.FileStreamSinkSuite.partitioned writing and batch reading</div></li><li><div>org.apache.spark.sql.streaming.FileStreamSinkSuite.partitioned writing and batch reading with 'basePath'</div></li><li><div>org.apache.spark.sql.streaming.FileStreamSinkSuite.writing with aggregation</div></li><li><div>org.apache.spark.sql.streaming.FileStreamSourceSuite.FileStreamSource schema: parquet, existing files, no schema</div></li><li><div>org.apache.spark.sql.streaming.FileStreamSourceSuite.FileStreamSource schema: parquet, existing files, schema</div></li><li><div>org.apache.spark.sql.streaming.FileStreamSourceSuite.read from parquet files</div></li><li><div>org.apache.spark.sql.streaming.FileStreamSourceSuite.read from parquet files with changing schema</div></li><li><div>org.apache.spark.sql.streaming.FileStreamStressSuite.fault tolerance stress test - unpartitioned output</div></li><li><div>org.apache.spark.sql.streaming.FileStreamStressSuite.fault tolerance stress test - partitioned output</div></li><li><div>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite.SPARK-18510: use user specified types for partition columns in file sources</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.write path implements onTaskCommit API correctly</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.parquet - API and behavior regarding schema</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.column nullability and comment - write and then read</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-17230: write out results of decimal calculation</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Append should not fail if the table not exists but a same-name temp view exist</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Append should not fail if the table already exists and a same-name temp view exist</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode ErrorIfExists should not fail if the table not exists but a same-name temp view exist</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Overwrite should not drop the temp view if the table not exists but a same-name temp view exist</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Overwrite should not fail if the table already exists and a same-name temp view exist</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Ignore should create the table if the table not exists but a same-name temp view exist</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-18510: use user specified types for partition columns in file sources</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-18899: append to a bucketed table using DataFrameWriter with mismatched bucketing</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-18913: append to a table with special column names</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-20460 Check name duplication in schema</div></li><li><div>org.apache.spark.sql.test.DataFrameReaderWriterSuite.use Spark jobs to list files</div></li><li><div>org.apache.spark.sql.util.DataFrameCallbackSuite.execute callback functions for DataFrameWriter</div></li><li><div>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</div></li><li><div>org.apache.spark.sql.hive.CachedTableSuite.REFRESH TABLE also needs to recache the data (data source tables)</div></li><li><div>org.apache.spark.sql.hive.CachedTableSuite.SPARK-15678: REFRESH PATH</div></li><li><div>org.apache.spark.sql.hive.CachedTableSuite.Cache/Uncache Qualified Tables</div></li><li><div>org.apache.spark.sql.hive.CachedTableSuite.SPARK-11246 cache parquet table</div></li><li><div>org.apache.spark.sql.hive.CompressionCodecSuite.both table-level and session-level compression are set</div></li><li><div>org.apache.spark.sql.hive.CompressionCodecSuite.table-level compression is not set but session-level compressions is set</div></li><li><div>org.apache.spark.sql.hive.CompressionCodecSuite.test table containing mixed compression codec</div></li><li><div>org.apache.spark.sql.hive.HiveParquetSuite.Case insensitive attribute names</div></li><li><div>org.apache.spark.sql.hive.HiveParquetSuite.SELECT on Parquet table</div></li><li><div>org.apache.spark.sql.hive.HiveParquetSuite.Simple column projection + filter on Parquet table</div></li><li><div>org.apache.spark.sql.hive.HiveParquetSuite.Converting Hive to Parquet Table via saveAsParquetFile</div></li><li><div>org.apache.spark.sql.hive.HiveParquetSuite.INSERT OVERWRITE TABLE Parquet table</div></li><li><div>org.apache.spark.sql.hive.InsertSuite.Data source table - INSERT OVERWRITE - partition IF NOT EXISTS</div></li><li><div>org.apache.spark.sql.hive.InsertSuite.Data source table - SPARK-16037: INSERT statement should match columns by position</div></li><li><div>org.apache.spark.sql.hive.InsertSuite.Data source table - INSERT INTO a partitioned table (semantic and error handling)</div></li><li><div>org.apache.spark.sql.hive.InsertSuite.Data source table - insertInto() should match columns by position and ignore column names</div></li><li><div>org.apache.spark.sql.hive.InsertSuite.Data source table - insertInto() should match unnamed columns by position</div></li><li><div>org.apache.spark.sql.hive.InsertSuite.SPARK-21165: FileFormatWriter should only rely on attributes from analyzed plan</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.convert partition provider to hive with repair table</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.when partition management is enabled, new tables have partition provider hive</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.when partition management is disabled, new tables have no partition provider</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.when partition management is disabled, we preserve the old behavior even for new tables</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert overwrite partition of legacy datasource table</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert overwrite partition of new datasource table overwrites just partition</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18544 append with saveAsTable - partition management true</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18635 special chars in partition values - partition management true</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18659 insert overwrite table files - partition management true</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18659 insert overwrite table with lowercase - partition management true</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-19887 partition value is null - partition management true</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18544 append with saveAsTable - partition management false</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18635 special chars in partition values - partition management false</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18659 insert overwrite table files - partition management false</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18659 insert overwrite table with lowercase - partition management false</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-19887 partition value is null - partition management false</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert into partial dynamic partitions</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert into fully dynamic partitions</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert into static partition</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.overwrite partial dynamic partitions</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.overwrite fully dynamic partitions</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.overwrite static partition</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.append data with DataFrameWriter</div></li><li><div>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-19359: renaming partition should not leave useless directories</div></li><li><div>org.apache.spark.sql.hive.ShowCreateTableSuite.data source table using Dataset API</div></li><li><div>org.apache.spark.sql.hive.StatisticsSuite.SPARK-18856: non-empty partitioned table should not report zero size</div></li><li><div>org.apache.spark.sql.hive.StatisticsSuite.size estimation for relations is based on row size * number of rows</div></li><li><div>org.apache.spark.sql.hive.StatisticsSuite.analyze non hive compatible datasource tables</div></li><li><div>org.apache.spark.sql.hive.StatisticsSuite.test statistics of LogicalRelation converted from Hive serde tables</div></li><li><div>org.apache.spark.sql.hive.StatisticsSuite.verify serialized column stats after analyzing columns</div></li><li><div>org.apache.spark.sql.hive.StatisticsSuite.verify column stats can be deserialized from tblproperties</div></li><li><div>org.apache.spark.sql.hive.StatisticsSuite.serialization and deserialization of histograms to/from hive metastore</div></li><li><div>org.apache.spark.sql.hive.StatisticsSuite.test table-level statistics for data source table created in HiveExternalCatalog</div></li><li><div>org.apache.spark.sql.hive.StatisticsSuite.test table-level statistics for partitioned data source table</div></li><li><div>org.apache.spark.sql.hive.StatisticsSuite.test refreshing table stats of cached data source table by `ANALYZE TABLE` statement</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.12: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.13: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.0.14: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.0: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.1: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.1.2: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.0: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.1: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.2: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.client.VersionsSuite.2.3: CTAS for managed data source tables</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create partitioned data source table without user specified schema</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create partitioned data source table with user specified schema</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create non-partitioned data source table without user specified schema</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create non-partitioned data source table with user specified schema</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.alter table: rename cached table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create Data Source Table As Select</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.truncate table - datasource table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.truncate partitioned table - datasource table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.create temporary view with mismatched schema</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.truncate table - external table, temporary table, view (not allowed)</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.truncate table - non-partitioned table (not allowed)</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.SPARK-16034 Partition columns should match when appending to existing data source tables</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.SET LOCATION for managed table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.insert data to a data source table which has a non-existing location should succeed</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.insert into a data source table with a non-existing partition location should succeed</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.read data from a data source table with non-existing partition location should succeed</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.create datasource table with a non-existing location</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.CTAS for external data source table with a non-existing location</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.CTAS for external data source table with a existed location</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.data source table:partition column name containing a b</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.data source table:partition column name containing a:b</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.data source table:partition column name containing a%b</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.data source table:partition column name containing a,b</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a b for datasource table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a:b for datasource table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a%b for datasource table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a b for database</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a:b for database</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a%b for database</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.alter datasource table add columns - parquet</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.alter datasource table add columns - partitioned - parquet</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.basic DDL using locale tr - caseSensitive true</div></li><li><div>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.basic DDL using locale tr - caseSensitive false</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.drop external tables in default database</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.drop external data source table in default database</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.SPARK-19129: drop partition with a empty string will drop the whole table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.drop views</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter views - rename</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter views - set/unset tblproperties</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter views and alter table - misuse</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.drop view using drop table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.create view with mismatched schema</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.CREATE TABLE LIKE a temporary view</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.CREATE TABLE LIKE an external data source table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.CREATE TABLE LIKE a view</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.desc table for data source table - no user-defined schema</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.truncate table - datasource table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.truncate partitioned table - datasource table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.create hive serde table with Catalog</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.create hive serde table with DataFrameWriter.saveAsTable</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.append data to hive serde table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.partitioned table should always put partition columns at the end of table schema</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.partition column name of parquet table containing a b</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.partition column name of parquet table containing a:b</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.partition column name of parquet table containing a%b</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.partition column name of parquet table containing a,b</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter hive serde table add columns -- partitioned - PARQUET</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter hive serde table add columns -- with predicate - PARQUET</div></li><li><div>org.apache.spark.sql.hive.execution.HiveDDLSuite.SPARK-22252: FileFormatWriter should respect the input query schema in HIVE</div></li><li><div>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.create view for partitioned parquet table</div></li><li><div>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.Using view after adding more columns</div></li><li><div>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.correctly handle a view with custom column names</div></li><li><div>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.resolve a view when the dataTypes of referenced table columns changed</div></li><li><div>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.sparkSession API view resolution with different default database</div></li><li><div>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.create a permanent/temp view using a hive, built-in, and permanent user function</div></li><li><div>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.create a permanent/temp view using a temporary function</div></li><li><div>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.SPARK-14933 - create view from hive parquet table</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-6851: Self-joined converted parquet tables</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.CTAS without serde without location</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.CTAS with default fileformat</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.CTAS without serde with location</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.CTAS with serde</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.specifying database name for a temporary view is not allowed</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-10741: Sort on Aggregate using parquet</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.run sql directly on files - parquet</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.run sql directly on files - hive</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-11453: append data to partitioned table</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.insert into datasource table</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-15752 optimize metadata only query for hive table</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-17354: Partitioning by dates/timestamps works with Parquet vectorized reader</div></li><li><div>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-19912 String literals should be escaped for Hive metastore partition pruning</div></li><li><div>org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite.saveAsTable()/load() - partitioned table - boolean type</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.read bucketed data</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.avoid shuffle when join 2 bucketed tables</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only shuffle one side when join bucketed table and non-bucketed table</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only shuffle one side when 2 bucketed tables have different bucket number</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only shuffle one side when 2 bucketed tables have different bucket keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.shuffle when join keys are not equal to bucket keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.shuffle when join 2 bucketed tables with bucketing disabled</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.check sort and shuffle when bucket and sort columns are join keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.avoid shuffle and sort when sort columns are a super set of join keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only sort one side when sort columns are different</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only sort one side when sort columns are same but their ordering is different</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.avoid shuffle when grouping keys are equal to bucket keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.avoid shuffle when grouping keys are a super-set of bucket keys</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.SPARK-17698 Join predicates should not contain filter clauses</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.SPARK-19122 Re-order join predicates if they match with the child's output partitioning</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.SPARK-19122 No re-ordering should happen if set of join columns != set of child's partitioning columns</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.SPARK-22042 ReorderJoinPredicates can break when child's partitioning is not decided</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.error if there exists any malformed bucket files</div></li><li><div>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.disable bucketing when the output doesn't contain all bucketing columns</div></li><li><div>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data</div></li><li><div>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data with sortBy</div></li><li><div>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data without partitionBy</div></li><li><div>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data without partitionBy with sortBy</div></li><li><div>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data with bucketing disabled</div></li><li><div>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite.saveAsTable()/load() - partitioned table - boolean type</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="sqoop"><div style="font-weight:bold;" class="panel-heading">SQOOP</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>a7f5e0d298ffbf8e674bd35ee10f2accc1da5453</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 720</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 720</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 720</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 720</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td><td><ol></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="storm"><div style="font-weight:bold;" class="panel-heading">STORM</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>ffa607e2464a361a8f2fa548cc8043f5a8818d04</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1173</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1173</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1173</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1173</div><div>Failed Count : 2</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol><div><li>org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithoutKey</li></div></ol></td><td><ol><div><li>org.apache.storm.utils.JCQueueTest.testFirstMessageFirst</li></div></ol></td><td><ol><div><li>org.apache.storm.flux.multilang.MultilangEnvironmentTest.testInvokeNode</li></div><div><li>org.apache.storm.utils.JCQueueTest.testFirstMessageFirst</li></div></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol><div><li>
Wanted but not invoked:
collector.ack(
    source: null:1, stream: , id: {}, [value-234] PROC_START_TIME(sampled): null EXEC_START_TIME(sampled): null
);
-&gt; at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithoutKey(KafkaBoltTest.java:260)

However, there were exactly 2 interactions with this mock:
collector.reportError(
    java.util.concurrent.ExecutionException: org.apache.kafka.common.err</li></div></ol></td><td><ol><div><li>Unable to send halt interrupt</li></div></ol></td><td><ol><div><li>Cannot run program "node": error=2, No such file or directory</li></div><div><li>test timed out after 10000 milliseconds</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithoutKey</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.storm.flux.multilang.MultilangEnvironmentTest.testInvokeNode</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="tez"><div style="font-weight:bold;" class="panel-heading">TEZ</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>75bc7c157682cc64eccfa4722226a8ac72161f17</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1767</div><div>Failed Count : 1</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1767</div><div>Failed Count : 3</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1767</div><div>Failed Count : 2</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1767</div><div>Failed Count : 1</div><div>Skipped Count : 14</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.history.TestHistoryParser.testParserWithFailedJob</li></div><div><li>org.apache.tez.history.TestHistoryParser.testParserWithSuccessfulJob</li></div></ol></td><td><ol><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.test.TestExceptionPropagation.testExceptionPropagationSession</li></div></ol></td><td><ol><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td></tr><tr><td>Description</td><td><ol><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.tez.history.TestHistoryParser.testParserWithFailedJob(TestHistoryParser.java:383)
</li></div><div><li>java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.tez.history.TestHistoryParser.testParserWithSuccessfulJob(TestHistoryParser.java:207)
</li></div></ol></td><td><ol><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>Application not running, applicationId=application_1521087598580_0001, yarnApplicationState=FAILED, finalApplicationStatus=FAILED, trackingUrl=N/A, diagnostics=[DAG completed with an ERROR state. Shutting down AM, Session stats:submittedDAGs=10, successfulDAGs=0, failedDAGs=11, killedDAGs=0]</li></div></ol></td><td><ol><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.tez.history.TestHistoryParser.testParserWithFailedJob</div></li><li><div>org.apache.tez.history.TestHistoryParser.testParserWithSuccessfulJob</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.tez.test.TestExceptionPropagation.testExceptionPropagationSession</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="zeppelin"><div style="font-weight:bold;" class="panel-heading">ZEPPELIN</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>e30fe73e9a3bcd9cb14f02915883761894ceb2e4</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 662</div><div>Failed Count : 33</div><div>Skipped Count : 5</div></td><td><div>Total Count : 649</div><div>Failed Count : 31</div><div>Skipped Count : 4</div></td><td><div>Total Count : 650</div><div>Failed Count : 36</div><div>Skipped Count : 5</div></td><td><div>Total Count : 646</div><div>Failed Count : 20</div><div>Skipped Count : 4</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.configuration.RequestHeaderSizeTest.increased_request_header_size_do_not_cause_413_when_request_size_is_over_8K</li></div><div><li>org.apache.zeppelin.recovery.RecoveryTest.org.apache.zeppelin.recovery.RecoveryTest</li></div><div><li>org.apache.zeppelin.rest.ConfigurationsRestApiTest.org.apache.zeppelin.rest.ConfigurationsRestApiTest</li></div><div><li>org.apache.zeppelin.rest.CredentialsRestApiTest.org.apache.zeppelin.rest.CredentialsRestApiTest</li></div><div><li>org.apache.zeppelin.rest.HeliumRestApiTest.org.apache.zeppelin.rest.HeliumRestApiTest</li></div><div><li>org.apache.zeppelin.rest.InterpreterRestApiTest.org.apache.zeppelin.rest.InterpreterRestApiTest</li></div><div><li>org.apache.zeppelin.rest.KnoxRestApiTest.org.apache.zeppelin.rest.KnoxRestApiTest</li></div><div><li>org.apache.zeppelin.rest.NotebookRepoRestApiTest.org.apache.zeppelin.rest.NotebookRepoRestApiTest</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.org.apache.zeppelin.rest.NotebookRestApiTest</li></div><div><li>org.apache.zeppelin.rest.NotebookSecurityRestApiTest.org.apache.zeppelin.rest.NotebookSecurityRestApiTest</li></div><div><li>org.apache.zeppelin.rest.SecurityRestApiTest.org.apache.zeppelin.rest.SecurityRestApiTest</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.org.apache.zeppelin.rest.ZeppelinSparkClusterTest</li></div><div><li>org.apache.zeppelin.socket.NotebookServerTest.org.apache.zeppelin.socket.NotebookServerTest</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[3]</li></div></ol></td><td><ol><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.interpreter.InterpreterOutputChangeWatcherTest.test</li></div><div><li>org.apache.zeppelin.rest.ConfigurationsRestApiTest.org.apache.zeppelin.rest.ConfigurationsRestApiTest</li></div><div><li>org.apache.zeppelin.rest.CredentialsRestApiTest.org.apache.zeppelin.rest.CredentialsRestApiTest</li></div><div><li>org.apache.zeppelin.rest.HeliumRestApiTest.org.apache.zeppelin.rest.HeliumRestApiTest</li></div><div><li>org.apache.zeppelin.rest.InterpreterRestApiTest.org.apache.zeppelin.rest.InterpreterRestApiTest</li></div><div><li>org.apache.zeppelin.rest.KnoxRestApiTest.org.apache.zeppelin.rest.KnoxRestApiTest</li></div><div><li>org.apache.zeppelin.rest.NotebookRepoRestApiTest.org.apache.zeppelin.rest.NotebookRepoRestApiTest</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.org.apache.zeppelin.rest.NotebookRestApiTest</li></div><div><li>org.apache.zeppelin.rest.NotebookSecurityRestApiTest.org.apache.zeppelin.rest.NotebookSecurityRestApiTest</li></div><div><li>org.apache.zeppelin.rest.SecurityRestApiTest.org.apache.zeppelin.rest.SecurityRestApiTest</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.org.apache.zeppelin.rest.ZeppelinSparkClusterTest</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[3]</li></div></ol></td><td><ol><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testGrpcFrameSize</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPython</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.interpreter.InterpreterOutputChangeWatcherTest.test</li></div><div><li>org.apache.zeppelin.rest.ConfigurationsRestApiTest.org.apache.zeppelin.rest.ConfigurationsRestApiTest</li></div><div><li>org.apache.zeppelin.rest.CredentialsRestApiTest.org.apache.zeppelin.rest.CredentialsRestApiTest</li></div><div><li>org.apache.zeppelin.rest.InterpreterRestApiTest.org.apache.zeppelin.rest.InterpreterRestApiTest</li></div><div><li>org.apache.zeppelin.rest.KnoxRestApiTest.org.apache.zeppelin.rest.KnoxRestApiTest</li></div><div><li>org.apache.zeppelin.rest.NotebookRepoRestApiTest.org.apache.zeppelin.rest.NotebookRepoRestApiTest</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.org.apache.zeppelin.rest.NotebookRestApiTest</li></div><div><li>org.apache.zeppelin.rest.NotebookSecurityRestApiTest.org.apache.zeppelin.rest.NotebookSecurityRestApiTest</li></div><div><li>org.apache.zeppelin.rest.SecurityRestApiTest.org.apache.zeppelin.rest.SecurityRestApiTest</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.org.apache.zeppelin.rest.ZeppelinSparkClusterTest</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageTest.testSingleInterpreterProcess</li></div><div><li>org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageTest.testSingleInterpreterProcess</li></div><div><li>org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageTest.testMultipleInterpreterProcess</li></div><div><li>org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageTest.testMultipleInterpreterProcess</li></div></ol></td><td><ol><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPython</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[1]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClientMode[3]</li></div></ol></td></tr><tr><td>Description</td><td><ol><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;[你好]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>/var/lib/jenkins/workspace/zeppelin/zeppelin-zengine/../bin/interpreter.sh: line 235: /var/lib/jenkins/workspace/zeppelin/run/zeppelin-interpreter-spark--36acd50f73a7.pid: No such file or directory
Warning: Master yarn-cluster is deprecated since 2.0. Please use master "yarn" with specified deploy mode instead.
03:30:09,827  WARN org.apache.hadoop.util.NativeCodeLoader:62 - Unable to load native-h</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div></ol></td><td><ol><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;[你好]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;2&gt;</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>/var/lib/jenkins/workspace/zeppelin/zeppelin-zengine/../bin/interpreter.sh: line 235: /var/lib/jenkins/workspace/zeppelin/run/zeppelin-interpreter-spark--2069d09be9d0.pid: No such file or directory
Warning: Master yarn-cluster is deprecated since 2.0. Please use master "yarn" with specified deploy mode instead.
07:45:36,881  WARN org.apache.hadoop.util.NativeCodeLoader:62 - Unable to load native-h</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div><div><li>java.lang.NullPointerException</li></div></ol></td><td><ol><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>Fail to open IPythonInterpreter</li></div><div><li>Fail to open IPythonInterpreter</li></div><div><li>Fail to open IPythonInterpreter</li></div><div><li>java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:712)
	at org.junit.Assert.assertNotNull(Assert.java:722)
	at org.apache.zeppelin.interpreter.InterpreterOutputChangeWatcherTest.test(InterpreterOutputChangeWatcherTest.java:99)
</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>Can not start Zeppelin server</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>res6: String = 2.2.1
 doesn't contain 2.1.2</li></div><div><li>res7: String = 2.2.1
 doesn't contain 2.1.2</li></div><div><li>res8: String = 2.2.1
 doesn't contain 2.1.2</li></div><div><li>res9: String = 2.2.1
 doesn't contain 2.0.2</li></div><div><li>res10: String = 2.2.1
 doesn't contain 2.0.2</li></div><div><li>res11: String = 2.2.1
 doesn't contain 2.0.2</li></div><div><li>res12: String = 2.2.1
 doesn't contain 1.6.3</li></div><div><li>res13: String = 2.2.1
 doesn't contain 1.6.3</li></div><div><li>res14: String = 2.2.1
 doesn't contain 1.6.3</li></div><div><li>Failed to create directory within 10000 attempts (tried 1521171155893-0 to 1521171155893-9999)</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.interpreter.AbstractInterpreterTest.tearDown(AbstractInterpreterTest.java:68)
	at org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageTest.tearDown(FileSystemRecoveryStorageTest.java:41)
</li></div><div><li>Failed to create directory within 10000 attempts (tried 1521171155985-0 to 1521171155985-9999)</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.interpreter.AbstractInterpreterTest.tearDown(AbstractInterpreterTest.java:68)
	at org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageTest.tearDown(FileSystemRecoveryStorageTest.java:41)
</li></div></ol></td><td><ol><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;[你好]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>res6: String = 2.2.1
 doesn't contain 2.1.2</li></div><div><li>res7: String = 2.2.1
 doesn't contain 2.1.2</li></div><div><li>res8: String = 2.2.1
 doesn't contain 2.1.2</li></div><div><li>res9: String = 2.2.1
 doesn't contain 2.0.2</li></div><div><li>res10: String = 2.2.1
 doesn't contain 2.0.2</li></div><div><li>res11: String = 2.2.1
 doesn't contain 2.0.2</li></div><div><li>res12: String = 2.2.1
 doesn't contain 1.6.3</li></div><div><li>res13: String = 2.2.1
 doesn't contain 1.6.3</li></div><div><li>res14: String = 2.2.1
 doesn't contain 1.6.3</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.zeppelin.configuration.RequestHeaderSizeTest.increased_request_header_size_do_not_cause_413_when_request_size_is_over_8K</div></li><li><div>org.apache.zeppelin.recovery.RecoveryTest.org.apache.zeppelin.recovery.RecoveryTest</div></li><li><div>org.apache.zeppelin.socket.NotebookServerTest.org.apache.zeppelin.socket.NotebookServerTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.zeppelin.python.IPythonInterpreterTest.testGrpcFrameSize</div></li><li><div>org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageTest.testSingleInterpreterProcess</div></li><li><div>org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageTest.testSingleInterpreterProcess</div></li><li><div>org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageTest.testMultipleInterpreterProcess</div></li><li><div>org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageTest.testMultipleInterpreterProcess</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="zookeeper"><div style="font-weight:bold;" class="panel-heading">ZOOKEEPER</div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>99c9bbb0ab1eef469e1662086532c58078b9909a</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th></th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1130</div><div>Failed Count : 0</div><div>Skipped Count : 1</div></td><td><div>Total Count : 1130</div><div>Failed Count : 3</div><div>Skipped Count : 1</div></td><td><div>Total Count : 1127</div><div>Failed Count : 3</div><div>Skipped Count : 1</div></td><td><div>Total Count : 1127</div><div>Failed Count : 4</div><div>Skipped Count : 1</div></td></tr><tr><td>Result</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol></ol></td><td><ol><div><li>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalFollowerRunWithDiff</li></div><div><li>org.apache.zookeeper.server.quorum.Zab1_0Test.testTxnTimeout</li></div><div><li>org.apache.zookeeper.test.WatchEventWhenAutoResetTest.testNodeDataChanged</li></div></ol></td><td><ol><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.MiniKdcTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosAuthTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosHostBasedAuthTest</li></div></ol></td><td><ol><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testFailedTxnAsPartOfQuorumLoss</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.MiniKdcTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosAuthTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosHostBasedAuthTest</li></div></ol></td></tr><tr><td>Description</td><td><ol></ol></td><td><ol><div><li>java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
	at org.apache.zookeeper.server.persistence.FileHeader.deserialize(FileHeader.java:66)
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.inStreamCreated(FileTxnLog.java:675)
	at org.apache.zookeeper.server.persistence.Fil</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>expected:&lt;NodeDataChanged&gt; but was:&lt;NodeDeleted&gt;</li></div></ol></td><td><ol><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div></ol></td><td><ol><div><li>expected:&lt;1&gt; but was:&lt;2&gt;</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalFollowerRunWithDiff</div></li><li><div>org.apache.zookeeper.server.quorum.Zab1_0Test.testTxnTimeout</div></li><li><div>org.apache.zookeeper.test.WatchEventWhenAutoResetTest.testNodeDataChanged</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol><li><div>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testFailedTxnAsPartOfQuorumLoss</div></li></ol></td></tr></tbody></table></div></div><div id="ppcubuntu16" style="font-weight:bold;font-size:12;display:block" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">PPC UBUNTU16 SUMMARY</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/red.png" align="top" style="width: 16px; height: 16px;"></img>0 (0)</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>4 (4)</td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>18 (6)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>13 (2)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>33 (33)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>20 (0)</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>24 (0)</td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>33 (3)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr></tbody></table></div><div id="x86ubuntu16" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">X86 UBUNTU16 SUMMARY</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>60 (46)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>12 (1)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>20 (0)</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>24 (0)</td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (2)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>31 (0)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (3)</td></tr></tbody></table></div><div id="ppcrhel7" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">PPC RHEL7 SUMMARY</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>16 (4)</td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>20 (15)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (2)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>393 (393)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>36 (5)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (0)</td></tr></tbody></table></div><div id="x86rhel7" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">X86 RHEL7 SUMMARY</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>12 (0)</td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>28 (23)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (0)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>20 (0)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>4 (1)</td></tr></tbody></table></div><div id="ppcx86" style="display:none;font-weight:bold" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">FULL SUMMARY</div></div><table class="table table-striped"><tbody><tr><th></th></tr><tr><th>Package Name</th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (2)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/red.png" align="top" style="width: 16px; height: 16px;"></img>0 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>4 (4)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>16 (4)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>12 (0)</td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>18 (6)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>60 (46)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>20 (15)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>28 (23)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (2)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>13 (2)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>12 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (2)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (0)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>33 (33)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>20 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>20 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>24 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>24 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>393 (393)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (2)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>2 (1)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>1 (0)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>33 (3)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>31 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>36 (5)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>20 (0)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/blue.png" align="top" style="width: 16px; height: 16px;"></img></td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (3)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>3 (0)</td><td><img src="https://builds.apache.org/static/53471590/images/48x48/yellow.png" align="top" style="width: 16px; height: 16px;"></img>4 (1)</td></tr></tbody></table></div></div></body></html>